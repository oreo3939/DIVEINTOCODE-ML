{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "線形回帰スクラッチ\n",
    "\n",
    "\n",
    "線形回帰のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "\n",
    "以下に雛形を用意してあります。このScratchLinearRegressionクラスにコードを書き加えていってください。\n",
    "\n",
    "    class ScratchLinearRegression():\n",
    "    \"\"\"\n",
    "        線形回帰のスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.no_bias = no_bias\n",
    "        self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程を出力\n",
    "            print()\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        線形回帰を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            線形回帰による推定結果\n",
    "        \"\"\"\n",
    "        pass\n",
    "        return\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題1】仮定関数\n",
    "\n",
    "以下の数式で表される線形回帰の仮定関数を実装してください。メソッドの雛形を用意してあります。\n",
    "\n",
    "$$\n",
    "h_\\theta(x) =  \\theta_0 x_0 + \\theta_1 x_1 + ... + \\theta_j x_j + ... +\\theta_n x_n.   (x_0 = 1)\\\\\n",
    "$$\n",
    "\n",
    "x\n",
    " : 特徴量ベクトル\n",
    "\n",
    "\n",
    "θ\n",
    " : パラメータベクトル\n",
    "\n",
    "\n",
    "n\n",
    " : 特徴量の数\n",
    "\n",
    "\n",
    "x\n",
    "j\n",
    " : j番目の特徴量\n",
    "\n",
    "\n",
    "θ\n",
    "j\n",
    " : j番目のパラメータ（重み）\n",
    "\n",
    "\n",
    "特徴量の数\n",
    "n\n",
    "は任意の値に対応できる実装にしてください。\n",
    "\n",
    "\n",
    "なお、ベクトル形式で表すと以下のようになります。\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta^T \\cdot x.\n",
    "$$\n",
    "\n",
    "雛形\n",
    "\n",
    "\n",
    "クラスの外から呼び出すことがないメソッドのため、Pythonの慣例としてアンダースコアを先頭にひとつつけています。\n",
    "\n",
    "    def _linear_hypothesis(self, X):\n",
    "    \"\"\"\n",
    "    線形の仮定関数を計算する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "      次の形のndarray, shape (n_samples, 1)\n",
    "      線形の仮定関数による推定結果\n",
    "\n",
    "    \"\"\"\n",
    "    pass\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _linear_hypothesis(self, X):\n",
    "    \"\"\"\n",
    "    線形の仮定関数を計算する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "      h_theta : 次の形のndarray, shape (n_samples, 1)\n",
    "      線形の仮定関数による推定結果\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    y_hut = np.dot(X, theta)\n",
    "    return  y_hut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題2】最急降下法\n",
    "\n",
    "最急降下法により学習させる実装を行なってください。以下の式で表されるパラメータの更新式のメソッド_gradient_descentを追加し、fit\n",
    "メソッドから呼び出すようにしてください。\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[(h_\\theta(x^{(i)}) - y^{(i)} )x_{j}^{(i)}]\n",
    "$$\n",
    "\n",
    "α\n",
    " : 学習率\n",
    "\n",
    "\n",
    "i\n",
    " : サンプルのインデックス\n",
    "\n",
    "\n",
    "j\n",
    " : 特徴量のインデックス\n",
    "\n",
    "\n",
    "雛形\n",
    "\n",
    "\n",
    "ScratchLinearRegressionクラスへ以下のメソッドを追加してください。コメントアウト部分の説明も記述してください。\n",
    "\n",
    "    def _gradient_descent(self, X, error):\n",
    "    \"\"\"\n",
    "    説明を記述\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "雛形として用意されたメソッドや関数以外でも必要があれば各自作成して完成させてください。雛形を外れても問題ありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gradient_descent(self, X, error):\n",
    "    \"\"\"\n",
    "    説明を記述\n",
    "    \"\"\"\n",
    "    y_hut = self._linear_hypothesis(X, theta)   \n",
    "    theta -= (lr / len(X)) * (np.sum(np.dot(y_hut - y, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題3】推定\n",
    "\n",
    "推定する仕組みを実装してください。ScratchLinearRegressionクラスの雛形に含まれるpredictメソッドに書き加えてください。\n",
    "\n",
    "\n",
    "仮定関数 \n",
    "h\n",
    "θ\n",
    "(\n",
    "x\n",
    ")\n",
    " の出力が推定結果です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    \"\"\"\n",
    "    線形回帰を使い推定する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        サンプル\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "      y_hut : 次の形のndarray, shape (n_samples, 1)\n",
    "        線形回帰による推定結果\n",
    "    \"\"\"\n",
    "     if self.no_bias is False:\n",
    "            X = self._define_feature_variables(X)\n",
    "            \n",
    "        y_hut = self._linear_hypothesis(X, self.theta)\n",
    "\n",
    "        return y_hut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題4】平均二乗誤差\n",
    "\n",
    "線形回帰の指標値として用いられる平均二乗誤差（mean square error, MSE）の関数を作成してください。\n",
    "\n",
    "\n",
    "平均二乗誤差関数は回帰問題全般で使える関数のため、ScratchLinearRegressionクラスのメソッドではなく、別の関数として作成してください。雛形を用意してあります。\n",
    "\n",
    "\n",
    "平均二乗誤差は以下の数式で表されます。\n",
    "\n",
    "$$\n",
    "L(\\theta)=  \\frac{1 }{ m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\n",
    "$$\n",
    "\n",
    "m\n",
    " : 入力されるデータの数\n",
    "\n",
    "\n",
    "h\n",
    "θ\n",
    "(\n",
    ")\n",
    " : 仮定関数\n",
    "\n",
    "\n",
    "x\n",
    "(\n",
    "i\n",
    ")\n",
    " : i番目のサンプルの特徴量ベクトル\n",
    "\n",
    "\n",
    "y\n",
    "(\n",
    "i\n",
    ")\n",
    " : i番目のサンプルの正解値\n",
    "\n",
    "\n",
    "なお、最急降下法のための目的関数（損失関数）としては、これを2で割ったものを使用します。（問題5, 9）\n",
    "\n",
    "\n",
    "雛形\n",
    "\n",
    "    \n",
    "    def MSE(y_pred, y):\n",
    "    \"\"\"\n",
    "    平均二乗誤差の計算\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : 次の形のndarray, shape (n_samples,)\n",
    "      推定した値\n",
    "    y : 次の形のndarray, shape (n_samples,)\n",
    "      正解値\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mse : numpy.float\n",
    "      平均二乗誤差\n",
    "    \"\"\"\n",
    "    pass\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_pred, y):\n",
    "\"\"\"\n",
    "平均二乗誤差の計算\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "y_pred : 次の形のndarray, shape (n_samples,)\n",
    "  推定した値\n",
    "y : 次の形のndarray, shape (n_samples,)\n",
    "  正解値\n",
    "\n",
    "Returns\n",
    "----------\n",
    "mse : numpy.float\n",
    "  平均二乗誤差\n",
    "\"\"\"\n",
    "    y_pred = _linear_hypothesis(self, X)\n",
    "    te = (y_pred-y)**2\n",
    "    mse = sum(te) / len(y)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題5】目的関数\n",
    "\n",
    "以下の数式で表される線形回帰の 目的関数（損失関数） を実装してください。そして、これをself.loss, self.val_lossに記録するようにしてください。\n",
    "\n",
    "\n",
    "目的関数（損失関数） \n",
    "J\n",
    "(\n",
    "θ\n",
    ")\n",
    " は次の式です\n",
    " \n",
    " $$\n",
    " J(\\theta)=  \\frac{1 }{ 2m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\n",
    " $$\n",
    " \n",
    " m\n",
    " : 入力されるデータの数\n",
    "\n",
    "\n",
    "h\n",
    "θ\n",
    "(\n",
    ")\n",
    " : 仮定関数\n",
    "\n",
    "\n",
    "x\n",
    "(\n",
    "i\n",
    ")\n",
    " : i番目のサンプルの特徴量ベクトル\n",
    "\n",
    "\n",
    "y\n",
    "(\n",
    "i\n",
    ")\n",
    " : i番目のサンプルの正解値\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$残差(h_\\theta(x^{(i)})-y^{(i)})^2.$$\n",
    "\n",
    "$$合計=sum()   \\sum_{i=1}^{m}$$\n",
    "\n",
    "$$m=len(y) \\frac{1 }{ 2m}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(self,y_hut,y):\n",
    "    \"\"\"\n",
    "        平均二乗誤差及びコスト関数による計算\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_hut : 次の形のndarray, shape (n_samples,)\n",
    "          推定した値(予測値)\n",
    "        y : 次の形のndarray, shape (n_samples,)\n",
    "          正解値(実測値)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        mse2 : numpy.float\n",
    "          平均二乗誤差割る2\n",
    "    \"\"\"\n",
    "    square = (y_hut - y) **2\n",
    "    mse2 = sum(square) * (2*len(y))\n",
    "    \n",
    "    return mse2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "    \"\"\"\n",
    "        線形回帰のスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_iter, lr=0.01, no_bias=True, verbose=False):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.no_bias = no_bias\n",
    "        self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        if self.no_bias is False:\n",
    "            X = self._add_intercept(X)\n",
    "        if not(X_val is None) and not(y_val is None):\n",
    "            X_val =self._add_intercept(X_val)\n",
    "\n",
    "        #初期化\n",
    "        theta = np.random.rand(X.shape[1]).reshape(-1, 1)\n",
    "\n",
    "        for i in range(self.iter):\n",
    "            self.theta = self._gradient_descent(X,y,theta)\n",
    "            y_hut = self._linear_hypothesis(X, theta)\n",
    "            self.loss[i] = self.cost(y_hut, y)\n",
    "\n",
    "            # loss\n",
    "            if (X_val is not None) and (y_val is not None):\n",
    "                y_val_hut = self._linear_hypothesis(X_val, theta)\n",
    "                self.val_loss[i] = self.cost(y_val_hut, y_val)\n",
    "\n",
    "\n",
    "            #verboseをTrueにした際は学習過程を出力\n",
    "            if self.verbose:\n",
    "                print(\"{}回目のtrain_loss:{}\".format(i,self.loss[i]))\n",
    "                if not(X_val is None) and not(y_val is None):\n",
    "                    print(\"{}回目のval_loss:{}\".format(i,self.val_loss[i]))\n",
    "\n",
    "\n",
    "\n",
    "    def _add_intercept(self, X):\n",
    "        \"\"\"\n",
    "        特徴量Xの1列目全て１の列結合\n",
    "\n",
    "        \"\"\"\n",
    "        intercept = np.ones(X.shape[0]).reshape(-1, 1)\n",
    "        X_new = np.concatenate((intercept, X), axis=1)\n",
    "        return X_new\n",
    "\n",
    "    def _linear_hypothesis(self, X, theta):\n",
    "        \"\"\"\n",
    "        線形の仮定関数を計算する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          訓練データ\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "          h_theta : 次の形のndarray, shape (n_samples, 1)\n",
    "          線形の仮定関数による推定結果\n",
    "\n",
    "        \"\"\"\n",
    "        y_hut = np.dot(X, theta)\n",
    "        return  y_hut\n",
    "\n",
    "    def _gradient_descent(self, X, y, theta):\n",
    "        \"\"\"\n",
    "        説明を記述\n",
    "        \"\"\"\n",
    "\n",
    "        y_hut = self._linear_hypothesis(X, theta)   \n",
    "        theta -= np.transpose(np.dot(np.transpose(y_hut - y.reshape(-1, 1)), X)*self.lr/len(X))\n",
    "\n",
    "        return theta \n",
    "\n",
    "    def cost(self,y_hut,y):\n",
    "        \"\"\"\n",
    "            平均二乗誤差及びコスト関数による計算\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            y_hut : 次の形のndarray, shape (n_samples,)\n",
    "              推定した値(予測値)\n",
    "            y : 次の形のndarray, shape (n_samples,)\n",
    "              正解値(実測値)\n",
    "\n",
    "            Returns\n",
    "            ----------\n",
    "            mse2 : numpy.float\n",
    "              平均二乗誤差割る2\n",
    "        \"\"\"\n",
    "        #要素数合わせる\n",
    "        square = (y_hut - y.reshape(-1, 1)) **2\n",
    "        mse2 = sum(square) / (2*len(y))\n",
    "\n",
    "        return mse2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        線形回帰を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "          y_hut : 次の形のndarray, shape (n_samples, 1)\n",
    "            線形回帰による推定結果\n",
    "        \"\"\"\n",
    "        if self.no_bias is False:\n",
    "            X = _add_intercept(self, X)\n",
    "\n",
    "        y_hut = self._linear_hypothesis(X, self.theta)\n",
    "        return y_hut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題6】学習と推定\n",
    "\n",
    "機械学習スクラッチ入門のSprintで用意したHouse Pricesコンペティションのデータに対してスクラッチ実装の学習と推定を行なってください。\n",
    "\n",
    "scikit-learnによる実装と比べ、正しく動いているかを確認してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/iwaju/Desktop/csv/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X = np.array(df[['GrLivArea', 'YearBuilt']])\n",
    "y = np.array(df[['SalePrice']]).reshape(-1,)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "#np.logで対数計算\n",
    "y_train_scaled = np.log(y_train)\n",
    "y_test_scaled = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ScratchLinearRegression(num_iter=1000, lr=0.005, no_bias=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data****************************************\n",
      "theta:[[900.01116091 275.68546614 210.60296187]]\n",
      "loss in cost function:19080720069.904583\n",
      "Validation Data****************************************\n",
      "theta:[[900.01116091 275.68546614 210.60296187]]\n",
      "loss in cost function:20338651336.848755\n",
      "Training Data****************************************\n",
      "theta:[[1795.52177291  549.61206193  419.48764876]]\n",
      "loss in cost function:18897070844.22603\n",
      "Validation Data****************************************\n",
      "theta:[[1795.52177291  549.61206193  419.48764876]]\n",
      "loss in cost function:20146314826.437477\n",
      "Training Data****************************************\n",
      "theta:[[2686.55483185  821.94925602  627.03971254]]\n",
      "loss in cost function:18715301267.039288\n",
      "Validation Data****************************************\n",
      "theta:[[2686.55483185  821.94925602  627.03971254]]\n",
      "loss in cost function:19955934563.287804\n",
      "Training Data****************************************\n",
      "theta:[[3573.1327255  1092.70639748  833.26748855]]\n",
      "loss in cost function:18535392008.950504\n",
      "Validation Data****************************************\n",
      "theta:[[3573.1327255  1092.70639748  833.26748855]]\n",
      "loss in cost function:19767490483.424187\n",
      "Training Data****************************************\n",
      "theta:[[4455.27772968 1361.89277985 1038.1792606 ]]\n",
      "loss in cost function:18357323940.404015\n",
      "Validation Data****************************************\n",
      "theta:[[4455.27772968 1361.89277985 1038.1792606 ]]\n",
      "loss in cost function:19580962730.108406\n",
      "Training Data****************************************\n",
      "theta:[[5333.01200884 1629.51764151 1241.78326132]]\n",
      "loss in cost function:18181078129.60334\n",
      "Validation Data****************************************\n",
      "theta:[[5333.01200884 1629.51764151 1241.78326132]]\n",
      "loss in cost function:19396331651.684944\n",
      "Training Data****************************************\n",
      "theta:[[6206.35761661 1895.59016597 1444.08767244]]\n",
      "loss in cost function:18006635840.454967\n",
      "Validation Data****************************************\n",
      "theta:[[6206.35761661 1895.59016597 1444.08767244]]\n",
      "loss in cost function:19213577799.448853\n",
      "Training Data****************************************\n",
      "theta:[[7075.33649633 2160.11948223 1645.10062515]]\n",
      "loss in cost function:17833978530.533047\n",
      "Validation Data****************************************\n",
      "theta:[[7075.33649633 2160.11948223 1645.10062515]]\n",
      "loss in cost function:19032681925.535736\n",
      "Training Data****************************************\n",
      "theta:[[7939.97048166 2423.11466508 1844.83020035]]\n",
      "loss in cost function:17663087849.06546\n",
      "Validation Data****************************************\n",
      "theta:[[7939.97048166 2423.11466508 1844.83020035]]\n",
      "loss in cost function:18853624980.834167\n",
      "Training Data****************************************\n",
      "theta:[[8800.28129706 2684.58473543 2043.284429  ]]\n",
      "loss in cost function:17493945634.941433\n",
      "Validation Data****************************************\n",
      "theta:[[8800.28129706 2684.58473543 2043.284429  ]]\n",
      "loss in cost function:18676388112.919678\n",
      "Training Data****************************************\n",
      "theta:[[9656.29055838 2944.53866065 2240.47129241]]\n",
      "loss in cost function:17326533914.739273\n",
      "Validation Data****************************************\n",
      "theta:[[9656.29055838 2944.53866065 2240.47129241]]\n",
      "loss in cost function:18500952664.01037\n",
      "Training Data****************************************\n",
      "theta:[[10508.0197734   3202.98535485  2436.39872254]]\n",
      "loss in cost function:17160834900.77543\n",
      "Validation Data****************************************\n",
      "theta:[[10508.0197734   3202.98535485  2436.39872254]]\n",
      "loss in cost function:18327300168.944183\n",
      "Training Data****************************************\n",
      "theta:[[11355.49034234  3459.93367926  2631.07460235]]\n",
      "loss in cost function:16996830989.17335\n",
      "Validation Data****************************************\n",
      "theta:[[11355.49034234  3459.93367926  2631.07460235]]\n",
      "loss in cost function:18155412353.177017\n",
      "Training Data****************************************\n",
      "theta:[[12198.72355844  3715.39244249  2824.50676601]]\n",
      "loss in cost function:16834504757.953173\n",
      "Validation Data****************************************\n",
      "theta:[[12198.72355844  3715.39244249  2824.50676601]]\n",
      "loss in cost function:17985271130.801865\n",
      "Training Data****************************************\n",
      "theta:[[13037.74060845  3969.37040086  3016.70299928]]\n",
      "loss in cost function:16673838965.140816\n",
      "Validation Data****************************************\n",
      "theta:[[13037.74060845  3969.37040086  3016.70299928]]\n",
      "loss in cost function:17816858602.588886\n",
      "Training Data****************************************\n",
      "theta:[[13872.56257322  4221.87625873  3207.67103978]]\n",
      "loss in cost function:16514816546.897373\n",
      "Validation Data****************************************\n",
      "theta:[[13872.56257322  4221.87625873  3207.67103978]]\n",
      "loss in cost function:17650157054.045715\n",
      "Training Data****************************************\n",
      "theta:[[14703.21042816  4472.91866878  3397.41857728]]\n",
      "loss in cost function:16357420615.667717\n",
      "Validation Data****************************************\n",
      "theta:[[14703.21042816  4472.91866878  3397.41857728]]\n",
      "loss in cost function:17485148953.498062\n",
      "Training Data****************************************\n",
      "theta:[[15529.70504383  4722.50623235  3585.953254  ]]\n",
      "loss in cost function:16201634458.348513\n",
      "Validation Data****************************************\n",
      "theta:[[15529.70504383  4722.50623235  3585.953254  ]]\n",
      "loss in cost function:17321816950.19077\n",
      "Training Data****************************************\n",
      "theta:[[16352.06718642  4970.64749974  3773.28266488]]\n",
      "loss in cost function:16047441534.475546\n",
      "Validation Data****************************************\n",
      "theta:[[16352.06718642  4970.64749974  3773.28266488]]\n",
      "loss in cost function:17160143872.408056\n",
      "Training Data****************************************\n",
      "theta:[[17170.31751829  5217.35097049  3959.41435792]]\n",
      "loss in cost function:15894825474.429989\n",
      "Validation Data****************************************\n",
      "theta:[[17170.31751829  5217.35097049  3959.41435792]]\n",
      "loss in cost function:17000112725.614157\n",
      "Training Data****************************************\n",
      "theta:[[17984.47659851  5462.62509371  4144.35583442]]\n",
      "loss in cost function:15743770077.66325\n",
      "Validation Data****************************************\n",
      "theta:[[17984.47659851  5462.62509371  4144.35583442]]\n",
      "loss in cost function:16841706690.612854\n",
      "Training Data****************************************\n",
      "theta:[[18794.56488333  5706.47826837  4328.11454929]]\n",
      "loss in cost function:15594259310.94064\n",
      "Validation Data****************************************\n",
      "theta:[[18794.56488333  5706.47826837  4328.11454929]]\n",
      "loss in cost function:16684909121.726395\n",
      "Training Data****************************************\n",
      "theta:[[19600.60272672  5948.91884361  4510.69791132]]\n",
      "loss in cost function:15446277306.603058\n",
      "Validation Data****************************************\n",
      "theta:[[19600.60272672  5948.91884361  4510.69791132]]\n",
      "loss in cost function:16529703544.993706\n",
      "Training Data****************************************\n",
      "theta:[[20402.61038089  6189.95511901  4692.11328347]]\n",
      "loss in cost function:15299808360.847258\n",
      "Validation Data****************************************\n",
      "theta:[[20402.61038089  6189.95511901  4692.11328347]]\n",
      "loss in cost function:16376073656.38704\n",
      "Training Data****************************************\n",
      "theta:[[21200.6079968   6429.59534493  4872.36798318]]\n",
      "loss in cost function:15154836932.023865\n",
      "Validation Data****************************************\n",
      "theta:[[21200.6079968   6429.59534493  4872.36798318]]\n",
      "loss in cost function:16224003320.047577\n",
      "Training Data****************************************\n",
      "theta:[[21994.61562462  6667.84772275  5051.46928259]]\n",
      "loss in cost function:15011347638.953169\n",
      "Validation Data****************************************\n",
      "theta:[[21994.61562462  6667.84772275  5051.46928259]]\n",
      "loss in cost function:16073476566.539259\n",
      "Training Data****************************************\n",
      "theta:[[22784.6532143   6904.72040523  5229.42440887]]\n",
      "loss in cost function:14869325259.258583\n",
      "Validation Data****************************************\n",
      "theta:[[22784.6532143   6904.72040523  5229.42440887]]\n",
      "loss in cost function:15924477591.121103\n",
      "Training Data****************************************\n",
      "theta:[[23570.74061604  7140.22149673  5406.24054447]]\n",
      "loss in cost function:14728754727.71751\n",
      "Validation Data****************************************\n",
      "theta:[[23570.74061604  7140.22149673  5406.24054447]]\n",
      "loss in cost function:15776990752.03723\n",
      "Training Data****************************************\n",
      "theta:[[24352.89758077  7374.35905354  5581.9248274 ]]\n",
      "loss in cost function:14589621134.629587\n",
      "Validation Data****************************************\n",
      "theta:[[24352.89758077  7374.35905354  5581.9248274 ]]\n",
      "loss in cost function:15631000568.825132\n",
      "Training Data****************************************\n",
      "theta:[[25131.14376067  7607.14108417  5756.4843515 ]]\n",
      "loss in cost function:14451909724.201569\n",
      "Validation Data****************************************\n",
      "theta:[[25131.14376067  7607.14108417  5756.4843515 ]]\n",
      "loss in cost function:15486491720.641401\n",
      "Training Data****************************************\n",
      "theta:[[25905.49870968  7838.57554962  5929.92616673]]\n",
      "loss in cost function:14315605892.949669\n",
      "Validation Data****************************************\n",
      "theta:[[25905.49870968  7838.57554962  5929.92616673]]\n",
      "loss in cost function:15343449044.60497\n",
      "Training Data****************************************\n",
      "theta:[[26675.98188394  8068.67036365  6102.25727942]]\n",
      "loss in cost function:14180695188.118069\n",
      "Validation Data****************************************\n",
      "theta:[[26675.98188394  8068.67036365  6102.25727942]]\n",
      "loss in cost function:15201857534.157671\n",
      "Training Data****************************************\n",
      "theta:[[27442.61264233  8297.4333931   6273.48465252]]\n",
      "loss in cost function:14047163306.114305\n",
      "Validation Data****************************************\n",
      "theta:[[27442.61264233  8297.4333931   6273.48465252]]\n",
      "loss in cost function:15061702337.442078\n",
      "Training Data****************************************\n",
      "theta:[[28205.41024692  8524.87245814  6443.61520592]]\n",
      "loss in cost function:13914996090.960917\n",
      "Validation Data****************************************\n",
      "theta:[[28205.41024692  8524.87245814  6443.61520592]]\n",
      "loss in cost function:14922968755.695978\n",
      "Training Data****************************************\n",
      "theta:[[28964.3938635   8750.99533257  6612.65581666]]\n",
      "loss in cost function:13784179532.763023\n",
      "Validation Data****************************************\n",
      "theta:[[28964.3938635   8750.99533257  6612.65581666]]\n",
      "loss in cost function:14785642241.663942\n",
      "Training Data****************************************\n",
      "theta:[[29719.58256199  8975.80974408  6780.61331923]]\n",
      "loss in cost function:13654699766.192362\n",
      "Validation Data****************************************\n",
      "theta:[[29719.58256199  8975.80974408  6780.61331923]]\n",
      "loss in cost function:14649708398.025204\n",
      "Training Data****************************************\n",
      "theta:[[30470.99531699  9199.32337451  6947.49450581]]\n",
      "loss in cost function:13526543068.986729\n",
      "Validation Data****************************************\n",
      "theta:[[30470.99531699  9199.32337451  6947.49450581]]\n",
      "loss in cost function:14515152975.83825\n",
      "Training Data****************************************\n",
      "theta:[[31218.65100821  9421.54386018  7113.30612654]]\n",
      "loss in cost function:13399695860.465433\n",
      "Validation Data****************************************\n",
      "theta:[[31218.65100821  9421.54386018  7113.30612654]]\n",
      "loss in cost function:14381961873.0013\n",
      "Training Data****************************************\n",
      "theta:[[31962.56842098  9642.47879211  7278.05488977]]\n",
      "loss in cost function:13274144700.05994\n",
      "Validation Data****************************************\n",
      "theta:[[31962.56842098  9642.47879211  7278.05488977]]\n",
      "loss in cost function:14250121132.729294\n",
      "Training Data****************************************\n",
      "theta:[[32702.76624668  9862.13571629  7441.74746233]]\n",
      "loss in cost function:13149876285.86018\n",
      "Validation Data****************************************\n",
      "theta:[[32702.76624668  9862.13571629  7441.74746233]]\n",
      "loss in cost function:14119616942.04631\n",
      "Training Data****************************************\n",
      "theta:[[33439.26308326 10080.522134    7604.39046978]]\n",
      "loss in cost function:13026877453.175842\n",
      "Validation Data****************************************\n",
      "theta:[[33439.26308326 10080.522134    7604.39046978]]\n",
      "loss in cost function:13990435630.294107\n",
      "Training Data****************************************\n",
      "theta:[[34172.07743565 10297.64550203  7765.99049663]]\n",
      "loss in cost function:12905135173.112598\n",
      "Validation Data****************************************\n",
      "theta:[[34172.07743565 10297.64550203  7765.99049663]]\n",
      "loss in cost function:13862563667.656082\n",
      "Training Data****************************************\n",
      "theta:[[34901.22771628 10513.51323293  7926.55408667]]\n",
      "loss in cost function:12784636551.163498\n",
      "Validation Data****************************************\n",
      "theta:[[34901.22771628 10513.51323293  7926.55408667]]\n",
      "loss in cost function:13735987663.696556\n",
      "Training Data****************************************\n",
      "theta:[[35626.7322455  10728.13269535  8086.08774313]]\n",
      "loss in cost function:12665368825.814823\n",
      "Validation Data****************************************\n",
      "theta:[[35626.7322455  10728.13269535  8086.08774313]]\n",
      "loss in cost function:13610694365.915518\n",
      "Training Data****************************************\n",
      "theta:[[36348.60925208 10941.51121422  8244.59792899]]\n",
      "loss in cost function:12547319367.166552\n",
      "Validation Data****************************************\n",
      "theta:[[36348.60925208 10941.51121422  8244.59792899]]\n",
      "loss in cost function:13486670658.318214\n",
      "Training Data****************************************\n",
      "theta:[[37066.87687363 11153.65607106  8402.09106721]]\n",
      "loss in cost function:12430475675.567268\n",
      "Validation Data****************************************\n",
      "theta:[[37066.87687363 11153.65607106  8402.09106721]]\n",
      "loss in cost function:13363903559.999887\n",
      "Training Data****************************************\n",
      "theta:[[37781.55315707 11364.57450421  8558.57354098]]\n",
      "loss in cost function:12314825380.263275\n",
      "Validation Data****************************************\n",
      "theta:[[37781.55315707 11364.57450421  8558.57354098]]\n",
      "loss in cost function:13242380223.745144\n",
      "Training Data****************************************\n",
      "theta:[[38492.6560591  11574.27370914  8714.05169393]]\n",
      "loss in cost function:12200356238.06189\n",
      "Validation Data****************************************\n",
      "theta:[[38492.6560591  11574.27370914  8714.05169393]]\n",
      "loss in cost function:13122087934.641933\n",
      "Training Data****************************************\n",
      "theta:[[39200.20344661 11782.76083861  8868.53183043]]\n",
      "loss in cost function:12087056132.008512\n",
      "Validation Data****************************************\n",
      "theta:[[39200.20344661 11782.76083861  8868.53183043]]\n",
      "loss in cost function:13003014108.710087\n",
      "Training Data****************************************\n",
      "theta:[[39904.21309718 11990.04300305  9022.02021579]]\n",
      "loss in cost function:11974913070.077673\n",
      "Validation Data****************************************\n",
      "theta:[[39904.21309718 11990.04300305  9022.02021579]]\n",
      "loss in cost function:12885146291.544174\n",
      "Training Data****************************************\n",
      "theta:[[40604.70269951 12196.12727068  9174.52307653]]\n",
      "loss in cost function:11863915183.877728\n",
      "Validation Data****************************************\n",
      "theta:[[40604.70269951 12196.12727068  9174.52307653]]\n",
      "loss in cost function:12768472156.970316\n",
      "Training Data****************************************\n",
      "theta:[[41301.68985382 12401.02066788  9326.04660057]]\n",
      "loss in cost function:11754050727.36891\n",
      "Validation Data****************************************\n",
      "theta:[[41301.68985382 12401.02066788  9326.04660057]]\n",
      "loss in cost function:12652979505.717258\n",
      "Training Data****************************************\n",
      "theta:[[41995.19207236 12604.73017937  9476.59693753]]\n",
      "loss in cost function:11645308075.595003\n",
      "Validation Data****************************************\n",
      "theta:[[41995.19207236 12604.73017937  9476.59693753]]\n",
      "loss in cost function:12538656264.101204\n",
      "Training Data****************************************\n",
      "theta:[[42685.2267798  12807.26274846  9626.1801989 ]]\n",
      "loss in cost function:11537675723.427956\n",
      "Validation Data****************************************\n",
      "theta:[[42685.2267798  12807.26274846  9626.1801989 ]]\n",
      "loss in cost function:12425490482.724293\n",
      "Training Data****************************************\n",
      "theta:[[43371.81131371 13008.62527733  9774.80245834]]\n",
      "loss in cost function:11431142284.325882\n",
      "Validation Data****************************************\n",
      "theta:[[43371.81131371 13008.62527733  9774.80245834]]\n",
      "loss in cost function:12313470335.186848\n",
      "Training Data****************************************\n",
      "theta:[[44054.96292495 13208.82462727  9922.46975186]]\n",
      "loss in cost function:11325696489.103777\n",
      "Validation Data****************************************\n",
      "theta:[[44054.96292495 13208.82462727  9922.46975186]]\n",
      "loss in cost function:12202584116.812836\n",
      "Training Data****************************************\n",
      "theta:[[44734.69877813 13407.86761888 10069.18807807]]\n",
      "loss in cost function:11221327184.717257\n",
      "Validation Data****************************************\n",
      "theta:[[44734.69877813 13407.86761888 10069.18807807]]\n",
      "loss in cost function:12092820243.388716\n",
      "Training Data****************************************\n",
      "theta:[[45411.03595205 13605.76103238 10214.96339843]]\n",
      "loss in cost function:11118023333.058872\n",
      "Validation Data****************************************\n",
      "theta:[[45411.03595205 13605.76103238 10214.96339843]]\n",
      "loss in cost function:11984167249.91554\n",
      "Training Data****************************************\n",
      "theta:[[46083.9914401  13802.5116078  10359.80163745]]\n",
      "loss in cost function:11015774009.766745\n",
      "Validation Data****************************************\n",
      "theta:[[46083.9914401  13802.5116078  10359.80163745]]\n",
      "loss in cost function:11876613789.373857\n",
      "Training Data****************************************\n",
      "theta:[[46753.58215071 13998.12604523 10503.7086829 ]]\n",
      "loss in cost function:10914568403.046371\n",
      "Validation Data****************************************\n",
      "theta:[[46753.58215071 13998.12604523 10503.7086829 ]]\n",
      "loss in cost function:11770148631.501688\n",
      "Training Data****************************************\n",
      "theta:[[47419.82490776 14192.61100509 10646.69038611]]\n",
      "loss in cost function:10814395812.50368\n",
      "Validation Data****************************************\n",
      "theta:[[47419.82490776 14192.61100509 10646.69038611]]\n",
      "loss in cost function:11664760661.585104\n",
      "Training Data****************************************\n",
      "theta:[[48082.73645103 14385.97310831 10788.7525621 ]]\n",
      "loss in cost function:10715245647.991196\n",
      "Validation Data****************************************\n",
      "theta:[[48082.73645103 14385.97310831 10788.7525621 ]]\n",
      "loss in cost function:11560438879.261518\n",
      "Training Data****************************************\n",
      "theta:[[48742.33343658 14578.21893662 10929.90098989]]\n",
      "loss in cost function:10617107428.46554\n",
      "Validation Data****************************************\n",
      "theta:[[48742.33343658 14578.21893662 10929.90098989]]\n",
      "loss in cost function:11457172397.335373\n",
      "Training Data****************************************\n",
      "theta:[[49398.63243721 14769.35503275 11070.14141264]]\n",
      "loss in cost function:10519970780.857306\n",
      "Validation Data****************************************\n",
      "theta:[[49398.63243721 14769.35503275 11070.14141264]]\n",
      "loss in cost function:11354950440.606194\n",
      "Training Data****************************************\n",
      "theta:[[50051.64994283 14959.38790068 11209.47953795]]\n",
      "loss in cost function:10423825438.9525\n",
      "Validation Data****************************************\n",
      "theta:[[50051.64994283 14959.38790068 11209.47953795]]\n",
      "loss in cost function:11253762344.708902\n",
      "Training Data****************************************\n",
      "theta:[[50701.40236093 15148.32400586 11347.921038  ]]\n",
      "loss in cost function:10328661242.285622\n",
      "Validation Data****************************************\n",
      "theta:[[50701.40236093 15148.32400586 11347.921038  ]]\n",
      "loss in cost function:11153597554.966179\n",
      "Training Data****************************************\n",
      "theta:[[51347.90601693 15336.16977546 11485.47154984]]\n",
      "loss in cost function:10234468135.044285\n",
      "Validation Data****************************************\n",
      "theta:[[51347.90601693 15336.16977546 11485.47154984]]\n",
      "loss in cost function:11054445625.25275\n",
      "Training Data****************************************\n",
      "theta:[[51991.17715465 15522.93159856 11622.13667556]]\n",
      "loss in cost function:10141236164.9856\n",
      "Validation Data****************************************\n",
      "theta:[[51991.17715465 15522.93159856 11622.13667556]]\n",
      "loss in cost function:10956296216.87158\n",
      "Training Data****************************************\n",
      "theta:[[52631.23193669 15708.61582641 11757.9219825 ]]\n",
      "loss in cost function:10048955482.3632\n",
      "Validation Data****************************************\n",
      "theta:[[52631.23193669 15708.61582641 11757.9219825 ]]\n",
      "loss in cost function:10859139097.441755\n",
      "Training Data****************************************\n",
      "theta:[[53268.08644481 15893.22877266 11892.83300352]]\n",
      "loss in cost function:9957616338.865974\n",
      "Validation Data****************************************\n",
      "theta:[[53268.08644481 15893.22877266 11892.83300352]]\n",
      "loss in cost function:10762964139.797909\n",
      "Training Data****************************************\n",
      "theta:[[53901.7566804  16076.77671356 12026.87523714]]\n",
      "loss in cost function:9867209086.567715\n",
      "Validation Data****************************************\n",
      "theta:[[53901.7566804  16076.77671356 12026.87523714]]\n",
      "loss in cost function:10667761320.901163\n",
      "Training Data****************************************\n",
      "theta:[[54532.2585648  16259.26588817 12160.05414779]]\n",
      "loss in cost function:9777724176.887636\n",
      "Validation Data****************************************\n",
      "theta:[[54532.2585648  16259.26588817 12160.05414779]]\n",
      "loss in cost function:10573520720.761412\n",
      "Training Data****************************************\n",
      "theta:[[55159.60793979 16440.70249865 12292.37516603]]\n",
      "loss in cost function:9689152159.561768\n",
      "Validation Data****************************************\n",
      "theta:[[55159.60793979 16440.70249865 12292.37516603]]\n",
      "loss in cost function:10480232521.370802\n",
      "Training Data****************************************\n",
      "theta:[[55783.8205679  16621.09271041 12423.84368872]]\n",
      "loss in cost function:9601483681.625021\n",
      "Validation Data****************************************\n",
      "theta:[[55783.8205679  16621.09271041 12423.84368872]]\n",
      "loss in cost function:10387887005.648355\n",
      "Training Data****************************************\n",
      "theta:[[56404.91213287 16800.44265236 12554.46507926]]\n",
      "loss in cost function:9514709486.404078\n",
      "Validation Data****************************************\n",
      "theta:[[56404.91213287 16800.44265236 12554.46507926]]\n",
      "loss in cost function:10296474556.395533\n",
      "Training Data****************************************\n",
      "theta:[[57022.89824001 16978.75841711 12684.2446678 ]]\n",
      "loss in cost function:9428820412.520384\n",
      "Validation Data****************************************\n",
      "theta:[[57022.89824001 16978.75841711 12684.2446678 ]]\n",
      "loss in cost function:10205985655.262783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data****************************************\n",
      "theta:[[57637.79441662 17156.04606122 12813.18775139]]\n",
      "loss in cost function:9343807392.90383\n",
      "Validation Data****************************************\n",
      "theta:[[57637.79441662 17156.04606122 12813.18775139]]\n",
      "loss in cost function:10116410881.726715\n",
      "Training Data****************************************\n",
      "theta:[[58249.61611234 17332.31160539 12941.29959427]]\n",
      "loss in cost function:9259661453.81666\n",
      "Validation Data****************************************\n",
      "theta:[[58249.61611234 17332.31160539 12941.29959427]]\n",
      "loss in cost function:10027740912.078033\n",
      "Training Data****************************************\n",
      "theta:[[58858.37869959 17507.56103466 13068.58542798]]\n",
      "loss in cost function:9176373713.887428\n",
      "Validation Data****************************************\n",
      "theta:[[58858.37869959 17507.56103466 13068.58542798]]\n",
      "loss in cost function:9939966518.419928\n",
      "Training Data****************************************\n",
      "theta:[[59464.0974739  17681.80029866 13195.05045164]]\n",
      "loss in cost function:9093935383.15508\n",
      "Validation Data****************************************\n",
      "theta:[[59464.0974739  17681.80029866 13195.05045164]]\n",
      "loss in cost function:9853078567.676977\n",
      "Training Data****************************************\n",
      "theta:[[60066.78765434 17855.03531179 13320.69983209]]\n",
      "loss in cost function:9012337762.123053\n",
      "Validation Data****************************************\n",
      "theta:[[60066.78765434 17855.03531179 13320.69983209]]\n",
      "loss in cost function:9767068020.614313\n",
      "Training Data****************************************\n",
      "theta:[[60666.46438387 18027.27195346 13445.53870414]]\n",
      "loss in cost function:8931572240.8231\n",
      "Validation Data****************************************\n",
      "theta:[[60666.46438387 18027.27195346 13445.53870414]]\n",
      "loss in cost function:9681925930.866999\n",
      "Training Data****************************************\n",
      "theta:[[61263.14272976 18198.51606825 13569.5721707 ]]\n",
      "loss in cost function:8851630297.888987\n",
      "Validation Data****************************************\n",
      "theta:[[61263.14272976 18198.51606825 13569.5721707 ]]\n",
      "loss in cost function:9597643443.979525\n",
      "Training Data****************************************\n",
      "theta:[[61856.83768392 18368.77346618 13692.80530306]]\n",
      "loss in cost function:8772503499.639677\n",
      "Validation Data****************************************\n",
      "theta:[[61856.83768392 18368.77346618 13692.80530306]]\n",
      "loss in cost function:9514211796.45533\n",
      "Training Data****************************************\n",
      "theta:[[62447.56416331 18538.04992287 13815.24314102]]\n",
      "loss in cost function:8694183499.172314\n",
      "Validation Data****************************************\n",
      "theta:[[62447.56416331 18538.04992287 13815.24314102]]\n",
      "loss in cost function:9431622314.816122\n",
      "Training Data****************************************\n",
      "theta:[[63035.3370103  18706.35117975 13936.89069309]]\n",
      "loss in cost function:8616662035.464249\n",
      "Validation Data****************************************\n",
      "theta:[[63035.3370103  18706.35117975 13936.89069309]]\n",
      "loss in cost function:9349866414.671124\n",
      "Training Data****************************************\n",
      "theta:[[63620.17099306 18873.68294429 14057.75293673]]\n",
      "loss in cost function:8539930932.484835\n",
      "Validation Data****************************************\n",
      "theta:[[63620.17099306 18873.68294429 14057.75293673]]\n",
      "loss in cost function:9268935599.795904\n",
      "Training Data****************************************\n",
      "theta:[[64202.0808059  19040.05089017 14177.83481848]]\n",
      "loss in cost function:8463982098.316211\n",
      "Validation Data****************************************\n",
      "theta:[[64202.0808059  19040.05089017 14177.83481848]]\n",
      "loss in cost function:9188821461.2208\n",
      "Training Data****************************************\n",
      "theta:[[64781.08106968 19205.46065752 14297.1412542 ]]\n",
      "loss in cost function:8388807524.283363\n",
      "Validation Data****************************************\n",
      "theta:[[64781.08106968 19205.46065752 14297.1412542 ]]\n",
      "loss in cost function:9109515676.328936\n",
      "Training Data****************************************\n",
      "theta:[[65357.18633214 19369.91785307 14415.67712921]]\n",
      "loss in cost function:8314399284.093026\n",
      "Validation Data****************************************\n",
      "theta:[[65357.18633214 19369.91785307 14415.67712921]]\n",
      "loss in cost function:9031010007.963436\n",
      "Training Data****************************************\n",
      "theta:[[65930.41106829 19533.42805041 14533.44729852]]\n",
      "loss in cost function:8240749532.981833\n",
      "Validation Data****************************************\n",
      "theta:[[65930.41106829 19533.42805041 14533.44729852]]\n",
      "loss in cost function:8953296303.544075\n",
      "Training Data****************************************\n",
      "theta:[[66500.76968076 19695.99679011 14650.45658702]]\n",
      "loss in cost function:8167850506.873176\n",
      "Validation Data****************************************\n",
      "theta:[[66500.76968076 19695.99679011 14650.45658702]]\n",
      "loss in cost function:8876366494.193094\n",
      "Training Data****************************************\n",
      "theta:[[67068.27650016 19857.62958    14766.70978961]]\n",
      "loss in cost function:8095694521.5428505\n",
      "Validation Data****************************************\n",
      "theta:[[67068.27650016 19857.62958    14766.70978961]]\n",
      "loss in cost function:8800212593.870115\n",
      "Training Data****************************************\n",
      "theta:[[67632.94578547 20018.33189531 14882.21167144]]\n",
      "loss in cost function:8024273971.793224\n",
      "Validation Data****************************************\n",
      "theta:[[67632.94578547 20018.33189531 14882.21167144]]\n",
      "loss in cost function:8724826698.515936\n",
      "Training Data****************************************\n",
      "theta:[[68194.79172435 20178.10917888 14996.96696807]]\n",
      "loss in cost function:7953581330.636463\n",
      "Validation Data****************************************\n",
      "theta:[[68194.79172435 20178.10917888 14996.96696807]]\n",
      "loss in cost function:8650200985.205397\n",
      "Training Data****************************************\n",
      "theta:[[68753.82843353 20336.96684135 15110.98038566]]\n",
      "loss in cost function:7883609148.485647\n",
      "Validation Data****************************************\n",
      "theta:[[68753.82843353 20336.96684135 15110.98038566]]\n",
      "loss in cost function:8576327711.309005\n",
      "Training Data****************************************\n",
      "theta:[[69310.06995918 20494.91026137 15224.25660115]]\n",
      "loss in cost function:7814350052.354734\n",
      "Validation Data****************************************\n",
      "theta:[[69310.06995918 20494.91026137 15224.25660115]]\n",
      "loss in cost function:8503199213.663195\n",
      "Training Data****************************************\n",
      "theta:[[69863.53027719 20651.94478576 15336.80026241]]\n",
      "loss in cost function:7745796745.066761\n",
      "Validation Data****************************************\n",
      "theta:[[69863.53027719 20651.94478576 15336.80026241]]\n",
      "loss in cost function:8430807907.749341\n",
      "Training Data****************************************\n",
      "theta:[[70414.22329361 20808.07572972 15448.61598849]]\n",
      "loss in cost function:7677942004.470062\n",
      "Validation Data****************************************\n",
      "theta:[[70414.22329361 20808.07572972 15448.61598849]]\n",
      "loss in cost function:8359146286.881194\n",
      "Training Data****************************************\n",
      "theta:[[70962.16284495 20963.30837702 15559.70836971]]\n",
      "loss in cost function:7610778682.6629305\n",
      "Validation Data****************************************\n",
      "theta:[[70962.16284495 20963.30837702 15559.70836971]]\n",
      "loss in cost function:8288206921.400886\n",
      "Training Data****************************************\n",
      "theta:[[71507.36269853 21117.64798019 15670.08196791]]\n",
      "loss in cost function:7544299705.226176\n",
      "Validation Data****************************************\n",
      "theta:[[71507.36269853 21117.64798019 15670.08196791]]\n",
      "loss in cost function:8217982457.883163\n",
      "Training Data****************************************\n",
      "theta:[[72049.83655285 21271.09976067 15779.74131657]]\n",
      "loss in cost function:7478498070.463674\n",
      "Validation Data****************************************\n",
      "theta:[[72049.83655285 21271.09976067 15779.74131657]]\n",
      "loss in cost function:8148465618.348001\n",
      "Training Data****************************************\n",
      "theta:[[72589.59803789 21423.66890904 15888.69092102]]\n",
      "loss in cost function:7413366848.650798\n",
      "Validation Data****************************************\n",
      "theta:[[72589.59803789 21423.66890904 15888.69092102]]\n",
      "loss in cost function:8079649199.481357\n",
      "Training Data****************************************\n",
      "theta:[[73126.66071551 21575.36058519 15996.93525862]]\n",
      "loss in cost function:7348899181.290833\n",
      "Validation Data****************************************\n",
      "theta:[[73126.66071551 21575.36058519 15996.93525862]]\n",
      "loss in cost function:8011526071.864005\n",
      "Training Data****************************************\n",
      "theta:[[73661.03807974 21726.17991851 16104.47877888]]\n",
      "loss in cost function:7285088280.378824\n",
      "Validation Data****************************************\n",
      "theta:[[73661.03807974 21726.17991851 16104.47877888]]\n",
      "loss in cost function:7944089179.208425\n",
      "Training Data****************************************\n",
      "theta:[[74192.74355715 21876.13200802 16211.32590369]]\n",
      "loss in cost function:7221927427.673338\n",
      "Validation Data****************************************\n",
      "theta:[[74192.74355715 21876.13200802 16211.32590369]]\n",
      "loss in cost function:7877331537.603621\n",
      "Training Data****************************************\n",
      "theta:[[74721.79050717 22025.22192265 16317.48102747]]\n",
      "loss in cost function:7159409973.975652\n",
      "Validation Data****************************************\n",
      "theta:[[74721.79050717 22025.22192265 16317.48102747]]\n",
      "loss in cost function:7811246234.767722\n",
      "Training Data****************************************\n",
      "theta:[[75248.19222245 22173.45470131 16422.9485173 ]]\n",
      "loss in cost function:7097529338.416498\n",
      "Validation Data****************************************\n",
      "theta:[[75248.19222245 22173.45470131 16422.9485173 ]]\n",
      "loss in cost function:7745826429.3085\n",
      "Training Data****************************************\n",
      "theta:[[75771.96192914 22320.83535315 16527.73271317]]\n",
      "loss in cost function:7036279007.750186\n",
      "Validation Data****************************************\n",
      "theta:[[75771.96192914 22320.83535315 16527.73271317]]\n",
      "loss in cost function:7681065349.991358\n",
      "Training Data****************************************\n",
      "theta:[[76293.11278731 22467.3688577  16631.83792806]]\n",
      "loss in cost function:6975652535.656001\n",
      "Validation Data****************************************\n",
      "theta:[[76293.11278731 22467.3688577  16631.83792806]]\n",
      "loss in cost function:7616956295.015108\n",
      "Training Data****************************************\n",
      "theta:[[76811.65789118 22613.06016507 16735.26844817]]\n",
      "loss in cost function:6915643542.047083\n",
      "Validation Data****************************************\n",
      "theta:[[76811.65789118 22613.06016507 16735.26844817]]\n",
      "loss in cost function:7553492631.295203\n",
      "Training Data****************************************\n",
      "theta:[[77327.61026953 22757.91419608 16838.02853306]]\n",
      "loss in cost function:6856245712.386185\n",
      "Validation Data****************************************\n",
      "theta:[[77327.61026953 22757.91419608 16838.02853306]]\n",
      "loss in cost function:7490667793.754415\n",
      "Training Data****************************************\n",
      "theta:[[77840.98288599 22901.93584248 16940.1224158 ]]\n",
      "loss in cost function:6797452797.008762\n",
      "Validation Data****************************************\n",
      "theta:[[77840.98288599 22901.93584248 16940.1224158 ]]\n",
      "loss in cost function:7428475284.620827\n",
      "Training Data****************************************\n",
      "theta:[[78351.78863937 23045.12996712 17041.55430317]]\n",
      "loss in cost function:6739258610.453038\n",
      "Validation Data****************************************\n",
      "theta:[[78351.78863937 23045.12996712 17041.55430317]]\n",
      "loss in cost function:7366908672.733308\n",
      "Training Data****************************************\n",
      "theta:[[78860.04036398 23187.50140409 17142.32837577]]\n",
      "loss in cost function:6681657030.796937\n",
      "Validation Data****************************************\n",
      "theta:[[78860.04036398 23187.50140409 17142.32837577]]\n",
      "loss in cost function:7305961592.853927\n",
      "Training Data****************************************\n",
      "theta:[[79365.75082997 23329.05495892 17242.44878824]]\n",
      "loss in cost function:6624641999.002004\n",
      "Validation Data****************************************\n",
      "theta:[[79365.75082997 23329.05495892 17242.44878824]]\n",
      "loss in cost function:7245627744.987725\n",
      "Training Data****************************************\n",
      "theta:[[79868.93274363 23469.79540874 17341.91966937]]\n",
      "loss in cost function:6568207518.264189\n",
      "Validation Data****************************************\n",
      "theta:[[79868.93274363 23469.79540874 17341.91966937]]\n",
      "loss in cost function:7185900893.709475\n",
      "Training Data****************************************\n",
      "theta:[[80369.59874772 23609.72750245 17440.7451223 ]]\n",
      "loss in cost function:6512347653.371201\n",
      "Validation Data****************************************\n",
      "theta:[[80369.59874772 23609.72750245 17440.7451223 ]]\n",
      "loss in cost function:7126774867.497414\n",
      "Training Data****************************************\n",
      "theta:[[80867.76142179 23748.85596089 17538.92922464]]\n",
      "loss in cost function:6457056530.066651\n",
      "Validation Data****************************************\n",
      "theta:[[80867.76142179 23748.85596089 17538.92922464]]\n",
      "loss in cost function:7068243558.0739355\n",
      "Training Data****************************************\n",
      "theta:[[81363.43328249 23887.18547701 17636.47602866]]\n",
      "loss in cost function:6402328334.4208145\n",
      "Validation Data****************************************\n",
      "theta:[[81363.43328249 23887.18547701 17636.47602866]]\n",
      "loss in cost function:7010300919.753094\n",
      "Training Data****************************************\n",
      "theta:[[81856.62678388 24024.72071601 17733.38956141]]\n",
      "loss in cost function:6348157312.207895\n",
      "Validation Data****************************************\n",
      "theta:[[81856.62678388 24024.72071601 17733.38956141]]\n",
      "loss in cost function:6952940968.79491\n",
      "Training Data****************************************\n",
      "theta:[[82347.35431777 24161.46631556 17829.67382491]]\n",
      "loss in cost function:6294537768.28966\n",
      "Validation Data****************************************\n",
      "theta:[[82347.35431777 24161.46631556 17829.67382491]]\n",
      "loss in cost function:6896157782.76636\n",
      "Training Data****************************************\n",
      "theta:[[82835.62821399 24297.42688591 17925.3327963 ]]\n",
      "loss in cost function:6241464066.005708\n",
      "Validation Data****************************************\n",
      "theta:[[82835.62821399 24297.42688591 17925.3327963 ]]\n",
      "loss in cost function:6839945499.909014\n",
      "Training Data****************************************\n",
      "theta:[[83321.46074073 24432.60701008 18020.37042798]]\n",
      "loss in cost function:6188930626.569823\n",
      "Validation Data****************************************\n",
      "theta:[[83321.46074073 24432.60701008 18020.37042798]]\n",
      "loss in cost function:6784298318.51321\n",
      "Training Data****************************************\n",
      "theta:[[83804.86410483 24567.01124402 18114.79064774]]\n",
      "loss in cost function:6136931928.4727\n",
      "Validation Data****************************************\n",
      "theta:[[83804.86410483 24567.01124402 18114.79064774]]\n",
      "loss in cost function:6729210496.298797\n",
      "Training Data****************************************\n",
      "theta:[[84285.85045212 24700.64411675 18208.59735897]]\n",
      "loss in cost function:6085462506.890924\n",
      "Validation Data****************************************\n",
      "theta:[[84285.85045212 24700.64411675 18208.59735897]]\n",
      "loss in cost function:6674676349.802167\n",
      "Training Data****************************************\n",
      "theta:[[84764.43186766 24833.51013057 18301.79444077]]\n",
      "loss in cost function:6034516953.101981\n",
      "Validation Data****************************************\n",
      "theta:[[84764.43186766 24833.51013057 18301.79444077]]\n",
      "loss in cost function:6620690253.769836\n",
      "Training Data****************************************\n",
      "theta:[[85240.62037613 24965.61376115 18394.38574809]]\n",
      "loss in cost function:5984089913.90536\n",
      "Validation Data****************************************\n",
      "theta:[[85240.62037613 24965.61376115 18394.38574809]]\n",
      "loss in cost function:6567246640.5581455\n",
      "Training Data****************************************\n",
      "theta:[[85714.42794206 25096.95945776 18486.37511193]]\n",
      "loss in cost function:5934176091.049807\n",
      "Validation Data****************************************\n",
      "theta:[[85714.42794206 25096.95945776 18486.37511193]]\n",
      "loss in cost function:6514339999.539297\n",
      "Training Data****************************************\n",
      "theta:[[86185.86647016 25227.55164337 18577.76633941]]\n",
      "loss in cost function:5884770240.666251\n",
      "Validation Data****************************************\n",
      "theta:[[86185.86647016 25227.55164337 18577.76633941]]\n",
      "loss in cost function:6461964876.5134945\n",
      "Training Data****************************************\n",
      "theta:[[86654.94780562 25357.39471484 18668.56321401]]\n",
      "loss in cost function:5835867172.70684\n",
      "Validation Data****************************************\n",
      "theta:[[86654.94780562 25357.39471484 18668.56321401]]\n",
      "loss in cost function:6410115873.127262\n",
      "Training Data****************************************\n",
      "theta:[[87121.6837344  25486.49304306 18758.76949562]]\n",
      "loss in cost function:5787461750.389734\n",
      "Validation Data****************************************\n",
      "theta:[[87121.6837344  25486.49304306 18758.76949562]]\n",
      "loss in cost function:6358787646.297682\n",
      "Training Data****************************************\n",
      "theta:[[87586.08598353 25614.85097312 18848.38892076]]\n",
      "loss in cost function:5739548889.64963\n",
      "Validation Data****************************************\n",
      "theta:[[87586.08598353 25614.85097312 18848.38892076]]\n",
      "loss in cost function:6307974907.642728\n",
      "Training Data****************************************\n",
      "theta:[[88048.16622142 25742.47282444 18937.42520269]]\n",
      "loss in cost function:5692123558.593991\n",
      "Validation Data****************************************\n",
      "theta:[[88048.16622142 25742.47282444 18937.42520269]]\n",
      "loss in cost function:6257672422.917367\n",
      "Training Data****************************************\n",
      "theta:[[88507.93605812 25869.36289094 19025.88203157]]\n",
      "loss in cost function:5645180776.964985\n",
      "Validation Data****************************************\n",
      "theta:[[88507.93605812 25869.36289094 19025.88203157]]\n",
      "loss in cost function:6207875011.455705\n",
      "Training Data****************************************\n",
      "theta:[[88965.40704564 25995.52544119 19113.76307456]]\n",
      "loss in cost function:5598715615.606882\n",
      "Validation Data****************************************\n",
      "theta:[[88965.40704564 25995.52544119 19113.76307456]]\n",
      "loss in cost function:6158577545.618705\n",
      "Training Data****************************************\n",
      "theta:[[89420.59067822 26120.96471856 19201.07197603]]\n",
      "loss in cost function:5552723195.939072\n",
      "Validation Data****************************************\n",
      "theta:[[89420.59067822 26120.96471856 19201.07197603]]\n",
      "loss in cost function:6109774950.247802\n",
      "Training Data****************************************\n",
      "theta:[[89873.49839264 26245.68494137 19287.81235764]]\n",
      "loss in cost function:5507198689.434536\n",
      "Validation Data****************************************\n",
      "theta:[[89873.49839264 26245.68494137 19287.81235764]]\n",
      "loss in cost function:6061462202.124073\n",
      "Training Data****************************************\n",
      "theta:[[90324.14156848 26369.69030304 19373.98781851]]\n",
      "loss in cost function:5462137317.103711\n",
      "Validation Data****************************************\n",
      "theta:[[90324.14156848 26369.69030304 19373.98781851]]\n",
      "loss in cost function:6013634329.433083\n",
      "Training Data****************************************\n",
      "theta:[[90772.53152845 26492.98497222 19459.60193535]]\n",
      "loss in cost function:5417534348.98366\n",
      "Validation Data****************************************\n",
      "theta:[[90772.53152845 26492.98497222 19459.60193535]]\n",
      "loss in cost function:5966286411.235265\n",
      "Training Data****************************************\n",
      "theta:[[91218.67953862 26615.57309297 19544.65826261]]\n",
      "loss in cost function:5373385103.632648\n",
      "Validation Data****************************************\n",
      "theta:[[91218.67953862 26615.57309297 19544.65826261]]\n",
      "loss in cost function:5919413576.941743\n",
      "Training Data****************************************\n",
      "theta:[[91662.59680873 26737.45878489 19629.1603326 ]]\n",
      "loss in cost function:5329684947.6298485\n",
      "Validation Data****************************************\n",
      "theta:[[91662.59680873 26737.45878489 19629.1603326 ]]\n",
      "loss in cost function:5873011005.7956915\n",
      "Training Data****************************************\n",
      "theta:[[92104.2944925  26858.64614326 19713.11165563]]\n",
      "loss in cost function:5286429295.080327\n",
      "Validation Data****************************************\n",
      "theta:[[92104.2944925  26858.64614326 19713.11165563]]\n",
      "loss in cost function:5827073926.358936\n",
      "Training Data****************************************\n",
      "theta:[[92543.78368784 26979.13923919 19796.51572016]]\n",
      "loss in cost function:5243613607.125066\n",
      "Validation Data****************************************\n",
      "theta:[[92543.78368784 26979.13923919 19796.51572016]]\n",
      "loss in cost function:5781597616.004027\n",
      "Training Data****************************************\n",
      "theta:[[92981.07543721 27098.94211977 19879.37599292]]\n",
      "loss in cost function:5201233391.456149\n",
      "Validation Data****************************************\n",
      "theta:[[92981.07543721 27098.94211977 19879.37599292]]\n",
      "loss in cost function:5736577400.411452\n",
      "Training Data****************************************\n",
      "theta:[[93416.18072783 27218.05880819 19961.69591905]]\n",
      "loss in cost function:5159284201.836959\n",
      "Validation Data****************************************\n",
      "theta:[[93416.18072783 27218.05880819 19961.69591905]]\n",
      "loss in cost function:5692008653.072118\n",
      "Training Data****************************************\n",
      "theta:[[93849.110492   27336.49330393 20043.47892223]]\n",
      "loss in cost function:5117761637.627289\n",
      "Validation Data****************************************\n",
      "theta:[[93849.110492   27336.49330393 20043.47892223]]\n",
      "loss in cost function:5647886794.794992\n",
      "Training Data****************************************\n",
      "theta:[[94279.87560735 27454.24958284 20124.72840481]]\n",
      "loss in cost function:5076661343.313428\n",
      "Validation Data****************************************\n",
      "theta:[[94279.87560735 27454.24958284 20124.72840481]]\n",
      "loss in cost function:5604207293.219841\n",
      "Training Data****************************************\n",
      "theta:[[94708.48689712 27571.33159733 20205.44774794]]\n",
      "loss in cost function:5035979008.0431385\n",
      "Validation Data****************************************\n",
      "theta:[[94708.48689712 27571.33159733 20205.44774794]]\n",
      "loss in cost function:5560965662.334984\n",
      "Training Data****************************************\n",
      "theta:[[95134.95513044 27687.74327648 20285.64031173]]\n",
      "loss in cost function:4995710365.165336\n",
      "Validation Data****************************************\n",
      "theta:[[95134.95513044 27687.74327648 20285.64031173]]\n",
      "loss in cost function:5518157462.000115\n",
      "Training Data****************************************\n",
      "theta:[[95559.2910226  27803.4885262  20365.30943534]]\n",
      "loss in cost function:4955851191.774752\n",
      "Validation Data****************************************\n",
      "theta:[[95559.2910226  27803.4885262  20365.30943534]]\n",
      "loss in cost function:5475778297.473991\n",
      "Training Data****************************************\n",
      "theta:[[95981.5052353  27918.57122935 20444.45843711]]\n",
      "loss in cost function:4916397308.261044\n",
      "Validation Data****************************************\n",
      "theta:[[95981.5052353  27918.57122935 20444.45843711]]\n",
      "loss in cost function:5433823818.947074\n",
      "Training Data****************************************\n",
      "theta:[[96401.60837693 28032.99524589 20523.09061474]]\n",
      "loss in cost function:4877344577.862869\n",
      "Validation Data****************************************\n",
      "theta:[[96401.60837693 28032.99524589 20523.09061474]]\n",
      "loss in cost function:5392289721.078981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data****************************************\n",
      "theta:[[96819.61100285 28146.76441299 20601.20924533]]\n",
      "loss in cost function:4838688906.2262535\n",
      "Validation Data****************************************\n",
      "theta:[[96819.61100285 28146.76441299 20601.20924533]]\n",
      "loss in cost function:5351171742.540731\n",
      "Training Data****************************************\n",
      "theta:[[97235.52361564 28259.88254522 20678.81758561]]\n",
      "loss in cost function:4800426240.967896\n",
      "Validation Data****************************************\n",
      "theta:[[97235.52361564 28259.88254522 20678.81758561]]\n",
      "loss in cost function:5310465665.561759\n",
      "Training Data****************************************\n",
      "theta:[[97649.35666537 28372.35343463 20755.91887198]]\n",
      "loss in cost function:4762552571.242659\n",
      "Validation Data****************************************\n",
      "theta:[[97649.35666537 28372.35343463 20755.91887198]]\n",
      "loss in cost function:5270167315.481588\n",
      "Training Data****************************************\n",
      "theta:[[98061.12054986 28484.18085091 20832.51632068]]\n",
      "loss in cost function:4725063927.315776\n",
      "Validation Data****************************************\n",
      "theta:[[98061.12054986 28484.18085091 20832.51632068]]\n",
      "loss in cost function:5230272560.306151\n",
      "Training Data****************************************\n",
      "theta:[[98470.82561491 28595.3685415  20908.6131279 ]]\n",
      "loss in cost function:4687956380.139395\n",
      "Validation Data****************************************\n",
      "theta:[[98470.82561491 28595.3685415  20908.6131279 ]]\n",
      "loss in cost function:5190777310.268739\n",
      "Training Data****************************************\n",
      "theta:[[98878.48215465 28705.92023178 20984.2124699 ]]\n",
      "loss in cost function:4651226040.933497\n",
      "Validation Data****************************************\n",
      "theta:[[98878.48215465 28705.92023178 20984.2124699 ]]\n",
      "loss in cost function:5151677517.39548\n",
      "Training Data****************************************\n",
      "theta:[[99284.10041168 28815.83962512 21059.31750314]]\n",
      "loss in cost function:4614869060.771176\n",
      "Validation Data****************************************\n",
      "theta:[[99284.10041168 28815.83962512 21059.31750314]]\n",
      "loss in cost function:5112969175.075257\n",
      "Training Data****************************************\n",
      "theta:[[99687.69057743 28925.13040307 21133.93136443]]\n",
      "loss in cost function:4578881630.168248\n",
      "Validation Data****************************************\n",
      "theta:[[99687.69057743 28925.13040307 21133.93136443]]\n",
      "loss in cost function:5074648317.63419\n",
      "Training Data****************************************\n",
      "theta:[[100089.26279235  29033.79622549  21208.05717098]]\n",
      "loss in cost function:4543259978.67706\n",
      "Validation Data****************************************\n",
      "theta:[[100089.26279235  29033.79622549  21208.05717098]]\n",
      "loss in cost function:5036711019.914454\n",
      "Training Data****************************************\n",
      "theta:[[100488.8271462   29141.84073062  21281.69802059]]\n",
      "loss in cost function:4508000374.4844675\n",
      "Validation Data****************************************\n",
      "theta:[[100488.8271462   29141.84073062  21281.69802059]]\n",
      "loss in cost function:4999153396.857456\n",
      "Training Data****************************************\n",
      "theta:[[100886.39367828  29249.26753529  21354.85699174]]\n",
      "loss in cost function:4473099124.014141\n",
      "Validation Data****************************************\n",
      "theta:[[100886.39367828  29249.26753529  21354.85699174]]\n",
      "loss in cost function:4961971603.091359\n",
      "Training Data****************************************\n",
      "theta:[[101281.97237769  29356.08023497  21427.5371437 ]]\n",
      "loss in cost function:4438552571.532766\n",
      "Validation Data****************************************\n",
      "theta:[[101281.97237769  29356.08023497  21427.5371437 ]]\n",
      "loss in cost function:4925161832.522848\n",
      "Training Data****************************************\n",
      "theta:[[101675.57318361  29462.28240397  21499.74151667]]\n",
      "loss in cost function:4404357098.760486\n",
      "Validation Data****************************************\n",
      "theta:[[101675.57318361  29462.28240397  21499.74151667]]\n",
      "loss in cost function:4888720317.933146\n",
      "Training Data****************************************\n",
      "theta:[[102067.2059855   29567.87759551  21571.47313189]]\n",
      "loss in cost function:4370509124.48533\n",
      "Validation Data****************************************\n",
      "theta:[[102067.2059855   29567.87759551  21571.47313189]]\n",
      "loss in cost function:4852643330.578183\n",
      "Training Data****************************************\n",
      "theta:[[102456.88062339  29672.86934187  21642.73499175]]\n",
      "loss in cost function:4337005104.181603\n",
      "Validation Data****************************************\n",
      "theta:[[102456.88062339  29672.86934187  21642.73499175]]\n",
      "loss in cost function:4816927179.792946\n",
      "Training Data****************************************\n",
      "theta:[[102844.60688808  29777.2611545   21713.53007993]]\n",
      "loss in cost function:4303841529.632228\n",
      "Validation Data****************************************\n",
      "theta:[[102844.60688808  29777.2611545   21713.53007993]]\n",
      "loss in cost function:4781568212.599863\n",
      "Training Data****************************************\n",
      "theta:[[103230.39452144  29881.05652416  21783.86136146]]\n",
      "loss in cost function:4271014928.5550747\n",
      "Validation Data****************************************\n",
      "theta:[[103230.39452144  29881.05652416  21783.86136146]]\n",
      "loss in cost function:4746562813.321305\n",
      "Training Data****************************************\n",
      "theta:[[103614.25321665  29984.25892105  21853.7317829 ]]\n",
      "loss in cost function:4238521864.233044\n",
      "Validation Data****************************************\n",
      "theta:[[103614.25321665  29984.25892105  21853.7317829 ]]\n",
      "loss in cost function:4711907403.196049\n",
      "Training Data****************************************\n",
      "theta:[[103996.19261837  30086.8717949   21923.14427243]]\n",
      "loss in cost function:4206358935.1480637\n",
      "Validation Data****************************************\n",
      "theta:[[103996.19261837  30086.8717949   21923.14427243]]\n",
      "loss in cost function:4677598439.999729\n",
      "Training Data****************************************\n",
      "theta:[[104376.22232309  30188.89857512  21992.10173995]]\n",
      "loss in cost function:4174522774.61881\n",
      "Validation Data****************************************\n",
      "theta:[[104376.22232309  30188.89857512  21992.10173995]]\n",
      "loss in cost function:4643632417.669231\n",
      "Training Data****************************************\n",
      "theta:[[104754.35187928  30290.34267091  22060.60707719]]\n",
      "loss in cost function:4143010050.4422684\n",
      "Validation Data****************************************\n",
      "theta:[[104754.35187928  30290.34267091  22060.60707719]]\n",
      "loss in cost function:4610005865.930926\n",
      "Training Data****************************************\n",
      "theta:[[105130.59078769  30391.20747138  22128.66315785]]\n",
      "loss in cost function:4111817464.538866\n",
      "Validation Data****************************************\n",
      "theta:[[105130.59078769  30391.20747138  22128.66315785]]\n",
      "loss in cost function:4576715349.932801\n",
      "Training Data****************************************\n",
      "theta:[[105504.94850156  30491.49634567  22196.27283771]]\n",
      "loss in cost function:4080941752.6014214\n",
      "Validation Data****************************************\n",
      "theta:[[105504.94850156  30491.49634567  22196.27283771]]\n",
      "loss in cost function:4543757469.880364\n",
      "Training Data****************************************\n",
      "theta:[[105877.43442686  30591.21264307  22263.43895468]]\n",
      "loss in cost function:4050379683.747624\n",
      "Validation Data****************************************\n",
      "theta:[[105877.43442686  30591.21264307  22263.43895468]]\n",
      "loss in cost function:4511128860.676354\n",
      "Training Data****************************************\n",
      "theta:[[106248.05792254  30690.35969315  22330.16432901]]\n",
      "loss in cost function:4020128060.176142\n",
      "Validation Data****************************************\n",
      "theta:[[106248.05792254  30690.35969315  22330.16432901]]\n",
      "loss in cost function:4478826191.564063\n",
      "Training Data****************************************\n",
      "theta:[[106616.82830073  30788.94080584  22396.45176331]]\n",
      "loss in cost function:3990183716.826309\n",
      "Validation Data****************************************\n",
      "theta:[[106616.82830073  30788.94080584  22396.45176331]]\n",
      "loss in cost function:4446846165.77456\n",
      "Training Data****************************************\n",
      "theta:[[106983.75482704  30886.9592716   22462.30404271]]\n",
      "loss in cost function:3960543521.041274\n",
      "Validation Data****************************************\n",
      "theta:[[106983.75482704  30886.9592716   22462.30404271]]\n",
      "loss in cost function:4415185520.177317\n",
      "Training Data****************************************\n",
      "theta:[[107348.84672071  30984.41836148  22527.72393494]]\n",
      "loss in cost function:3931204372.2347093\n",
      "Validation Data****************************************\n",
      "theta:[[107348.84672071  30984.41836148  22527.72393494]]\n",
      "loss in cost function:4383841024.934642\n",
      "Training Data****************************************\n",
      "theta:[[107712.11315491  31081.32132728  22592.71419048]]\n",
      "loss in cost function:3902163201.5609217\n",
      "Validation Data****************************************\n",
      "theta:[[107712.11315491  31081.32132728  22592.71419048]]\n",
      "loss in cost function:4352809483.159577\n",
      "Training Data****************************************\n",
      "theta:[[108073.56325695  31177.67140165  22657.27754259]]\n",
      "loss in cost function:3873416971.588342\n",
      "Validation Data****************************************\n",
      "theta:[[108073.56325695  31177.67140165  22657.27754259]]\n",
      "loss in cost function:4322087730.577386\n",
      "Training Data****************************************\n",
      "theta:[[108433.20610847  31273.4717982   22721.41670749]]\n",
      "loss in cost function:3844962675.9764895\n",
      "Validation Data****************************************\n",
      "theta:[[108433.20610847  31273.4717982   22721.41670749]]\n",
      "loss in cost function:4291672635.1904764\n",
      "Training Data****************************************\n",
      "theta:[[108791.05074574  31368.72571159  22785.13438446]]\n",
      "loss in cost function:3816797339.1561623\n",
      "Validation Data****************************************\n",
      "theta:[[108791.05074574  31368.72571159  22785.13438446]]\n",
      "loss in cost function:4261561096.9469185\n",
      "Training Data****************************************\n",
      "theta:[[109147.10615982  31463.43631769  22848.43325587]]\n",
      "loss in cost function:3788918016.0130196\n",
      "Validation Data****************************************\n",
      "theta:[[109147.10615982  31463.43631769  22848.43325587]]\n",
      "loss in cost function:4231750047.4122195\n",
      "Training Data****************************************\n",
      "theta:[[109501.38129683  31557.60677368  22911.31598737]]\n",
      "loss in cost function:3761321791.5743976\n",
      "Validation Data****************************************\n",
      "theta:[[109501.38129683  31557.60677368  22911.31598737]]\n",
      "loss in cost function:4202236449.4446135\n",
      "Training Data****************************************\n",
      "theta:[[109853.88505815  31651.24021812  22973.78522796]]\n",
      "loss in cost function:3734005780.699379\n",
      "Validation Data****************************************\n",
      "theta:[[109853.88505815  31651.24021812  22973.78522796]]\n",
      "loss in cost function:4173017296.8736696\n",
      "Training Data****************************************\n",
      "theta:[[110204.62630067  31744.33977111  23035.84361009]]\n",
      "loss in cost function:3706967127.772022\n",
      "Validation Data****************************************\n",
      "theta:[[110204.62630067  31744.33977111  23035.84361009]]\n",
      "loss in cost function:4144089614.182229\n",
      "Training Data****************************************\n",
      "theta:[[110553.61383697  31836.90853436  23097.49374976]]\n",
      "loss in cost function:3680203006.397853\n",
      "Validation Data****************************************\n",
      "theta:[[110553.61383697  31836.90853436  23097.49374976]]\n",
      "loss in cost function:4115450456.1916027\n",
      "Training Data****************************************\n",
      "theta:[[110900.85643559  31928.94959135  23158.73824663]]\n",
      "loss in cost function:3653710619.1033983\n",
      "Validation Data****************************************\n",
      "theta:[[110900.85643559  31928.94959135  23158.73824663]]\n",
      "loss in cost function:4087096907.75006\n",
      "Training Data****************************************\n",
      "theta:[[111246.36282123  32020.46600739  23219.5796841 ]]\n",
      "loss in cost function:3627487197.0388675\n",
      "Validation Data****************************************\n",
      "theta:[[111246.36282123  32020.46600739  23219.5796841 ]]\n",
      "loss in cost function:4059026083.424512\n",
      "Training Data****************************************\n",
      "theta:[[111590.14167493  32111.46082973  23280.02062947]]\n",
      "loss in cost function:3601529999.6839013\n",
      "Validation Data****************************************\n",
      "theta:[[111590.14167493  32111.46082973  23280.02062947]]\n",
      "loss in cost function:4031235127.1953826\n",
      "Training Data****************************************\n",
      "theta:[[111932.20163436  32201.93708771  23340.06363396]]\n",
      "loss in cost function:3575836314.556396\n",
      "Validation Data****************************************\n",
      "theta:[[111932.20163436  32201.93708771  23340.06363396]]\n",
      "loss in cost function:4003721212.154642\n",
      "Training Data****************************************\n",
      "theta:[[112272.551294    32291.89779282  23399.71123286]]\n",
      "loss in cost function:3550403456.924268\n",
      "Validation Data****************************************\n",
      "theta:[[112272.551294    32291.89779282  23399.71123286]]\n",
      "loss in cost function:3976481540.2069793\n",
      "Training Data****************************************\n",
      "theta:[[112611.19920534  32381.34593885  23458.96594561]]\n",
      "loss in cost function:3525228769.5202394\n",
      "Validation Data****************************************\n",
      "theta:[[112611.19920534  32381.34593885  23458.96594561]]\n",
      "loss in cost function:3949513341.773992\n",
      "Training Data****************************************\n",
      "theta:[[112948.15387712  32470.28450193  23517.83027592]]\n",
      "loss in cost function:3500309622.259585\n",
      "Validation Data****************************************\n",
      "theta:[[112948.15387712  32470.28450193  23517.83027592]]\n",
      "loss in cost function:3922813875.501507\n",
      "Training Data****************************************\n",
      "theta:[[113283.42377554  32558.71644072  23576.30671182]]\n",
      "loss in cost function:3475643411.9607453\n",
      "Validation Data****************************************\n",
      "theta:[[113283.42377554  32558.71644072  23576.30671182]]\n",
      "loss in cost function:3896380427.969914\n",
      "Training Data****************************************\n",
      "theta:[[113617.01732447  32646.64469643  23634.39772581]]\n",
      "loss in cost function:3451227562.0688834\n",
      "Validation Data****************************************\n",
      "theta:[[113617.01732447  32646.64469643  23634.39772581]]\n",
      "loss in cost function:3870210313.407415\n",
      "Training Data****************************************\n",
      "theta:[[113948.94290566  32734.07219299  23692.10577493]]\n",
      "loss in cost function:3427059522.382248\n",
      "Validation Data****************************************\n",
      "theta:[[113948.94290566  32734.07219299  23692.10577493]]\n",
      "loss in cost function:3844300873.406352\n",
      "Training Data****************************************\n",
      "theta:[[114279.20885894  32821.00183711  23749.43330086]]\n",
      "loss in cost function:3403136768.7814007\n",
      "Validation Data****************************************\n",
      "theta:[[114279.20885894  32821.00183711  23749.43330086]]\n",
      "loss in cost function:3818649476.6423883\n",
      "Training Data****************************************\n",
      "theta:[[114607.82348245  32907.4365184   23806.38273001]]\n",
      "loss in cost function:3379456802.961202\n",
      "Validation Data****************************************\n",
      "theta:[[114607.82348245  32907.4365184   23806.38273001]]\n",
      "loss in cost function:3793253518.596581\n",
      "Training Data****************************************\n",
      "theta:[[114934.79503285  32993.37910948  23862.9564736 ]]\n",
      "loss in cost function:3356017152.165624\n",
      "Validation Data****************************************\n",
      "theta:[[114934.79503285  32993.37910948  23862.9564736 ]]\n",
      "loss in cost function:3768110421.280393\n",
      "Training Data****************************************\n",
      "theta:[[115260.13172549  33078.83246606  23919.1569278 ]]\n",
      "loss in cost function:3332815368.9252234\n",
      "Validation Data****************************************\n",
      "theta:[[115260.13172549  33078.83246606  23919.1569278 ]]\n",
      "loss in cost function:3743217632.9634614\n",
      "Training Data****************************************\n",
      "theta:[[115583.84173467  33163.79942705  23974.98647379]]\n",
      "loss in cost function:3309849030.797392\n",
      "Validation Data****************************************\n",
      "theta:[[115583.84173467  33163.79942705  23974.98647379]]\n",
      "loss in cost function:3718572627.9042444\n",
      "Training Data****************************************\n",
      "theta:[[115905.93319381  33248.28281467  24030.44747784]]\n",
      "loss in cost function:3287115740.1092854\n",
      "Validation Data****************************************\n",
      "theta:[[115905.93319381  33248.28281467  24030.44747784]]\n",
      "loss in cost function:3694172906.0834064\n",
      "Training Data****************************************\n",
      "theta:[[116226.41419564  33332.28543451  24085.54229144]]\n",
      "loss in cost function:3264613123.70333\n",
      "Validation Data****************************************\n",
      "theta:[[116226.41419564  33332.28543451  24085.54229144]]\n",
      "loss in cost function:3670015992.9399858\n",
      "Training Data****************************************\n",
      "theta:[[116545.29279247  33415.81007571  24140.27325135]]\n",
      "loss in cost function:3242338832.6854453\n",
      "Validation Data****************************************\n",
      "theta:[[116545.29279247  33415.81007571  24140.27325135]]\n",
      "loss in cost function:3646099439.110245\n",
      "Training Data****************************************\n",
      "theta:[[116862.57699632  33498.85951095  24194.64267972]]\n",
      "loss in cost function:3220290542.175874\n",
      "Validation Data****************************************\n",
      "theta:[[116862.57699632  33498.85951095  24194.64267972]]\n",
      "loss in cost function:3622420820.1692758\n",
      "Training Data****************************************\n",
      "theta:[[117178.27477915  33581.43649664  24248.65288419]]\n",
      "loss in cost function:3198465951.0624747\n",
      "Validation Data****************************************\n",
      "theta:[[117178.27477915  33581.43649664  24248.65288419]]\n",
      "loss in cost function:3598977736.3752127\n",
      "Training Data****************************************\n",
      "theta:[[117492.39407306  33663.54377298  24302.30615793]]\n",
      "loss in cost function:3176862781.756664\n",
      "Validation Data****************************************\n",
      "theta:[[117492.39407306  33663.54377298  24302.30615793]]\n",
      "loss in cost function:3575767812.416114\n",
      "Training Data****************************************\n",
      "theta:[[117804.9427705   33745.18406402  24355.6047798 ]]\n",
      "loss in cost function:3155478779.95187\n",
      "Validation Data****************************************\n",
      "theta:[[117804.9427705   33745.18406402  24355.6047798 ]]\n",
      "loss in cost function:3552788697.1594763\n",
      "Training Data****************************************\n",
      "theta:[[118115.92872446  33826.36007784  24408.55101435]]\n",
      "loss in cost function:3134311714.384388\n",
      "Validation Data****************************************\n",
      "theta:[[118115.92872446  33826.36007784  24408.55101435]]\n",
      "loss in cost function:3530038063.404275\n",
      "Training Data****************************************\n",
      "theta:[[118425.35974864  33907.07450657  24461.147112  ]]\n",
      "loss in cost function:3113359376.596864\n",
      "Validation Data****************************************\n",
      "theta:[[118425.35974864  33907.07450657  24461.147112  ]]\n",
      "loss in cost function:3507513607.6356397\n",
      "Training Data****************************************\n",
      "theta:[[118733.24361771  33987.33002651  24513.39530907]]\n",
      "loss in cost function:3092619580.7040386\n",
      "Validation Data****************************************\n",
      "theta:[[118733.24361771  33987.33002651  24513.39530907]]\n",
      "loss in cost function:3485213049.781975\n",
      "Training Data****************************************\n",
      "theta:[[119039.58806743  34067.12929823  24565.29782787]]\n",
      "loss in cost function:3072090163.1610646\n",
      "Validation Data****************************************\n",
      "theta:[[119039.58806743  34067.12929823  24565.29782787]]\n",
      "loss in cost function:3463134132.974676\n",
      "Training Data****************************************\n",
      "theta:[[119344.4007949   34146.47496667  24616.85687684]]\n",
      "loss in cost function:3051768982.5340915\n",
      "Validation Data****************************************\n",
      "theta:[[119344.4007949   34146.47496667  24616.85687684]]\n",
      "loss in cost function:3441274623.310249\n",
      "Training Data****************************************\n",
      "theta:[[119647.68945873  34225.36966121  24668.07465054]]\n",
      "loss in cost function:3031653919.2733135\n",
      "Validation Data****************************************\n",
      "theta:[[119647.68945873  34225.36966121  24668.07465054]]\n",
      "loss in cost function:3419632309.6149383\n",
      "Training Data****************************************\n",
      "theta:[[119949.46167925  34303.81599578  24718.95332984]]\n",
      "loss in cost function:3011742875.488317\n",
      "Validation Data****************************************\n",
      "theta:[[119949.46167925  34303.81599578  24718.95332984]]\n",
      "loss in cost function:3398205003.2117395\n",
      "Training Data****************************************\n",
      "theta:[[120249.72503866  34381.81656893  24769.49508192]]\n",
      "loss in cost function:2992033774.725714\n",
      "Validation Data****************************************\n",
      "theta:[[120249.72503866  34381.81656893  24769.49508192]]\n",
      "loss in cost function:3376990537.6898513\n",
      "Training Data****************************************\n",
      "theta:[[120548.48708127  34459.37396397  24819.70206041]]\n",
      "loss in cost function:2972524561.7491436\n",
      "Validation Data****************************************\n",
      "theta:[[120548.48708127  34459.37396397  24819.70206041]]\n",
      "loss in cost function:3355986768.6764975\n",
      "Training Data****************************************\n",
      "theta:[[120845.75531368  34536.49074899  24869.57640544]]\n",
      "loss in cost function:2953213202.3214784\n",
      "Validation Data****************************************\n",
      "theta:[[120845.75531368  34536.49074899  24869.57640544]]\n",
      "loss in cost function:3335191573.6110616\n",
      "Training Data****************************************\n",
      "theta:[[121141.53720492  34613.16947701  24919.12024376]]\n",
      "loss in cost function:2934097682.989324\n",
      "Validation Data****************************************\n",
      "theta:[[121141.53720492  34613.16947701  24919.12024376]]\n",
      "loss in cost function:3314602851.5216184\n",
      "Training Data****************************************\n",
      "theta:[[121435.8401867   34689.41268605  24968.33568877]]\n",
      "loss in cost function:2915176010.869714\n",
      "Validation Data****************************************\n",
      "theta:[[121435.8401867   34689.41268605  24968.33568877]]\n",
      "loss in cost function:3294218522.803728\n",
      "Training Data****************************************\n",
      "theta:[[121728.67165357  34765.22289921  25017.22484066]]\n",
      "loss in cost function:2896446213.4389834\n",
      "Validation Data****************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta:[[121728.67165357  34765.22289921  25017.22484066]]\n",
      "loss in cost function:3274036529.001505\n",
      "Training Data****************************************\n",
      "theta:[[122020.03896312  34840.60262475  25065.78978645]]\n",
      "loss in cost function:2877906338.323883\n",
      "Validation Data****************************************\n",
      "theta:[[122020.03896312  34840.60262475  25065.78978645]]\n",
      "loss in cost function:3254054832.5909743\n",
      "Training Data****************************************\n",
      "theta:[[122309.94943611  34915.55435622  25114.03260008]]\n",
      "loss in cost function:2859554453.09481\n",
      "Validation Data****************************************\n",
      "theta:[[122309.94943611  34915.55435622  25114.03260008]]\n",
      "loss in cost function:3234271416.7656364\n",
      "Training Data****************************************\n",
      "theta:[[122598.41035674  34990.0805725   25161.95534252]]\n",
      "loss in cost function:2841388645.061166\n",
      "Validation Data****************************************\n",
      "theta:[[122598.41035674  34990.0805725   25161.95534252]]\n",
      "loss in cost function:3214684285.2242618\n",
      "Training Data****************************************\n",
      "theta:[[122885.42897276  35064.18373793  25209.56006181]]\n",
      "loss in cost function:2823407021.0688133\n",
      "Validation Data****************************************\n",
      "theta:[[122885.42897276  35064.18373793  25209.56006181]]\n",
      "loss in cost function:3195291461.960844\n",
      "Training Data****************************************\n",
      "theta:[[123171.0124957   35137.86630233  25256.84879315]]\n",
      "loss in cost function:2805607707.2996893\n",
      "Validation Data****************************************\n",
      "theta:[[123171.0124957   35137.86630233  25256.84879315]]\n",
      "loss in cost function:3176090991.0567718\n",
      "Training Data****************************************\n",
      "theta:[[123455.16810103  35211.13070119  25303.82355901]]\n",
      "loss in cost function:2787988849.073381\n",
      "Validation Data****************************************\n",
      "theta:[[123455.16810103  35211.13070119  25303.82355901]]\n",
      "loss in cost function:3157080936.475068\n",
      "Training Data****************************************\n",
      "theta:[[123737.90292834  35283.97935563  25350.48636916]]\n",
      "loss in cost function:2770548610.6508517\n",
      "Validation Data****************************************\n",
      "theta:[[123737.90292834  35283.97935563  25350.48636916]]\n",
      "loss in cost function:3138259381.8568273\n",
      "Training Data****************************************\n",
      "theta:[[124019.2240815   35356.41467261  25396.83922079]]\n",
      "loss in cost function:2753285175.040097\n",
      "Validation Data****************************************\n",
      "theta:[[124019.2240815   35356.41467261  25396.83922079]]\n",
      "loss in cost function:3119624430.319686\n",
      "Training Data****************************************\n",
      "theta:[[124299.1386289   35428.43904492  25442.88409857]]\n",
      "loss in cost function:2736196743.8038945\n",
      "Validation Data****************************************\n",
      "theta:[[124299.1386289   35428.43904492  25442.88409857]]\n",
      "loss in cost function:3101174204.2584167\n",
      "Training Data****************************************\n",
      "theta:[[124577.65360357  35500.05485129  25488.62297472]]\n",
      "loss in cost function:2719281536.8694544\n",
      "Validation Data****************************************\n",
      "theta:[[124577.65360357  35500.05485129  25488.62297472]]\n",
      "loss in cost function:3082906845.1475596\n",
      "Training Data****************************************\n",
      "theta:[[124854.77600336  35571.26445651  25534.0578091 ]]\n",
      "loss in cost function:2702537792.3400817\n",
      "Validation Data****************************************\n",
      "theta:[[124854.77600336  35571.26445651  25534.0578091 ]]\n",
      "loss in cost function:3064820513.3460617\n",
      "Training Data****************************************\n",
      "theta:[[125130.51279115  35642.07021146  25579.1905493 ]]\n",
      "loss in cost function:2685963766.308779\n",
      "Validation Data****************************************\n",
      "theta:[[125130.51279115  35642.07021146  25579.1905493 ]]\n",
      "loss in cost function:3046913387.9040017\n",
      "Training Data****************************************\n",
      "theta:[[125404.870895    35712.47445324  25624.02313068]]\n",
      "loss in cost function:2669557732.673721\n",
      "Validation Data****************************************\n",
      "theta:[[125404.870895    35712.47445324  25624.02313068]]\n",
      "loss in cost function:3029183666.3712015\n",
      "Training Data****************************************\n",
      "theta:[[125677.85720834  35782.4795052   25668.55747645]]\n",
      "loss in cost function:2653317982.95571\n",
      "Validation Data****************************************\n",
      "theta:[[125677.85720834  35782.4795052   25668.55747645]]\n",
      "loss in cost function:3011629564.607918\n",
      "Training Data****************************************\n",
      "theta:[[125949.4785901   35852.08767707  25712.7954978 ]]\n",
      "loss in cost function:2637242826.1174335\n",
      "Validation Data****************************************\n",
      "theta:[[125949.4785901   35852.08767707  25712.7954978 ]]\n",
      "loss in cost function:2994249316.5974\n",
      "Training Data****************************************\n",
      "theta:[[126219.74186496  35921.30126501  25756.73909391]]\n",
      "loss in cost function:2621330588.384659\n",
      "Validation Data****************************************\n",
      "theta:[[126219.74186496  35921.30126501  25756.73909391]]\n",
      "loss in cost function:2977041174.2604303\n",
      "Training Data****************************************\n",
      "theta:[[126488.65382344  35990.12255172  25800.39015203]]\n",
      "loss in cost function:2605579613.0691805\n",
      "Validation Data****************************************\n",
      "theta:[[126488.65382344  35990.12255172  25800.39015203]]\n",
      "loss in cost function:2960003407.271741\n",
      "Training Data****************************************\n",
      "theta:[[126756.22122213  36058.55380648  25843.75054761]]\n",
      "loss in cost function:2589988260.3936872\n",
      "Validation Data****************************************\n",
      "theta:[[126756.22122213  36058.55380648  25843.75054761]]\n",
      "loss in cost function:2943134302.878355\n",
      "Training Data****************************************\n",
      "theta:[[127022.45078383  36126.59728525  25886.82214432]]\n",
      "loss in cost function:2574554907.318392\n",
      "Validation Data****************************************\n",
      "theta:[[127022.45078383  36126.59728525  25886.82214432]]\n",
      "loss in cost function:2926432165.7197623\n",
      "Training Data****************************************\n",
      "theta:[[127287.34919772  36194.25523077  25929.60679412]]\n",
      "loss in cost function:2559277947.3693943\n",
      "Validation Data****************************************\n",
      "theta:[[127287.34919772  36194.25523077  25929.60679412]]\n",
      "loss in cost function:2909895317.649978\n",
      "Training Data****************************************\n",
      "theta:[[127550.92311954  36261.52987259  25972.10633738]]\n",
      "loss in cost function:2544155790.468932\n",
      "Validation Data****************************************\n",
      "theta:[[127550.92311954  36261.52987259  25972.10633738]]\n",
      "loss in cost function:2893522097.561418\n",
      "Training Data****************************************\n",
      "theta:[[127813.17917175  36328.4234272   26014.3226029 ]]\n",
      "loss in cost function:2529186862.7672596\n",
      "Validation Data****************************************\n",
      "theta:[[127813.17917175  36328.4234272   26014.3226029 ]]\n",
      "loss in cost function:2877310861.210583\n",
      "Training Data****************************************\n",
      "theta:[[128074.1239437   36394.93809808  26056.25740802]]\n",
      "loss in cost function:2514369606.476336\n",
      "Validation Data****************************************\n",
      "theta:[[128074.1239437   36394.93809808  26056.25740802]]\n",
      "loss in cost function:2861259981.0455613\n",
      "Training Data****************************************\n",
      "theta:[[128333.76399179  36461.07607576  26097.91255867]]\n",
      "loss in cost function:2499702479.705184\n",
      "Validation Data****************************************\n",
      "theta:[[128333.76399179  36461.07607576  26097.91255867]]\n",
      "loss in cost function:2845367846.0352745\n",
      "Training Data****************************************\n",
      "theta:[[128592.10583964  36526.83953794  26139.28984943]]\n",
      "loss in cost function:2485183956.296986\n",
      "Validation Data****************************************\n",
      "theta:[[128592.10583964  36526.83953794  26139.28984943]]\n",
      "loss in cost function:2829632861.5005136\n",
      "Training Data****************************************\n",
      "theta:[[128849.15597825  36592.23064953  26180.39106363]]\n",
      "loss in cost function:2470812525.667805\n",
      "Validation Data****************************************\n",
      "theta:[[128849.15597825  36592.23064953  26180.39106363]]\n",
      "loss in cost function:2814053448.9466853\n",
      "Training Data****************************************\n",
      "theta:[[129104.92086617  36657.25156275  26221.21797342]]\n",
      "loss in cost function:2456586692.64702\n",
      "Validation Data****************************************\n",
      "theta:[[129104.92086617  36657.25156275  26221.21797342]]\n",
      "loss in cost function:2798628045.8982964\n",
      "Training Data****************************************\n",
      "theta:[[129359.40692964  36721.90441719  26261.7723398 ]]\n",
      "loss in cost function:2442504977.319365\n",
      "Validation Data****************************************\n",
      "theta:[[129359.40692964  36721.90441719  26261.7723398 ]]\n",
      "loss in cost function:2783355105.7351584\n",
      "Training Data****************************************\n",
      "theta:[[129612.6205628   36786.19133989  26302.05591272]]\n",
      "loss in cost function:2428565914.8686056\n",
      "Validation Data****************************************\n",
      "theta:[[129612.6205628   36786.19133989  26302.05591272]]\n",
      "loss in cost function:2768233097.5302305\n",
      "Training Data****************************************\n",
      "theta:[[129864.5681278   36850.1144454   26342.07043116]]\n",
      "loss in cost function:2414768055.4228406\n",
      "Validation Data****************************************\n",
      "theta:[[129864.5681278   36850.1144454   26342.07043116]]\n",
      "loss in cost function:2753260505.889181\n",
      "Training Data****************************************\n",
      "theta:[[130115.25595497  36913.6758359   26381.81762315]]\n",
      "loss in cost function:2401109963.901388\n",
      "Validation Data****************************************\n",
      "theta:[[130115.25595497  36913.6758359   26381.81762315]]\n",
      "loss in cost function:2738435830.791561\n",
      "Training Data****************************************\n",
      "theta:[[130364.690343   36976.8776012  26421.2992059]]\n",
      "loss in cost function:2387590219.863214\n",
      "Validation Data****************************************\n",
      "theta:[[130364.690343   36976.8776012  26421.2992059]]\n",
      "loss in cost function:2723757587.43366\n",
      "Training Data****************************************\n",
      "theta:[[130612.87755909  37039.72181888  26460.5168858 ]]\n",
      "loss in cost function:2374207417.3569975\n",
      "Validation Data****************************************\n",
      "theta:[[130612.87755909  37039.72181888  26460.5168858 ]]\n",
      "loss in cost function:2709224306.072906\n",
      "Training Data****************************************\n",
      "theta:[[130859.82383911  37102.21055434  26499.47235856]]\n",
      "loss in cost function:2360960164.772656\n",
      "Validation Data****************************************\n",
      "theta:[[130859.82383911  37102.21055434  26499.47235856]]\n",
      "loss in cost function:2694834531.8739543\n",
      "Training Data****************************************\n",
      "theta:[[131105.53538772  37164.34586086  26538.16730922]]\n",
      "loss in cost function:2347847084.694444\n",
      "Validation Data****************************************\n",
      "theta:[[131105.53538772  37164.34586086  26538.16730922]]\n",
      "loss in cost function:2680586824.7562847\n",
      "Training Data****************************************\n",
      "theta:[[131350.01837859  37226.12977967  26576.60341222]]\n",
      "loss in cost function:2334866813.7555804\n",
      "Validation Data****************************************\n",
      "theta:[[131350.01837859  37226.12977967  26576.60341222]]\n",
      "loss in cost function:2666479759.2434\n",
      "Training Data****************************************\n",
      "theta:[[131593.2789545   37287.56434006  26614.78233151]]\n",
      "loss in cost function:2322018002.494313\n",
      "Validation Data****************************************\n",
      "theta:[[131593.2789545   37287.56434006  26614.78233151]]\n",
      "loss in cost function:2652511924.3135905\n",
      "Training Data****************************************\n",
      "theta:[[131835.32322754  37348.65155939  26652.70572057]]\n",
      "loss in cost function:2309299315.2115235\n",
      "Validation Data****************************************\n",
      "theta:[[131835.32322754  37348.65155939  26652.70572057]]\n",
      "loss in cost function:2638681923.2522006\n",
      "Training Data****************************************\n",
      "theta:[[132076.15727921  37409.39344322  26690.37522248]]\n",
      "loss in cost function:2296709429.8297796\n",
      "Validation Data****************************************\n",
      "theta:[[132076.15727921  37409.39344322  26690.37522248]]\n",
      "loss in cost function:2624988373.505431\n",
      "Training Data****************************************\n",
      "theta:[[132315.78716062  37469.79198534  26727.79247001]]\n",
      "loss in cost function:2284247037.7537947\n",
      "Validation Data****************************************\n",
      "theta:[[132315.78716062  37469.79198534  26727.79247001]]\n",
      "loss in cost function:2611429906.5356593\n",
      "Training Data****************************************\n",
      "theta:[[132554.21889263  37529.84916785  26764.95908567]]\n",
      "loss in cost function:2271910843.7324033\n",
      "Validation Data****************************************\n",
      "theta:[[132554.21889263  37529.84916785  26764.95908567]]\n",
      "loss in cost function:2598005167.678204\n",
      "Training Data****************************************\n",
      "theta:[[132791.45846597  37589.56696126  26801.87668175]]\n",
      "loss in cost function:2259699565.7218904\n",
      "Validation Data****************************************\n",
      "theta:[[132791.45846597  37589.56696126  26801.87668175]]\n",
      "loss in cost function:2584712815.999617\n",
      "Training Data****************************************\n",
      "theta:[[133027.51184145  37648.94732449  26838.54686044]]\n",
      "loss in cost function:2247611934.750759\n",
      "Validation Data****************************************\n",
      "theta:[[133027.51184145  37648.94732449  26838.54686044]]\n",
      "loss in cost function:2571551524.157406\n",
      "Training Data****************************************\n",
      "theta:[[133262.38495005  37707.99220501  26874.97121383]]\n",
      "loss in cost function:2235646694.785882\n",
      "Validation Data****************************************\n",
      "theta:[[133262.38495005  37707.99220501  26874.97121383]]\n",
      "loss in cost function:2558519978.261186\n",
      "Training Data****************************************\n",
      "theta:[[133496.08369311  37766.70353887  26911.15132401]]\n",
      "loss in cost function:2223802602.6000137\n",
      "Validation Data****************************************\n",
      "theta:[[133496.08369311  37766.70353887  26911.15132401]]\n",
      "loss in cost function:2545616877.7352796\n",
      "Training Data****************************************\n",
      "theta:[[133728.61394245  37825.08325076  26947.08876312]]\n",
      "loss in cost function:2212078427.6407175\n",
      "Validation Data****************************************\n",
      "theta:[[133728.61394245  37825.08325076  26947.08876312]]\n",
      "loss in cost function:2532840935.1827283\n",
      "Training Data****************************************\n",
      "theta:[[133959.98154055  37883.13325412  26982.78509342]]\n",
      "loss in cost function:2200472951.900562\n",
      "Validation Data****************************************\n",
      "theta:[[133959.98154055  37883.13325412  26982.78509342]]\n",
      "loss in cost function:2520190876.250682\n",
      "Training Data****************************************\n",
      "theta:[[134190.19230065  37940.85545115  27018.24186734]]\n",
      "loss in cost function:2188984969.788715\n",
      "Validation Data****************************************\n",
      "theta:[[134190.19230065  37940.85545115  27018.24186734]]\n",
      "loss in cost function:2507665439.497221\n",
      "Training Data****************************************\n",
      "theta:[[134419.25200696  37998.25173294  27053.46062755]]\n",
      "loss in cost function:2177613288.003847\n",
      "Validation Data****************************************\n",
      "theta:[[134419.25200696  37998.25173294  27053.46062755]]\n",
      "loss in cost function:2495263376.2594657\n",
      "Training Data****************************************\n",
      "theta:[[134647.16641473  38055.32397946  27088.44290702]]\n",
      "loss in cost function:2166356725.4083204\n",
      "Validation Data****************************************\n",
      "theta:[[134647.16641473  38055.32397946  27088.44290702]]\n",
      "loss in cost function:2482983450.523128\n",
      "Training Data****************************************\n",
      "theta:[[134873.94125047  38112.0740597   27123.19022906]]\n",
      "loss in cost function:2155214112.903688\n",
      "Validation Data****************************************\n",
      "theta:[[134873.94125047  38112.0740597   27123.19022906]]\n",
      "loss in cost function:2470824438.7933493\n",
      "Training Data****************************************\n",
      "theta:[[135099.58221202  38168.50383169  27157.70410742]]\n",
      "loss in cost function:2144184293.3074896\n",
      "Validation Data****************************************\n",
      "theta:[[135099.58221202  38168.50383169  27157.70410742]]\n",
      "loss in cost function:2458785129.9668818\n",
      "Training Data****************************************\n",
      "theta:[[135324.09496877  38224.61514259  27191.98604632]]\n",
      "loss in cost function:2133266121.2312396\n",
      "Validation Data****************************************\n",
      "theta:[[135324.09496877  38224.61514259  27191.98604632]]\n",
      "loss in cost function:2446864325.2055883\n",
      "Training Data****************************************\n",
      "theta:[[135547.48516174  38280.40982871  27226.03754051]]\n",
      "loss in cost function:2122458462.9598062\n",
      "Validation Data****************************************\n",
      "theta:[[135547.48516174  38280.40982871  27226.03754051]]\n",
      "loss in cost function:2435060837.811235\n",
      "Training Data****************************************\n",
      "theta:[[135769.75840374  38335.88971563  27259.86007533]]\n",
      "loss in cost function:2111760196.3318741\n",
      "Validation Data****************************************\n",
      "theta:[[135769.75840374  38335.88971563  27259.86007533]]\n",
      "loss in cost function:2423373493.1015654\n",
      "Training Data****************************************\n",
      "theta:[[135990.92027952  38391.05661825  27293.45512678]]\n",
      "loss in cost function:2101170210.6217656\n",
      "Validation Data****************************************\n",
      "theta:[[135990.92027952  38391.05661825  27293.45512678]]\n",
      "loss in cost function:2411801128.2876706\n",
      "Training Data****************************************\n",
      "theta:[[136210.97634594  38445.91234082  27326.82416158]]\n",
      "loss in cost function:2090687406.4223938\n",
      "Validation Data****************************************\n",
      "theta:[[136210.97634594  38445.91234082  27326.82416158]]\n",
      "loss in cost function:2400342592.352596\n",
      "Training Data****************************************\n",
      "theta:[[136429.93213201  38500.45867703  27359.96863719]]\n",
      "loss in cost function:2080310695.5294561\n",
      "Validation Data****************************************\n",
      "theta:[[136429.93213201  38500.45867703  27359.96863719]]\n",
      "loss in cost function:2388996745.931188\n",
      "Training Data****************************************\n",
      "theta:[[136647.79313916  38554.69741007  27392.89000192]]\n",
      "loss in cost function:2070039000.8268065\n",
      "Validation Data****************************************\n",
      "theta:[[136647.79313916  38554.69741007  27392.89000192]]\n",
      "loss in cost function:2377762461.191226\n",
      "Training Data****************************************\n",
      "theta:[[136864.56484127  38608.6303127   27425.58969497]]\n",
      "loss in cost function:2059871256.173038\n",
      "Validation Data****************************************\n",
      "theta:[[136864.56484127  38608.6303127   27425.58969497]]\n",
      "loss in cost function:2366638621.715698\n",
      "Training Data****************************************\n",
      "theta:[[137080.25268488  38662.25914729  27458.06914644]]\n",
      "loss in cost function:2049806406.2891545\n",
      "Validation Data****************************************\n",
      "theta:[[137080.25268488  38662.25914729  27458.06914644]]\n",
      "loss in cost function:2355624122.3863544\n",
      "Training Data****************************************\n",
      "theta:[[137294.86208926  38715.5856659   27490.32977747]]\n",
      "loss in cost function:2039843406.647483\n",
      "Validation Data****************************************\n",
      "theta:[[137294.86208926  38715.5856659   27490.32977747]]\n",
      "loss in cost function:2344717869.2684355\n",
      "Training Data****************************************\n",
      "theta:[[137508.39844662  38768.61161032  27522.37300021]]\n",
      "loss in cost function:2029981223.3616781\n",
      "Validation Data****************************************\n",
      "theta:[[137508.39844662  38768.61161032  27522.37300021]]\n",
      "loss in cost function:2333918779.4965425\n",
      "Training Data****************************************\n",
      "theta:[[137720.8671222   38821.33871218  27554.20021796]]\n",
      "loss in cost function:2020218833.0778754\n",
      "Validation Data****************************************\n",
      "theta:[[137720.8671222   38821.33871218  27554.20021796]]\n",
      "loss in cost function:2323225781.161764\n",
      "Training Data****************************************\n",
      "theta:[[137932.27345439  38873.76869293  27585.81282514]]\n",
      "loss in cost function:2010555222.8669379\n",
      "Validation Data****************************************\n",
      "theta:[[137932.27345439  38873.76869293  27585.81282514]]\n",
      "loss in cost function:2312637813.1998644\n",
      "Training Data****************************************\n",
      "theta:[[138142.62275493  38925.90326399  27617.21220742]]\n",
      "loss in cost function:2000989390.117865\n",
      "Validation Data****************************************\n",
      "theta:[[138142.62275493  38925.90326399  27617.21220742]]\n",
      "loss in cost function:2302153825.2807198\n",
      "Training Data****************************************\n",
      "theta:[[138351.92030896  38977.74412672  27648.39974172]]\n",
      "loss in cost function:1991520342.432216\n",
      "Validation Data****************************************\n",
      "theta:[[138351.92030896  38977.74412672  27648.39974172]]\n",
      "loss in cost function:2291772777.6987805\n",
      "Training Data****************************************\n",
      "theta:[[138560.17137523  39029.29297257  27679.37679629]]\n",
      "loss in cost function:1982147097.5197065\n",
      "Validation Data****************************************\n",
      "theta:[[138560.17137523  39029.29297257  27679.37679629]]\n",
      "loss in cost function:2281493641.264785\n",
      "Training Data****************************************\n",
      "theta:[[138767.38118616  39080.55148305  27710.14473076]]\n",
      "loss in cost function:1972868683.0948036\n",
      "Validation Data****************************************\n",
      "theta:[[138767.38118616  39080.55148305  27710.14473076]]\n",
      "loss in cost function:2271315397.198455\n",
      "Training Data****************************************\n",
      "theta:[[138973.55494804  39131.52132988  27740.7048962 ]]\n",
      "loss in cost function:1963684136.7744246\n",
      "Validation Data****************************************\n",
      "theta:[[138973.55494804  39131.52132988  27740.7048962 ]]\n",
      "loss in cost function:2261237037.0223947\n",
      "Training Data****************************************\n",
      "theta:[[139178.6978411   39182.20417495  27771.05863516]]\n",
      "loss in cost function:1954592505.9766846\n",
      "Validation Data****************************************\n",
      "theta:[[139178.6978411   39182.20417495  27771.05863516]]\n",
      "loss in cost function:2251257562.4570174\n",
      "Training Data****************************************\n",
      "theta:[[139382.81501971  39232.60167045  27801.20728172]]\n",
      "loss in cost function:1945592847.8206637\n",
      "Validation Data****************************************\n",
      "theta:[[139382.81501971  39232.60167045  27801.20728172]]\n",
      "loss in cost function:2241375985.3165846\n",
      "Training Data****************************************\n",
      "theta:[[139585.91161242  39282.71545893  27831.15216157]]\n",
      "loss in cost function:1936684229.0272112\n",
      "Validation Data****************************************\n",
      "theta:[[139585.91161242  39282.71545893  27831.15216157]]\n",
      "loss in cost function:2231591327.4062905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data****************************************\n",
      "theta:[[139787.99272216  39332.54717329  27860.89459203]]\n",
      "loss in cost function:1927865725.82081\n",
      "Validation Data****************************************\n",
      "theta:[[139787.99272216  39332.54717329  27860.89459203]]\n",
      "loss in cost function:2221902620.4204216\n",
      "Training Data****************************************\n",
      "theta:[[139989.06342636  39382.0984369   27890.43588211]]\n",
      "loss in cost function:1919136423.8323553\n",
      "Validation Data****************************************\n",
      "theta:[[139989.06342636  39382.0984369   27890.43588211]]\n",
      "loss in cost function:2212308905.841537\n",
      "Training Data****************************************\n",
      "theta:[[140189.12877704  39431.37086365  27919.77733259]]\n",
      "loss in cost function:1910495418.0030262\n",
      "Validation Data****************************************\n",
      "theta:[[140189.12877704  39431.37086365  27919.77733259]]\n",
      "loss in cost function:2202809234.840716\n",
      "Training Data****************************************\n",
      "theta:[[140388.19380096  39480.36605795  27948.92023603]]\n",
      "loss in cost function:1901941812.489114\n",
      "Validation Data****************************************\n",
      "theta:[[140388.19380096  39480.36605795  27948.92023603]]\n",
      "loss in cost function:2193402668.178801\n",
      "Training Data****************************************\n",
      "theta:[[140586.26349976  39529.08561488  27977.86587683]]\n",
      "loss in cost function:1893474720.567795\n",
      "Validation Data****************************************\n",
      "theta:[[140586.26349976  39529.08561488  27977.86587683]]\n",
      "loss in cost function:2184088276.1086597\n",
      "Training Data****************************************\n",
      "theta:[[140783.34285007  39577.53112016  28006.61553132]]\n",
      "loss in cost function:1885093264.5439093\n",
      "Validation Data****************************************\n",
      "theta:[[140783.34285007  39577.53112016  28006.61553132]]\n",
      "loss in cost function:2174865138.2784686\n",
      "Training Data****************************************\n",
      "theta:[[140979.43680363  39625.70415024  28035.17046775]]\n",
      "loss in cost function:1876796575.657696\n",
      "Validation Data****************************************\n",
      "theta:[[140979.43680363  39625.70415024  28035.17046775]]\n",
      "loss in cost function:2165732343.635958\n",
      "Training Data****************************************\n",
      "theta:[[141174.55028742  39673.60627236  28063.53194639]]\n",
      "loss in cost function:1868583793.9934354\n",
      "Validation Data****************************************\n",
      "theta:[[141174.55028742  39673.60627236  28063.53194639]]\n",
      "loss in cost function:2156688990.333665\n",
      "Training Data****************************************\n",
      "theta:[[141368.68820379  39721.23904461  28091.70121956]]\n",
      "loss in cost function:1860454068.389093\n",
      "Validation Data****************************************\n",
      "theta:[[141368.68820379  39721.23904461  28091.70121956]]\n",
      "loss in cost function:2147734185.635157\n",
      "Training Data****************************************\n",
      "theta:[[141561.85543058  39768.60401594  28119.67953165]]\n",
      "loss in cost function:1852406556.3468313\n",
      "Validation Data****************************************\n",
      "theta:[[141561.85543058  39768.60401594  28119.67953165]]\n",
      "loss in cost function:2138867045.8221924\n",
      "Training Data****************************************\n",
      "theta:[[141754.05682124  39815.70272629  28147.46811924]]\n",
      "loss in cost function:1844440423.944474\n",
      "Validation Data****************************************\n",
      "theta:[[141754.05682124  39815.70272629  28147.46811924]]\n",
      "loss in cost function:2130086696.1028585\n",
      "Training Data****************************************\n",
      "theta:[[141945.29720494  39862.53670656  28175.06821108]]\n",
      "loss in cost function:1836554845.747875\n",
      "Validation Data****************************************\n",
      "theta:[[141945.29720494  39862.53670656  28175.06821108]]\n",
      "loss in cost function:2121392270.5206525\n",
      "Training Data****************************************\n",
      "theta:[[142135.58138672  39909.10747871  28202.48102817]]\n",
      "loss in cost function:1828749004.7241716\n",
      "Validation Data****************************************\n",
      "theta:[[142135.58138672  39909.10747871  28202.48102817]]\n",
      "loss in cost function:2112782911.8644648\n",
      "Training Data****************************************\n",
      "theta:[[142324.9141476   39955.41655584  28229.70778381]]\n",
      "loss in cost function:1821022092.155944\n",
      "Validation Data****************************************\n",
      "theta:[[142324.9141476   39955.41655584  28229.70778381]]\n",
      "loss in cost function:2104257771.5795298\n",
      "Training Data****************************************\n",
      "theta:[[142513.30024467  40001.46544215  28256.74968364]]\n",
      "loss in cost function:1813373307.5562613\n",
      "Validation Data****************************************\n",
      "theta:[[142513.30024467  40001.46544215  28256.74968364]]\n",
      "loss in cost function:2095816009.6792514\n",
      "Training Data****************************************\n",
      "theta:[[142700.74441125  40047.25563311  28283.60792568]]\n",
      "loss in cost function:1805801858.5845633\n",
      "Validation Data****************************************\n",
      "theta:[[142700.74441125  40047.25563311  28283.60792568]]\n",
      "loss in cost function:2087456794.6579552\n",
      "Training Data****************************************\n",
      "theta:[[142887.251357   40092.7886154  28310.2837004]]\n",
      "loss in cost function:1798306960.9634616\n",
      "Validation Data****************************************\n",
      "theta:[[142887.251357   40092.7886154  28310.2837004]]\n",
      "loss in cost function:2079179303.4045222\n",
      "Training Data****************************************\n",
      "theta:[[143072.82576803  40138.06586706  28336.77819075]]\n",
      "loss in cost function:1790887838.3963492\n",
      "Validation Data****************************************\n",
      "theta:[[143072.82576803  40138.06586706  28336.77819075]]\n",
      "loss in cost function:2070982721.1169167\n",
      "Training Data****************************************\n",
      "theta:[[143257.472307    40183.08885746  28363.0925722 ]]\n",
      "loss in cost function:1783543722.4858582\n",
      "Validation Data****************************************\n",
      "theta:[[143257.472307    40183.08885746  28363.0925722 ]]\n",
      "loss in cost function:2062866241.2175903\n",
      "Training Data****************************************\n",
      "theta:[[143441.19561327  40227.8590474   28389.22801281]]\n",
      "loss in cost function:1776273852.653192\n",
      "Validation Data****************************************\n",
      "theta:[[143441.19561327  40227.8590474   28389.22801281]]\n",
      "loss in cost function:2054829065.269746\n",
      "Training Data****************************************\n",
      "theta:[[143624.00030301  40272.37788915  28415.18567325]]\n",
      "loss in cost function:1769077476.0582466\n",
      "Validation Data****************************************\n",
      "theta:[[143624.00030301  40272.37788915  28415.18567325]]\n",
      "loss in cost function:2046870402.8944669\n",
      "Training Data****************************************\n",
      "theta:[[143805.8909693   40316.64682649  28440.96670687]]\n",
      "loss in cost function:1761953847.5205595\n",
      "Validation Data****************************************\n",
      "theta:[[143805.8909693   40316.64682649  28440.96670687]]\n",
      "loss in cost function:2038989471.6886852\n",
      "Training Data****************************************\n",
      "theta:[[143986.87218227  40360.66729478  28466.57225974]]\n",
      "loss in cost function:1754902229.441076\n",
      "Validation Data****************************************\n",
      "theta:[[143986.87218227  40360.66729478  28466.57225974]]\n",
      "loss in cost function:2031185497.1440136\n",
      "Training Data****************************************\n",
      "theta:[[144166.94848916  40404.44072099  28492.00347066]]\n",
      "loss in cost function:1747921891.7247424\n",
      "Validation Data****************************************\n",
      "theta:[[144166.94848916  40404.44072099  28492.00347066]]\n",
      "loss in cost function:2023457712.5663753\n",
      "Training Data****************************************\n",
      "theta:[[144346.12441452  40447.96852375  28517.26147126]]\n",
      "loss in cost function:1741012111.7038207\n",
      "Validation Data****************************************\n",
      "theta:[[144346.12441452  40447.96852375  28517.26147126]]\n",
      "loss in cost function:2015805358.9964955\n",
      "Training Data****************************************\n",
      "theta:[[144524.40446026  40491.25211341  28542.34738602]]\n",
      "loss in cost function:1734172174.062082\n",
      "Validation Data****************************************\n",
      "theta:[[144524.40446026  40491.25211341  28542.34738602]]\n",
      "loss in cost function:2008227685.1311724\n",
      "Training Data****************************************\n",
      "theta:[[144701.79310577  40534.2928921   28567.26233231]]\n",
      "loss in cost function:1727401370.7597072\n",
      "Validation Data****************************************\n",
      "theta:[[144701.79310577  40534.2928921   28567.26233231]]\n",
      "loss in cost function:2000723947.2453752\n",
      "Training Data****************************************\n",
      "theta:[[144878.29480805  40577.09225374  28592.00742042]]\n",
      "loss in cost function:1720699000.9589903\n",
      "Validation Data****************************************\n",
      "theta:[[144878.29480805  40577.09225374  28592.00742042]]\n",
      "loss in cost function:1993293409.1151488\n",
      "Training Data****************************************\n",
      "theta:[[145053.91400181  40619.65158413  28616.58375365]]\n",
      "loss in cost function:1714064370.9507954\n",
      "Validation Data****************************************\n",
      "theta:[[145053.91400181  40619.65158413  28616.58375365]]\n",
      "loss in cost function:1985935341.9412715\n",
      "Training Data****************************************\n",
      "theta:[[145228.65509961  40661.97226098  28640.99242831]]\n",
      "loss in cost function:1707496794.0817697\n",
      "Validation Data****************************************\n",
      "theta:[[145228.65509961  40661.97226098  28640.99242831]]\n",
      "loss in cost function:1978649024.2737331\n",
      "Training Data****************************************\n",
      "theta:[[145402.52249192  40704.05565393  28665.23453378]]\n",
      "loss in cost function:1700995590.6823113\n",
      "Validation Data****************************************\n",
      "theta:[[145402.52249192  40704.05565393  28665.23453378]]\n",
      "loss in cost function:1971433741.9369736\n",
      "Training Data****************************************\n",
      "theta:[[145575.52054727  40745.90312468  28689.31115255]]\n",
      "loss in cost function:1694560087.995247\n",
      "Validation Data****************************************\n",
      "theta:[[145575.52054727  40745.90312468  28689.31115255]]\n",
      "loss in cost function:1964288787.9558837\n",
      "Training Data****************************************\n",
      "theta:[[145747.65361234  40787.51602692  28713.22336028]]\n",
      "loss in cost function:1688189620.105281\n",
      "Validation Data****************************************\n",
      "theta:[[145747.65361234  40787.51602692  28713.22336028]]\n",
      "loss in cost function:1957213462.4825656\n",
      "Training Data****************************************\n",
      "theta:[[145918.92601209  40828.89570649  28736.9722258 ]]\n",
      "loss in cost function:1681883527.8691385\n",
      "Validation Data****************************************\n",
      "theta:[[145918.92601209  40828.89570649  28736.9722258 ]]\n",
      "loss in cost function:1950207072.723836\n",
      "Training Data****************************************\n",
      "theta:[[146089.34204984  40870.04350135  28760.55881121]]\n",
      "loss in cost function:1675641158.8464353\n",
      "Validation Data****************************************\n",
      "theta:[[146089.34204984  40870.04350135  28760.55881121]]\n",
      "loss in cost function:1943268932.8694918\n",
      "Training Data****************************************\n",
      "theta:[[146258.9060074   40910.96074166  28783.98417187]]\n",
      "loss in cost function:1669461867.2312527\n",
      "Validation Data****************************************\n",
      "theta:[[146258.9060074   40910.96074166  28783.98417187]]\n",
      "loss in cost function:1936398364.0212991\n",
      "Training Data****************************************\n",
      "theta:[[146427.62214517  40951.64874982  28807.24935648]]\n",
      "loss in cost function:1663345013.784429\n",
      "Validation Data****************************************\n",
      "theta:[[146427.62214517  40951.64874982  28807.24935648]]\n",
      "loss in cost function:1929594694.1226933\n",
      "Training Data****************************************\n",
      "theta:[[146595.49470225  40992.1088405   28830.35540708]]\n",
      "loss in cost function:1657289965.7665138\n",
      "Validation Data****************************************\n",
      "theta:[[146595.49470225  40992.1088405   28830.35540708]]\n",
      "loss in cost function:1922857257.8892245\n",
      "Training Data****************************************\n",
      "theta:[[146762.52789655  41032.34232073  28853.30335915]]\n",
      "loss in cost function:1651296096.871442\n",
      "Validation Data****************************************\n",
      "theta:[[146762.52789655  41032.34232073  28853.30335915]]\n",
      "loss in cost function:1916185396.7397096\n",
      "Training Data****************************************\n",
      "theta:[[146928.72592487  41072.3504899   28876.09424159]]\n",
      "loss in cost function:1645362787.1608744\n",
      "Validation Data****************************************\n",
      "theta:[[146928.72592487  41072.3504899   28876.09424159]]\n",
      "loss in cost function:1909578458.7280796\n",
      "Training Data****************************************\n",
      "theta:[[147094.09296306  41112.1346398   28898.7290768 ]]\n",
      "loss in cost function:1639489422.9992032\n",
      "Validation Data****************************************\n",
      "theta:[[147094.09296306  41112.1346398   28898.7290768 ]]\n",
      "loss in cost function:1903035798.4759188\n",
      "Training Data****************************************\n",
      "theta:[[147258.63316605  41151.69605472  28921.20888072]]\n",
      "loss in cost function:1633675396.9892323\n",
      "Validation Data****************************************\n",
      "theta:[[147258.63316605  41151.69605472  28921.20888072]]\n",
      "loss in cost function:1896556777.1057208\n",
      "Training Data****************************************\n",
      "theta:[[147422.35066803  41191.03601145  28943.53466283]]\n",
      "loss in cost function:1627920107.9085324\n",
      "Validation Data****************************************\n",
      "theta:[[147422.35066803  41191.03601145  28943.53466283]]\n",
      "loss in cost function:1890140762.1748114\n",
      "Training Data****************************************\n",
      "theta:[[147585.2495825   41230.15577931  28965.70742625]]\n",
      "loss in cost function:1622222960.6464036\n",
      "Validation Data****************************************\n",
      "theta:[[147585.2495825   41230.15577931  28965.70742625]]\n",
      "loss in cost function:1883787127.6099353\n",
      "Training Data****************************************\n",
      "theta:[[147747.33400239  41269.05662024  28987.72816773]]\n",
      "loss in cost function:1616583366.1415386\n",
      "Validation Data****************************************\n",
      "theta:[[147747.33400239  41269.05662024  28987.72816773]]\n",
      "loss in cost function:1877495253.6425312\n",
      "Training Data****************************************\n",
      "theta:[[147908.60800019  41307.73978883  29009.59787772]]\n",
      "loss in cost function:1611000741.3202844\n",
      "Validation Data****************************************\n",
      "theta:[[147908.60800019  41307.73978883  29009.59787772]]\n",
      "loss in cost function:1871264526.7446773\n",
      "Training Data****************************************\n",
      "theta:[[148069.075628    41346.20653233  29031.31754038]]\n",
      "loss in cost function:1605474509.0355625\n",
      "Validation Data****************************************\n",
      "theta:[[148069.075628    41346.20653233  29031.31754038]]\n",
      "loss in cost function:1865094339.565655\n",
      "Training Data****************************************\n",
      "theta:[[148228.74091766  41384.45809074  29052.88813365]]\n",
      "loss in cost function:1600004098.0063887\n",
      "Validation Data****************************************\n",
      "theta:[[148228.74091766  41384.45809074  29052.88813365]]\n",
      "loss in cost function:1858984090.8692005\n",
      "Training Data****************************************\n",
      "theta:[[148387.60788088  41422.49569682  29074.31062929]]\n",
      "loss in cost function:1594588942.7580278\n",
      "Validation Data****************************************\n",
      "theta:[[148387.60788088  41422.49569682  29074.31062929]]\n",
      "loss in cost function:1852933185.4713855\n",
      "Training Data****************************************\n",
      "theta:[[148545.68050929  41460.32057614  29095.58599287]]\n",
      "loss in cost function:1589228483.5627804\n",
      "Validation Data****************************************\n",
      "theta:[[148545.68050929  41460.32057614  29095.58599287]]\n",
      "loss in cost function:1846941034.1791136\n",
      "Training Data****************************************\n",
      "theta:[[148702.96277455  41497.93394713  29116.71518387]]\n",
      "loss in cost function:1583922166.3813453\n",
      "Validation Data****************************************\n",
      "theta:[[148702.96277455  41497.93394713  29116.71518387]]\n",
      "loss in cost function:1841007053.7292907\n",
      "Training Data****************************************\n",
      "theta:[[148859.45862848  41535.33702114  29137.69915568]]\n",
      "loss in cost function:1578669442.8047688\n",
      "Validation Data****************************************\n",
      "theta:[[148859.45862848  41535.33702114  29137.69915568]]\n",
      "loss in cost function:1835130666.7285607\n",
      "Training Data****************************************\n",
      "theta:[[149015.17200315  41572.53100242  29158.53885565]]\n",
      "loss in cost function:1573469769.9970753\n",
      "Validation Data****************************************\n",
      "theta:[[149015.17200315  41572.53100242  29158.53885565]]\n",
      "loss in cost function:1829311301.593711\n",
      "Training Data****************************************\n",
      "theta:[[149170.10681094  41609.51708824  29179.23522513]]\n",
      "loss in cost function:1568322610.638354\n",
      "Validation Data****************************************\n",
      "theta:[[149170.10681094  41609.51708824  29179.23522513]]\n",
      "loss in cost function:1823548392.49265\n",
      "Training Data****************************************\n",
      "theta:[[149324.2669447   41646.29646886  29199.7891995 ]]\n",
      "loss in cost function:1563227432.8685532\n",
      "Validation Data****************************************\n",
      "theta:[[149324.2669447   41646.29646886  29199.7891995 ]]\n",
      "loss in cost function:1817841379.2860084\n",
      "Training Data****************************************\n",
      "theta:[[149477.65627778  41682.87032762  29220.20170821]]\n",
      "loss in cost function:1558183710.2317584\n",
      "Validation Data****************************************\n",
      "theta:[[149477.65627778  41682.87032762  29220.20170821]]\n",
      "loss in cost function:1812189707.4693341\n",
      "Training Data****************************************\n",
      "theta:[[149630.2786642   41719.23984099  29240.47367482]]\n",
      "loss in cost function:1553190921.6211066\n",
      "Validation Data****************************************\n",
      "theta:[[149630.2786642   41719.23984099  29240.47367482]]\n",
      "loss in cost function:1806592828.1158662\n",
      "Training Data****************************************\n",
      "theta:[[149782.13793869  41755.40617853  29260.60601703]]\n",
      "loss in cost function:1548248551.2242162\n",
      "Validation Data****************************************\n",
      "theta:[[149782.13793869  41755.40617853  29260.60601703]]\n",
      "loss in cost function:1801050197.8199162\n",
      "Training Data****************************************\n",
      "theta:[[149933.2379168   41791.37050305  29280.59964673]]\n",
      "loss in cost function:1543356088.469218\n",
      "Validation Data****************************************\n",
      "theta:[[149933.2379168   41791.37050305  29280.59964673]]\n",
      "loss in cost function:1795561278.6408038\n",
      "Training Data****************************************\n",
      "theta:[[150083.58239503  41827.13397053  29300.45547001]]\n",
      "loss in cost function:1538513027.9712875\n",
      "Validation Data****************************************\n",
      "theta:[[150083.58239503  41827.13397053  29300.45547001]]\n",
      "loss in cost function:1790125538.0473895\n",
      "Training Data****************************************\n",
      "theta:[[150233.17515086  41862.69773025  29320.17438724]]\n",
      "loss in cost function:1533718869.4797819\n",
      "Validation Data****************************************\n",
      "theta:[[150233.17515086  41862.69773025  29320.17438724]]\n",
      "loss in cost function:1784742448.8631516\n",
      "Training Data****************************************\n",
      "theta:[[150382.01994291  41898.0629248   29339.75729306]]\n",
      "loss in cost function:1528973117.8258803\n",
      "Validation Data****************************************\n",
      "theta:[[150382.01994291  41898.0629248   29339.75729306]]\n",
      "loss in cost function:1779411489.211845\n",
      "Training Data****************************************\n",
      "theta:[[150530.12051101  41933.23069008  29359.20507643]]\n",
      "loss in cost function:1524275282.870763\n",
      "Validation Data****************************************\n",
      "theta:[[150530.12051101  41933.23069008  29359.20507643]]\n",
      "loss in cost function:1774132142.4636996\n",
      "Training Data****************************************\n",
      "theta:[[150677.48057626  41968.20215543  29378.51862068]]\n",
      "loss in cost function:1519624879.4543393\n",
      "Validation Data****************************************\n",
      "theta:[[150677.48057626  41968.20215543  29378.51862068]]\n",
      "loss in cost function:1768903897.1821847\n",
      "Training Data****************************************\n",
      "theta:[[150824.10384119  42002.97844356  29397.69880355]]\n",
      "loss in cost function:1515021427.344494\n",
      "Validation Data****************************************\n",
      "theta:[[150824.10384119  42002.97844356  29397.69880355]]\n",
      "loss in cost function:1763726247.0712953\n",
      "Training Data****************************************\n",
      "theta:[[150969.99398979  42037.56067067  29416.74649717]]\n",
      "loss in cost function:1510464451.1868372\n",
      "Validation Data****************************************\n",
      "theta:[[150969.99398979  42037.56067067  29416.74649717]]\n",
      "loss in cost function:1758598690.923402\n",
      "Training Data****************************************\n",
      "theta:[[151115.15468765  42071.94994647  29435.66256818]]\n",
      "loss in cost function:1505953480.4549816\n",
      "Validation Data****************************************\n",
      "theta:[[151115.15468765  42071.94994647  29435.66256818]]\n",
      "loss in cost function:1753520732.567615\n",
      "Training Data****************************************\n",
      "theta:[[151259.58958202  42106.14737418  29454.44787768]]\n",
      "loss in cost function:1501488049.4013364\n",
      "Validation Data****************************************\n",
      "theta:[[151259.58958202  42106.14737418  29454.44787768]]\n",
      "loss in cost function:1748491880.818684\n",
      "Training Data****************************************\n",
      "theta:[[151403.30230192  42140.15405063  29473.10328134]]\n",
      "loss in cost function:1497067697.0083938\n",
      "Validation Data****************************************\n",
      "theta:[[151403.30230192  42140.15405063  29473.10328134]]\n",
      "loss in cost function:1743511649.4264252\n",
      "Training Data****************************************\n",
      "theta:[[151546.29645822  42173.97106624  29491.62962937]]\n",
      "loss in cost function:1492691966.9405086\n",
      "Validation Data****************************************\n",
      "theta:[[151546.29645822  42173.97106624  29491.62962937]]\n",
      "loss in cost function:1738579557.0256534\n",
      "Training Data****************************************\n",
      "theta:[[151688.57564373  42207.5995051   29510.02776658]]\n",
      "loss in cost function:1488360407.4961972\n",
      "Validation Data****************************************\n",
      "theta:[[151688.57564373  42207.5995051   29510.02776658]]\n",
      "loss in cost function:1733695127.0866284\n",
      "Training Data****************************************\n",
      "theta:[[151830.14343332  42241.04044499  29528.29853245]]\n",
      "loss in cost function:1484072571.5608766\n",
      "Validation Data****************************************\n",
      "theta:[[151830.14343332  42241.04044499  29528.29853245]]\n",
      "loss in cost function:1728857887.866019\n",
      "Training Data****************************************\n",
      "theta:[[151971.00338396  42274.2949574   29546.44276109]]\n",
      "loss in cost function:1479828016.5601528\n",
      "Validation Data****************************************\n",
      "theta:[[151971.00338396  42274.2949574   29546.44276109]]\n",
      "loss in cost function:1724067372.3583531\n",
      "Training Data****************************************\n",
      "theta:[[152111.15903485  42307.36410761  29564.46128134]]\n",
      "loss in cost function:1475626304.4135299\n",
      "Validation Data****************************************\n",
      "theta:[[152111.15903485  42307.36410761  29564.46128134]]\n",
      "loss in cost function:1719323118.2479708\n",
      "Training Data****************************************\n",
      "theta:[[152250.61390749  42340.24895469  29582.35491674]]\n",
      "loss in cost function:1471467001.488612\n",
      "Validation Data****************************************\n",
      "theta:[[152250.61390749  42340.24895469  29582.35491674]]\n",
      "loss in cost function:1714624667.8614764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data****************************************\n",
      "theta:[[152389.37150576  42372.95055154  29600.12448565]]\n",
      "loss in cost function:1467349678.555803\n",
      "Validation Data****************************************\n",
      "theta:[[152389.37150576  42372.95055154  29600.12448565]]\n",
      "loss in cost function:1709971568.1206515\n",
      "Training Data****************************************\n",
      "theta:[[152527.43531604  42405.46994494  29617.77080118]]\n",
      "loss in cost function:1463273910.7434049\n",
      "Validation Data****************************************\n",
      "theta:[[152527.43531604  42405.46994494  29617.77080118]]\n",
      "loss in cost function:1705363370.4958835\n",
      "Training Data****************************************\n",
      "theta:[[152664.80880726  42437.80817559  29635.29467129]]\n",
      "loss in cost function:1459239277.4932494\n",
      "Validation Data****************************************\n",
      "theta:[[152664.80880726  42437.80817559  29635.29467129]]\n",
      "loss in cost function:1700799630.9600344\n",
      "Training Data****************************************\n",
      "theta:[[152801.49543104  42469.96627812  29652.69689883]]\n",
      "loss in cost function:1455245362.5167172\n",
      "Validation Data****************************************\n",
      "theta:[[152801.49543104  42469.96627812  29652.69689883]]\n",
      "loss in cost function:1696279909.9428089\n",
      "Training Data****************************************\n",
      "theta:[[152937.49862169  42501.94528117  29669.97828151]]\n",
      "loss in cost function:1451291753.7512746\n",
      "Validation Data****************************************\n",
      "theta:[[152937.49862169  42501.94528117  29669.97828151]]\n",
      "loss in cost function:1691803772.2855709\n",
      "Training Data****************************************\n",
      "theta:[[153072.82179639  42533.74620736  29687.139612  ]]\n",
      "loss in cost function:1447378043.3173778\n",
      "Validation Data****************************************\n",
      "theta:[[153072.82179639  42533.74620736  29687.139612  ]]\n",
      "loss in cost function:1687370787.1966352\n",
      "Training Data****************************************\n",
      "theta:[[153207.46835522  42565.37007339  29704.1816779 ]]\n",
      "loss in cost function:1443503827.4758902\n",
      "Validation Data****************************************\n",
      "theta:[[153207.46835522  42565.37007339  29704.1816779 ]]\n",
      "loss in cost function:1682980528.2070003\n",
      "Training Data****************************************\n",
      "theta:[[153341.44168125  42596.81789004  29721.10526184]]\n",
      "loss in cost function:1439668706.5858884\n",
      "Validation Data****************************************\n",
      "theta:[[153341.44168125  42596.81789004  29721.10526184]]\n",
      "loss in cost function:1678632573.1265388\n",
      "Training Data****************************************\n",
      "theta:[[153474.74514065  42628.09066221  29737.91114143]]\n",
      "loss in cost function:1435872285.0629122\n",
      "Validation Data****************************************\n",
      "theta:[[153474.74514065  42628.09066221  29737.91114143]]\n",
      "loss in cost function:1674326504.000651\n",
      "Training Data****************************************\n",
      "theta:[[153607.38208275  42659.18938896  29754.60008937]]\n",
      "loss in cost function:1432114171.337662\n",
      "Validation Data****************************************\n",
      "theta:[[153607.38208275  42659.18938896  29754.60008937]]\n",
      "loss in cost function:1670061907.0673316\n",
      "Training Data****************************************\n",
      "theta:[[153739.35584015  42690.11506355  29771.17287342]]\n",
      "loss in cost function:1428393977.8150756\n",
      "Validation Data****************************************\n",
      "theta:[[153739.35584015  42690.11506355  29771.17287342]]\n",
      "loss in cost function:1665838372.714699\n",
      "Training Data****************************************\n",
      "theta:[[153870.66972876  42720.86867344  29787.63025649]]\n",
      "loss in cost function:1424711320.8338752\n",
      "Validation Data****************************************\n",
      "theta:[[153870.66972876  42720.86867344  29787.63025649]]\n",
      "loss in cost function:1661655495.43895\n",
      "Training Data****************************************\n",
      "theta:[[154001.32704792  42751.45120038  29803.9729966 ]]\n",
      "loss in cost function:1421065820.6264877\n",
      "Validation Data****************************************\n",
      "theta:[[154001.32704792  42751.45120038  29803.9729966 ]]\n",
      "loss in cost function:1657512873.80274\n",
      "Training Data****************************************\n",
      "theta:[[154131.33108049  42781.8636204   29820.20184696]]\n",
      "loss in cost function:1417457101.2793756\n",
      "Validation Data****************************************\n",
      "theta:[[154131.33108049  42781.8636204   29820.20184696]]\n",
      "loss in cost function:1653410110.3939867\n",
      "Training Data****************************************\n",
      "theta:[[154260.6850929   42812.10690386  29836.317556  ]]\n",
      "loss in cost function:1413884790.6938217\n",
      "Validation Data****************************************\n",
      "theta:[[154260.6850929   42812.10690386  29836.317556  ]]\n",
      "loss in cost function:1649346811.7851105\n",
      "Training Data****************************************\n",
      "theta:[[154389.39233524  42842.18201547  29852.32086736]]\n",
      "loss in cost function:1410348520.547049\n",
      "Validation Data****************************************\n",
      "theta:[[154389.39233524  42842.18201547  29852.32086736]]\n",
      "loss in cost function:1645322588.4926665\n",
      "Training Data****************************************\n",
      "theta:[[154517.45604137  42872.08991436  29868.21251998]]\n",
      "loss in cost function:1406847926.2537847\n",
      "Validation Data****************************************\n",
      "theta:[[154517.45604137  42872.08991436  29868.21251998]]\n",
      "loss in cost function:1641337054.9374106\n",
      "Training Data****************************************\n",
      "theta:[[154644.87942897  42901.83155405  29883.99324807]]\n",
      "loss in cost function:1403382646.9281878\n",
      "Validation Data****************************************\n",
      "theta:[[154644.87942897  42901.83155405  29883.99324807]]\n",
      "loss in cost function:1637389829.4047606\n",
      "Training Data****************************************\n",
      "theta:[[154771.66569964  42931.40788256  29899.66378118]]\n",
      "loss in cost function:1399952325.346209\n",
      "Validation Data****************************************\n",
      "theta:[[154771.66569964  42931.40788256  29899.66378118]]\n",
      "loss in cost function:1633480534.0056496\n",
      "Training Data****************************************\n",
      "theta:[[154897.81803895  42960.81984237  29915.22484421]]\n",
      "loss in cost function:1396556607.9082646\n",
      "Validation Data****************************************\n",
      "theta:[[154897.81803895  42960.81984237  29915.22484421]]\n",
      "loss in cost function:1629608794.6378148\n",
      "Training Data****************************************\n",
      "theta:[[155023.33961656  42990.0683705   29930.67715744]]\n",
      "loss in cost function:1393195144.6023698\n",
      "Validation Data****************************************\n",
      "theta:[[155023.33961656  42990.0683705   29930.67715744]]\n",
      "loss in cost function:1625774240.9474485\n",
      "Training Data****************************************\n",
      "theta:[[155148.23358628  43019.15439852  29946.02143656]]\n",
      "loss in cost function:1389867588.9675894\n",
      "Validation Data****************************************\n",
      "theta:[[155148.23358628  43019.15439852  29946.02143656]]\n",
      "loss in cost function:1621976506.2912483\n",
      "Training Data****************************************\n",
      "theta:[[155272.50308616  43048.07885261  29961.25839272]]\n",
      "loss in cost function:1386573598.0578983\n",
      "Validation Data****************************************\n",
      "theta:[[155272.50308616  43048.07885261  29961.25839272]]\n",
      "loss in cost function:1618215227.6988559\n",
      "Training Data****************************************\n",
      "theta:[[155396.15123854  43076.84265354  29976.38873252]]\n",
      "loss in cost function:1383312832.4063907\n",
      "Validation Data****************************************\n",
      "theta:[[155396.15123854  43076.84265354  29976.38873252]]\n",
      "loss in cost function:1614490045.83569\n",
      "Training Data****************************************\n",
      "theta:[[155519.18115015  43105.44671675  29991.41315807]]\n",
      "loss in cost function:1380084955.989873\n",
      "Validation Data****************************************\n",
      "theta:[[155519.18115015  43105.44671675  29991.41315807]]\n",
      "loss in cost function:1610800604.9661376\n",
      "Training Data****************************************\n",
      "theta:[[155641.59591221  43133.89195237  30006.332367  ]]\n",
      "loss in cost function:1376889636.1937919\n",
      "Validation Data****************************************\n",
      "theta:[[155641.59591221  43133.89195237  30006.332367  ]]\n",
      "loss in cost function:1607146552.917135\n",
      "Training Data****************************************\n",
      "theta:[[155763.39860046  43162.17926522  30021.14705249]]\n",
      "loss in cost function:1373726543.777553\n",
      "Validation Data****************************************\n",
      "theta:[[155763.39860046  43162.17926522  30021.14705249]]\n",
      "loss in cost function:1603527541.0421164\n",
      "Training Data****************************************\n",
      "theta:[[155884.59227527  43190.3095549   30035.85790332]]\n",
      "loss in cost function:1370595352.8401747\n",
      "Validation Data****************************************\n",
      "theta:[[155884.59227527  43190.3095549   30035.85790332]]\n",
      "loss in cost function:1599943224.185331\n",
      "Training Data****************************************\n",
      "theta:[[156005.1799817   43218.28371577  30050.46560387]]\n",
      "loss in cost function:1367495740.7862818\n",
      "Validation Data****************************************\n",
      "theta:[[156005.1799817   43218.28371577  30050.46560387]]\n",
      "loss in cost function:1596393260.6465187\n",
      "Training Data****************************************\n",
      "theta:[[156125.1647496   43246.10263698  30064.97083414]]\n",
      "loss in cost function:1364427388.2924871\n",
      "Validation Data****************************************\n",
      "theta:[[156125.1647496   43246.10263698  30064.97083414]]\n",
      "loss in cost function:1592877312.1459615\n",
      "Training Data****************************************\n",
      "theta:[[156244.54959366  43273.76720256  30079.37426982]]\n",
      "loss in cost function:1361389979.274072\n",
      "Validation Data****************************************\n",
      "theta:[[156244.54959366  43273.76720256  30079.37426982]]\n",
      "loss in cost function:1589395043.7898624\n",
      "Training Data****************************************\n",
      "theta:[[156363.3375135   43301.27829138  30093.67658229]]\n",
      "loss in cost function:1358383200.8520272\n",
      "Validation Data****************************************\n",
      "theta:[[156363.3375135   43301.27829138  30093.67658229]]\n",
      "loss in cost function:1585946124.03611\n",
      "Training Data****************************************\n",
      "theta:[[156481.53149374  43328.63677721  30107.87843862]]\n",
      "loss in cost function:1355406743.320437\n",
      "Validation Data****************************************\n",
      "theta:[[156481.53149374  43328.63677721  30107.87843862]]\n",
      "loss in cost function:1582530224.6603684\n",
      "Training Data****************************************\n",
      "theta:[[156599.13450408  43355.84352876  30121.98050166]]\n",
      "loss in cost function:1352460300.1141841\n",
      "Validation Data****************************************\n",
      "theta:[[156599.13450408  43355.84352876  30121.98050166]]\n",
      "loss in cost function:1579147020.7225308\n",
      "Training Data****************************************\n",
      "theta:[[156716.14949937  43382.8994097   30135.98343001]]\n",
      "loss in cost function:1349543567.7769961\n",
      "Validation Data****************************************\n",
      "theta:[[156716.14949937  43382.8994097   30135.98343001]]\n",
      "loss in cost function:1575796190.5334845\n",
      "Training Data****************************************\n",
      "theta:[[156832.57941968  43409.80527867  30149.88787807]]\n",
      "loss in cost function:1346656245.9298038\n",
      "Validation Data****************************************\n",
      "theta:[[156832.57941968  43409.80527867  30149.88787807]]\n",
      "loss in cost function:1572477415.6222653\n",
      "Training Data****************************************\n",
      "theta:[[156948.42719039  43436.56198936  30163.69449609]]\n",
      "loss in cost function:1343798037.2394223\n",
      "Validation Data****************************************\n",
      "theta:[[156948.42719039  43436.56198936  30163.69449609]]\n",
      "loss in cost function:1569190380.7034874\n",
      "Training Data****************************************\n",
      "theta:[[157063.69572224  43463.17039047  30177.40393013]]\n",
      "loss in cost function:1340968647.387589\n",
      "Validation Data****************************************\n",
      "theta:[[157063.69572224  43463.17039047  30177.40393013]]\n",
      "loss in cost function:1565934773.6451595\n",
      "Training Data****************************************\n",
      "theta:[[157178.38791144  43489.6313258   30191.01682216]]\n",
      "loss in cost function:1338167785.0402563\n",
      "Validation Data****************************************\n",
      "theta:[[157178.38791144  43489.6313258   30191.01682216]]\n",
      "loss in cost function:1562710285.436788\n",
      "Training Data****************************************\n",
      "theta:[[157292.50663969  43515.94563426  30204.53381003]]\n",
      "loss in cost function:1335395161.8172562\n",
      "Validation Data****************************************\n",
      "theta:[[157292.50663969  43515.94563426  30204.53381003]]\n",
      "loss in cost function:1559516610.1578226\n",
      "Training Data****************************************\n",
      "theta:[[157406.0547743   43542.11414988  30217.95552753]]\n",
      "loss in cost function:1332650492.262231\n",
      "Validation Data****************************************\n",
      "theta:[[157406.0547743   43542.11414988  30217.95552753]]\n",
      "loss in cost function:1556353444.946424\n",
      "Training Data****************************************\n",
      "theta:[[157519.03516824  43568.13770186  30231.28260441]]\n",
      "loss in cost function:1329933493.8128965\n",
      "Validation Data****************************************\n",
      "theta:[[157519.03516824  43568.13770186  30231.28260441]]\n",
      "loss in cost function:1553220489.9685638\n",
      "Training Data****************************************\n",
      "theta:[[157631.4506602   43594.01711459  30244.51566638]]\n",
      "loss in cost function:1327243886.7716079\n",
      "Validation Data****************************************\n",
      "theta:[[157631.4506602   43594.01711459  30244.51566638]]\n",
      "loss in cost function:1550117448.3873858\n",
      "Training Data****************************************\n",
      "theta:[[157743.30407471  43619.75320767  30257.65533517]]\n",
      "loss in cost function:1324581394.2762122\n",
      "Validation Data****************************************\n",
      "theta:[[157743.30407471  43619.75320767  30257.65533517]]\n",
      "loss in cost function:1547044026.3329525\n",
      "Training Data****************************************\n",
      "theta:[[157854.59822215  43645.34679598  30270.70222853]]\n",
      "loss in cost function:1321945742.2712107\n",
      "Validation Data****************************************\n",
      "theta:[[157854.59822215  43645.34679598  30270.70222853]]\n",
      "loss in cost function:1543999932.872247\n",
      "Training Data****************************************\n",
      "theta:[[157965.33589884  43670.79868963  30283.65696027]]\n",
      "loss in cost function:1319336659.4791944\n",
      "Validation Data****************************************\n",
      "theta:[[157965.33589884  43670.79868963  30283.65696027]]\n",
      "loss in cost function:1540984879.9794981\n",
      "Training Data****************************************\n",
      "theta:[[158075.51988716  43696.10969407  30296.52014027]]\n",
      "loss in cost function:1316753877.3726273\n",
      "Validation Data****************************************\n",
      "theta:[[158075.51988716  43696.10969407  30296.52014027]]\n",
      "loss in cost function:1537998582.5068111\n",
      "Training Data****************************************\n",
      "theta:[[158185.15295553  43721.28061007  30309.29237453]]\n",
      "loss in cost function:1314197130.1458309\n",
      "Validation Data****************************************\n",
      "theta:[[158185.15295553  43721.28061007  30309.29237453]]\n",
      "loss in cost function:1535040758.1550963\n",
      "Training Data****************************************\n",
      "theta:[[158294.23785856  43746.31223375  30321.97426516]]\n",
      "loss in cost function:1311666154.687344\n",
      "Validation Data****************************************\n",
      "theta:[[158294.23785856  43746.31223375  30321.97426516]]\n",
      "loss in cost function:1532111127.4452705\n",
      "Training Data****************************************\n",
      "theta:[[158402.77733708  43771.20535662  30334.56641042]]\n",
      "loss in cost function:1309160690.5524876\n",
      "Validation Data****************************************\n",
      "theta:[[158402.77733708  43771.20535662  30334.56641042]]\n",
      "loss in cost function:1529209413.6898012\n",
      "Training Data****************************************\n",
      "theta:[[158510.7741182   43795.96076562  30347.06940477]]\n",
      "loss in cost function:1306680479.9362695\n",
      "Validation Data****************************************\n",
      "theta:[[158510.7741182   43795.96076562  30347.06940477]]\n",
      "loss in cost function:1526335342.9644926\n",
      "Training Data****************************************\n",
      "theta:[[158618.23091542  43820.5792431   30359.48383884]]\n",
      "loss in cost function:1304225267.6465404\n",
      "Validation Data****************************************\n",
      "theta:[[158618.23091542  43820.5792431   30359.48383884]]\n",
      "loss in cost function:1523488644.0805774\n",
      "Training Data****************************************\n",
      "theta:[[158725.15042865  43845.0615669   30371.81029951]]\n",
      "loss in cost function:1301794801.0774076\n",
      "Validation Data****************************************\n",
      "theta:[[158725.15042865  43845.0615669   30371.81029951]]\n",
      "loss in cost function:1520669048.5571008\n",
      "Training Data****************************************\n",
      "theta:[[158831.53534431  43869.40851034  30384.04936988]]\n",
      "loss in cost function:1299388830.1829555\n",
      "Validation Data****************************************\n",
      "theta:[[158831.53534431  43869.40851034  30384.04936988]]\n",
      "loss in cost function:1517876290.5935712\n",
      "Training Data****************************************\n",
      "theta:[[158937.3883354   43893.62084228  30396.20162936]]\n",
      "loss in cost function:1297007107.4512086\n",
      "Validation Data****************************************\n",
      "theta:[[158937.3883354   43893.62084228  30396.20162936]]\n",
      "loss in cost function:1515110107.0429006\n",
      "Training Data****************************************\n",
      "theta:[[159042.71206153  43917.69932709  30408.2676536 ]]\n",
      "loss in cost function:1294649387.878361\n",
      "Validation Data****************************************\n",
      "theta:[[159042.71206153  43917.69932709  30408.2676536 ]]\n",
      "loss in cost function:1512370237.3846064\n",
      "Training Data****************************************\n",
      "theta:[[159147.50916903  43941.64472475  30420.24801462]]\n",
      "loss in cost function:1292315428.9432907\n",
      "Validation Data****************************************\n",
      "theta:[[159147.50916903  43941.64472475  30420.24801462]]\n",
      "loss in cost function:1509656423.6983082\n",
      "Training Data****************************************\n",
      "theta:[[159251.78229099  43965.45779082  30432.14328075]]\n",
      "loss in cost function:1290004990.5823104\n",
      "Validation Data****************************************\n",
      "theta:[[159251.78229099  43965.45779082  30432.14328075]]\n",
      "loss in cost function:1506968410.6374674\n",
      "Training Data****************************************\n",
      "theta:[[159355.53404735  43989.13927648  30443.9540167 ]]\n",
      "loss in cost function:1287717835.1641822\n",
      "Validation Data****************************************\n",
      "theta:[[159355.53404735  43989.13927648  30443.9540167 ]]\n",
      "loss in cost function:1504305945.4034176\n",
      "Training Data****************************************\n",
      "theta:[[159458.76704492  44012.68992858  30455.68078354]]\n",
      "loss in cost function:1285453727.4653993\n",
      "Validation Data****************************************\n",
      "theta:[[159458.76704492  44012.68992858  30455.68078354]]\n",
      "loss in cost function:1501668777.7196455\n",
      "Training Data****************************************\n",
      "theta:[[159561.4838775   44036.11048961  30467.32413877]]\n",
      "loss in cost function:1283212434.6456935\n",
      "Validation Data****************************************\n",
      "theta:[[159561.4838775   44036.11048961  30467.32413877]]\n",
      "loss in cost function:1499056659.8063323\n",
      "Training Data****************************************\n",
      "theta:[[159663.68712592  44059.40169781  30478.88463633]]\n",
      "loss in cost function:1280993726.2238326\n",
      "Validation Data****************************************\n",
      "theta:[[159663.68712592  44059.40169781  30478.88463633]]\n",
      "loss in cost function:1496469346.3551712\n",
      "Training Data****************************************\n",
      "theta:[[159765.3793581  44082.5642871  30490.3628266]]\n",
      "loss in cost function:1278797374.0536199\n",
      "Validation Data****************************************\n",
      "theta:[[159765.3793581  44082.5642871  30490.3628266]]\n",
      "loss in cost function:1493906594.504418\n",
      "Training Data****************************************\n",
      "theta:[[159866.56312912  44105.59898718  30501.75925642]]\n",
      "loss in cost function:1276623152.3001785\n",
      "Validation Data****************************************\n",
      "theta:[[159866.56312912  44105.59898718  30501.75925642]]\n",
      "loss in cost function:1491368163.8142095\n",
      "Training Data****************************************\n",
      "theta:[[159967.24098128  44128.50652351  30513.07446916]]\n",
      "loss in cost function:1274470837.416439\n",
      "Validation Data****************************************\n",
      "theta:[[159967.24098128  44128.50652351  30513.07446916]]\n",
      "loss in cost function:1488853816.2421305\n",
      "Training Data****************************************\n",
      "theta:[[160067.41544418  44151.28761736  30524.3090047 ]]\n",
      "loss in cost function:1272340208.1199088\n",
      "Validation Data****************************************\n",
      "theta:[[160067.41544418  44151.28761736  30524.3090047 ]]\n",
      "loss in cost function:1486363316.119029\n",
      "Training Data****************************************\n",
      "theta:[[160167.08903477  44173.94298583  30535.46339944]]\n",
      "loss in cost function:1270231045.3696399\n",
      "Validation Data****************************************\n",
      "theta:[[160167.08903477  44173.94298583  30535.46339944]]\n",
      "loss in cost function:1483896430.1250806\n",
      "Training Data****************************************\n",
      "theta:[[160266.26425741  44196.47334185  30546.53818637]]\n",
      "loss in cost function:1268143132.3434587\n",
      "Validation Data****************************************\n",
      "theta:[[160266.26425741  44196.47334185  30546.53818637]]\n",
      "loss in cost function:1481452927.266084\n",
      "Training Data****************************************\n",
      "theta:[[160364.94360393  44218.87939425  30557.53389505]]\n",
      "loss in cost function:1266076254.4154165\n",
      "Validation Data****************************************\n",
      "theta:[[160364.94360393  44218.87939425  30557.53389505]]\n",
      "loss in cost function:1479032578.8500133\n",
      "Training Data****************************************\n",
      "theta:[[160463.12955371  44241.16184774  30568.45105166]]\n",
      "loss in cost function:1264030199.133465\n",
      "Validation Data****************************************\n",
      "theta:[[160463.12955371  44241.16184774  30568.45105166]]\n",
      "loss in cost function:1476635158.4638016\n",
      "Training Data****************************************\n",
      "theta:[[160560.82457375  44263.32140296  30579.290179  ]]\n",
      "loss in cost function:1262004756.1973827\n",
      "Validation Data****************************************\n",
      "theta:[[160560.82457375  44263.32140296  30579.290179  ]]\n",
      "loss in cost function:1474260441.9503522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data****************************************\n",
      "theta:[[160658.03111869  44285.35875651  30590.05179651]]\n",
      "loss in cost function:1259999717.4368813\n",
      "Validation Data****************************************\n",
      "theta:[[160658.03111869  44285.35875651  30590.05179651]]\n",
      "loss in cost function:1471908207.3858075\n",
      "Training Data****************************************\n",
      "theta:[[160754.75163091  44307.27460092  30600.73642031]]\n",
      "loss in cost function:1258014876.7899997\n",
      "Validation Data****************************************\n",
      "theta:[[160754.75163091  44307.27460092  30600.73642031]]\n",
      "loss in cost function:1469578235.057021\n",
      "Training Data****************************************\n",
      "theta:[[160850.98854056  44329.06962477  30611.34456321]]\n",
      "loss in cost function:1256050030.2816393\n",
      "Validation Data****************************************\n",
      "theta:[[160850.98854056  44329.06962477  30611.34456321]]\n",
      "loss in cost function:1467270307.4392748\n",
      "Training Data****************************************\n",
      "theta:[[160946.74426567  44350.74451261  30621.87673474]]\n",
      "loss in cost function:1254104976.0024152\n",
      "Validation Data****************************************\n",
      "theta:[[160946.74426567  44350.74451261  30621.87673474]]\n",
      "loss in cost function:1464984209.1742225\n",
      "Training Data****************************************\n",
      "theta:[[161042.02121215  44372.29994505  30632.33344113]]\n",
      "loss in cost function:1252179514.087619\n",
      "Validation Data****************************************\n",
      "theta:[[161042.02121215  44372.29994505  30632.33344113]]\n",
      "loss in cost function:1462719727.048048\n",
      "Training Data****************************************\n",
      "theta:[[161136.8217739   44393.73659878  30642.71518541]]\n",
      "loss in cost function:1250273446.6964731\n",
      "Validation Data****************************************\n",
      "theta:[[161136.8217739   44393.73659878  30642.71518541]]\n",
      "loss in cost function:1460476649.9698534\n",
      "Training Data****************************************\n",
      "theta:[[161231.14833283  44415.05514654  30653.02246734]]\n",
      "loss in cost function:1248386577.9915657\n",
      "Validation Data****************************************\n",
      "theta:[[161231.14833283  44415.05514654  30653.02246734]]\n",
      "loss in cost function:1458254768.9502733\n",
      "Training Data****************************************\n",
      "theta:[[161325.00325898  44436.25625722  30663.2557835 ]]\n",
      "loss in cost function:1246518714.1185017\n",
      "Validation Data****************************************\n",
      "theta:[[161325.00325898  44436.25625722  30663.2557835 ]]\n",
      "loss in cost function:1456053877.0802877\n",
      "Training Data****************************************\n",
      "theta:[[161418.38891049  44457.34059582  30673.41562728]]\n",
      "loss in cost function:1244669663.185747\n",
      "Validation Data****************************************\n",
      "theta:[[161418.38891049  44457.34059582  30673.41562728]]\n",
      "loss in cost function:1453873769.5102599\n",
      "Training Data****************************************\n",
      "theta:[[161511.30763375  44478.3088235   30683.50248889]]\n",
      "loss in cost function:1242839235.2447019\n",
      "Validation Data****************************************\n",
      "theta:[[161511.30763375  44478.3088235   30683.50248889]]\n",
      "loss in cost function:1451714243.429199\n",
      "Training Data****************************************\n",
      "theta:[[161603.76176339  44499.1615976   30693.5168554 ]]\n",
      "loss in cost function:1241027242.2699602\n",
      "Validation Data****************************************\n",
      "theta:[[161603.76176339  44499.1615976   30693.5168554 ]]\n",
      "loss in cost function:1449575098.0442111\n",
      "Training Data****************************************\n",
      "theta:[[161695.75362238  44519.89957165  30703.45921077]]\n",
      "loss in cost function:1239233498.1397905\n",
      "Validation Data****************************************\n",
      "theta:[[161695.75362238  44519.89957165  30703.45921077]]\n",
      "loss in cost function:1447456134.5601788\n",
      "Training Data****************************************\n",
      "theta:[[161787.28552207  44540.52339543  30713.33003582]]\n",
      "loss in cost function:1237457818.6167777\n",
      "Validation Data****************************************\n",
      "theta:[[161787.28552207  44540.52339543  30713.33003582]]\n",
      "loss in cost function:1445357156.1596317\n",
      "Training Data****************************************\n",
      "theta:[[161878.35976227  44561.03371494  30723.12980831]]\n",
      "loss in cost function:1235700021.328691\n",
      "Validation Data****************************************\n",
      "theta:[[161878.35976227  44561.03371494  30723.12980831]]\n",
      "loss in cost function:1443277967.9828339\n",
      "Training Data****************************************\n",
      "theta:[[161968.97863127  44581.43117246  30732.85900291]]\n",
      "loss in cost function:1233959925.7495654\n",
      "Validation Data****************************************\n",
      "theta:[[161968.97863127  44581.43117246  30732.85900291]]\n",
      "loss in cost function:1441218377.108059\n",
      "Training Data****************************************\n",
      "theta:[[162059.14440592  44601.71640655  30742.51809126]]\n",
      "loss in cost function:1232237353.180909\n",
      "Validation Data****************************************\n",
      "theta:[[162059.14440592  44601.71640655  30742.51809126]]\n",
      "loss in cost function:1439178192.5320897\n",
      "Training Data****************************************\n",
      "theta:[[162148.8593517   44621.89005209  30752.10754197]]\n",
      "loss in cost function:1230532126.733199\n",
      "Validation Data****************************************\n",
      "theta:[[162148.8593517   44621.89005209  30752.10754197]]\n",
      "loss in cost function:1437157225.1508887\n",
      "Training Data****************************************\n",
      "theta:[[162238.12572275  44641.95274029  30761.62782062]]\n",
      "loss in cost function:1228844071.3074563\n",
      "Validation Data****************************************\n",
      "theta:[[162238.12572275  44641.95274029  30761.62782062]]\n",
      "loss in cost function:1435155287.740481\n",
      "Training Data****************************************\n",
      "theta:[[162326.94576194  44661.90509872  30771.07938981]]\n",
      "loss in cost function:1227173013.5771\n",
      "Validation Data****************************************\n",
      "theta:[[162326.94576194  44661.90509872  30771.07938981]]\n",
      "loss in cost function:1433172194.9380255\n",
      "Training Data****************************************\n",
      "theta:[[162415.32170094  44681.7477513   30780.46270917]]\n",
      "loss in cost function:1225518781.9699516\n",
      "Validation Data****************************************\n",
      "theta:[[162415.32170094  44681.7477513   30780.46270917]]\n",
      "loss in cost function:1431207763.2230825\n",
      "Training Data****************************************\n",
      "theta:[[162503.25576025  44701.48131838  30789.77823537]]\n",
      "loss in cost function:1223881206.6503856\n",
      "Validation Data****************************************\n",
      "theta:[[162503.25576025  44701.48131838  30789.77823537]]\n",
      "loss in cost function:1429261810.899071\n",
      "Training Data****************************************\n",
      "theta:[[162590.75014925  44721.1064167   30799.02642215]]\n",
      "loss in cost function:1222260119.5017493\n",
      "Validation Data****************************************\n",
      "theta:[[162590.75014925  44721.1064167   30799.02642215]]\n",
      "loss in cost function:1427334158.0749054\n",
      "Training Data****************************************\n",
      "theta:[[162677.80706631  44740.62365947  30808.20772031]]\n",
      "loss in cost function:1220655354.1088662\n",
      "Validation Data****************************************\n",
      "theta:[[162677.80706631  44740.62365947  30808.20772031]]\n",
      "loss in cost function:1425424626.6468344\n",
      "Training Data****************************************\n",
      "theta:[[162764.42869879  44760.03365632  30817.32257779]]\n",
      "loss in cost function:1219066745.740782\n",
      "Validation Data****************************************\n",
      "theta:[[162764.42869879  44760.03365632  30817.32257779]]\n",
      "loss in cost function:1423533040.2804432\n",
      "Training Data****************************************\n",
      "theta:[[162850.61722311  44779.33701341  30826.37143962]]\n",
      "loss in cost function:1217494131.33367\n",
      "Validation Data****************************************\n",
      "theta:[[162850.61722311  44779.33701341  30826.37143962]]\n",
      "loss in cost function:1421659224.392869\n",
      "Training Data****************************************\n",
      "theta:[[162936.3748048   44798.53433335  30835.35474797]]\n",
      "loss in cost function:1215937349.4738994\n",
      "Validation Data****************************************\n",
      "theta:[[162936.3748048   44798.53433335  30835.35474797]]\n",
      "loss in cost function:1419803006.135161\n",
      "Training Data****************************************\n",
      "theta:[[163021.70359858  44817.6262153   30844.27294217]]\n",
      "loss in cost function:1214396240.3812788\n",
      "Validation Data****************************************\n",
      "theta:[[163021.70359858  44817.6262153   30844.27294217]]\n",
      "loss in cost function:1417964214.374844\n",
      "Training Data****************************************\n",
      "theta:[[163106.6057484   44836.61325496  30853.12645872]]\n",
      "loss in cost function:1212870645.892486\n",
      "Validation Data****************************************\n",
      "theta:[[163106.6057484   44836.61325496  30853.12645872]]\n",
      "loss in cost function:1416142679.6786542\n",
      "Training Data****************************************\n",
      "theta:[[163191.08338746  44855.49604459  30861.91573133]]\n",
      "loss in cost function:1211360409.4446511\n",
      "Validation Data****************************************\n",
      "theta:[[163191.08338746  44855.49604459  30861.91573133]]\n",
      "loss in cost function:1414338234.2954457\n",
      "Training Data****************************************\n",
      "theta:[[163275.13863833  44874.27517303  30870.64119089]]\n",
      "loss in cost function:1209865376.0591035\n",
      "Validation Data****************************************\n",
      "theta:[[163275.13863833  44874.27517303  30870.64119089]]\n",
      "loss in cost function:1412550712.1392727\n",
      "Training Data****************************************\n",
      "theta:[[163358.77361295  44892.95122572  30879.30326552]]\n",
      "loss in cost function:1208385392.3253121\n",
      "Validation Data****************************************\n",
      "theta:[[163358.77361295  44892.95122572  30879.30326552]]\n",
      "loss in cost function:1410779948.7726424\n",
      "Training Data****************************************\n",
      "theta:[[163441.99041269  44911.52478473  30887.9023806 ]]\n",
      "loss in cost function:1206920306.3849487\n",
      "Validation Data****************************************\n",
      "theta:[[163441.99041269  44911.52478473  30887.9023806 ]]\n",
      "loss in cost function:1409025781.3899412\n",
      "Training Data****************************************\n",
      "theta:[[163524.79112844  44929.99642877  30896.43895877]]\n",
      "loss in cost function:1205469967.9161468\n",
      "Validation Data****************************************\n",
      "theta:[[163524.79112844  44929.99642877  30896.43895877]]\n",
      "loss in cost function:1407288048.8010306\n",
      "Training Data****************************************\n",
      "theta:[[163607.17784061  44948.36673321  30904.91341992]]\n",
      "loss in cost function:1204034228.1179025\n",
      "Validation Data****************************************\n",
      "theta:[[163607.17784061  44948.36673321  30904.91341992]]\n",
      "loss in cost function:1405566591.4149907\n",
      "Training Data****************************************\n",
      "theta:[[163689.15261921  44966.63627011  30913.32618127]]\n",
      "loss in cost function:1202612939.6946487\n",
      "Validation Data****************************************\n",
      "theta:[[163689.15261921  44966.63627011  30913.32618127]]\n",
      "loss in cost function:1403861251.2240617\n",
      "Training Data****************************************\n",
      "theta:[[163770.71752392  44984.8056082   30921.67765734]]\n",
      "loss in cost function:1201205956.8409584\n",
      "Validation Data****************************************\n",
      "theta:[[163770.71752392  44984.8056082   30921.67765734]]\n",
      "loss in cost function:1402171871.7877154\n",
      "Training Data****************************************\n",
      "theta:[[163851.87460411  45002.87531297  30929.96825996]]\n",
      "loss in cost function:1199813135.2264545\n",
      "Validation Data****************************************\n",
      "theta:[[163851.87460411  45002.87531297  30929.96825996]]\n",
      "loss in cost function:1400498298.2169144\n",
      "Training Data****************************************\n",
      "theta:[[163932.6258989   45020.84594663  30938.19839834]]\n",
      "loss in cost function:1198434331.9808097\n",
      "Validation Data****************************************\n",
      "theta:[[163932.6258989   45020.84594663  30938.19839834]]\n",
      "loss in cost function:1398840377.158511\n",
      "Training Data****************************************\n",
      "theta:[[164012.97343721  45038.71806815  30946.36847902]]\n",
      "loss in cost function:1197069405.6789525\n",
      "Validation Data****************************************\n",
      "theta:[[164012.97343721  45038.71806815  30946.36847902]]\n",
      "loss in cost function:1397197956.779818\n",
      "Training Data****************************************\n",
      "theta:[[164092.91923784  45056.49223327  30954.47890594]]\n",
      "loss in cost function:1195718216.3263888\n",
      "Validation Data****************************************\n",
      "theta:[[164092.91923784  45056.49223327  30954.47890594]]\n",
      "loss in cost function:1395570886.7533257\n",
      "Training Data****************************************\n",
      "theta:[[164172.46530945  45074.16899454  30962.53008042]]\n",
      "loss in cost function:1194380625.3447053\n",
      "Validation Data****************************************\n",
      "theta:[[164172.46530945  45074.16899454  30962.53008042]]\n",
      "loss in cost function:1393959018.2415876\n",
      "Training Data****************************************\n",
      "theta:[[164251.61365072  45091.74890132  30970.52240122]]\n",
      "loss in cost function:1193056495.5571868\n",
      "Validation Data****************************************\n",
      "theta:[[164251.61365072  45091.74890132  30970.52240122]]\n",
      "loss in cost function:1392362203.882238\n",
      "Training Data****************************************\n",
      "theta:[[164330.36625027  45109.2324998   30978.4562645 ]]\n",
      "loss in cost function:1191745691.1745903\n",
      "Validation Data****************************************\n",
      "theta:[[164330.36625027  45109.2324998   30978.4562645 ]]\n",
      "loss in cost function:1390780297.7731826\n",
      "Training Data****************************************\n",
      "theta:[[164408.72508683  45126.62033303  30986.33206387]]\n",
      "loss in cost function:1190448077.781084\n",
      "Validation Data****************************************\n",
      "theta:[[164408.72508683  45126.62033303  30986.33206387]]\n",
      "loss in cost function:1389213155.4579325\n",
      "Training Data****************************************\n",
      "theta:[[164486.6921292   45143.91294092  30994.15019041]]\n",
      "loss in cost function:1189163522.3203142\n",
      "Validation Data****************************************\n",
      "theta:[[164486.6921292   45143.91294092  30994.15019041]]\n",
      "loss in cost function:1387660633.9110777\n",
      "Training Data****************************************\n",
      "theta:[[164564.26933636  45161.11086027  31001.91103267]]\n",
      "loss in cost function:1187891893.081603\n",
      "Validation Data****************************************\n",
      "theta:[[164564.26933636  45161.11086027  31001.91103267]]\n",
      "loss in cost function:1386122591.5239186\n",
      "Training Data****************************************\n",
      "theta:[[164641.45865749  45178.21462481  31009.6149767 ]]\n",
      "loss in cost function:1186633059.686298\n",
      "Validation Data****************************************\n",
      "theta:[[164641.45865749  45178.21462481  31009.6149767 ]]\n",
      "loss in cost function:1384598888.0902414\n",
      "Training Data****************************************\n",
      "theta:[[164718.26203201  45195.22476516  31017.26240604]]\n",
      "loss in cost function:1185386893.0742714\n",
      "Validation Data****************************************\n",
      "theta:[[164718.26203201  45195.22476516  31017.26240604]]\n",
      "loss in cost function:1383089384.7922382\n",
      "Training Data****************************************\n",
      "theta:[[164794.68138966  45212.14180891  31024.85370177]]\n",
      "loss in cost function:1184153265.4905457\n",
      "Validation Data****************************************\n",
      "theta:[[164794.68138966  45212.14180891  31024.85370177]]\n",
      "loss in cost function:1381593944.1865566\n",
      "Training Data****************************************\n",
      "theta:[[164870.71865052  45228.96628059  31032.3892425 ]]\n",
      "loss in cost function:1182932050.4720323\n",
      "Validation Data****************************************\n",
      "theta:[[164870.71865052  45228.96628059  31032.3892425 ]]\n",
      "loss in cost function:1380112430.1905084\n",
      "Training Data****************************************\n",
      "theta:[[164946.37572507  45245.69870173  31039.86940442]]\n",
      "loss in cost function:1181723122.834462\n",
      "Validation Data****************************************\n",
      "theta:[[164946.37572507  45245.69870173  31039.86940442]]\n",
      "loss in cost function:1378644708.068422\n",
      "Training Data****************************************\n",
      "theta:[[165021.65451426  45262.33959085  31047.29456125]]\n",
      "loss in cost function:1180526358.6593893\n",
      "Validation Data****************************************\n",
      "theta:[[165021.65451426  45262.33959085  31047.29456125]]\n",
      "loss in cost function:1377190644.4181004\n",
      "Training Data****************************************\n",
      "theta:[[165096.55690949  45278.88946347  31054.66508432]]\n",
      "loss in cost function:1179341635.2813656\n",
      "Validation Data****************************************\n",
      "theta:[[165096.55690949  45278.88946347  31054.66508432]]\n",
      "loss in cost function:1375750107.1574578\n",
      "Training Data****************************************\n",
      "theta:[[165171.08479275  45295.34883216  31061.98134257]]\n",
      "loss in cost function:1178168831.2752337\n",
      "Validation Data****************************************\n",
      "theta:[[165171.08479275  45295.34883216  31061.98134257]]\n",
      "loss in cost function:1374322965.5112607\n",
      "Training Data****************************************\n",
      "theta:[[165245.2400366   45311.71820654  31069.24370254]]\n",
      "loss in cost function:1177007826.4435399\n",
      "Validation Data****************************************\n",
      "theta:[[165245.2400366   45311.71820654  31069.24370254]]\n",
      "loss in cost function:1372909089.9980154\n",
      "Training Data****************************************\n",
      "theta:[[165319.02450422  45327.99809327  31076.4525284 ]]\n",
      "loss in cost function:1175858501.8040905\n",
      "Validation Data****************************************\n",
      "theta:[[165319.02450422  45327.99809327  31076.4525284 ]]\n",
      "loss in cost function:1371508352.4169939\n",
      "Training Data****************************************\n",
      "theta:[[165392.44004951  45344.18899612  31083.60818197]]\n",
      "loss in cost function:1174720739.5776398\n",
      "Validation Data****************************************\n",
      "theta:[[165392.44004951  45344.18899612  31083.60818197]]\n",
      "loss in cost function:1370120625.835378\n",
      "Training Data****************************************\n",
      "theta:[[165465.48851707  45360.29141596  31090.71102273]]\n",
      "loss in cost function:1173594423.17566\n",
      "Validation Data****************************************\n",
      "theta:[[165465.48851707  45360.29141596  31090.71102273]]\n",
      "loss in cost function:1368745784.575549\n",
      "Training Data****************************************\n",
      "theta:[[165538.17174229  45376.30585076  31097.76140785]]\n",
      "loss in cost function:1172479437.1883025\n",
      "Validation Data****************************************\n",
      "theta:[[165538.17174229  45376.30585076  31097.76140785]]\n",
      "loss in cost function:1367383704.202486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data****************************************\n",
      "theta:[[165610.49155139  45392.23279564  31104.75969217]]\n",
      "loss in cost function:1171375667.3724272\n",
      "Validation Data****************************************\n",
      "theta:[[165610.49155139  45392.23279564  31104.75969217]]\n",
      "loss in cost function:1366034261.5113087\n",
      "Training Data****************************************\n",
      "theta:[[165682.44976144  45408.07274286  31111.70622824]]\n",
      "loss in cost function:1170283000.6397781\n",
      "Validation Data****************************************\n",
      "theta:[[165682.44976144  45408.07274286  31111.70622824]]\n",
      "loss in cost function:1364697334.5149488\n",
      "Training Data****************************************\n",
      "theta:[[165754.04818044  45423.82618185  31118.60136633]]\n",
      "loss in cost function:1169201325.0452724\n",
      "Validation Data****************************************\n",
      "theta:[[165754.04818044  45423.82618185  31118.60136633]]\n",
      "loss in cost function:1363372802.4319253\n",
      "Training Data****************************************\n",
      "theta:[[165825.28860735  45439.49359924  31125.44545445]]\n",
      "loss in cost function:1168130529.775416\n",
      "Validation Data****************************************\n",
      "theta:[[165825.28860735  45439.49359924  31125.44545445]]\n",
      "loss in cost function:1362060545.674263\n",
      "Training Data****************************************\n",
      "theta:[[165896.17283212  45455.07547883  31132.23883836]]\n",
      "loss in cost function:1167070505.1368175\n",
      "Validation Data****************************************\n",
      "theta:[[165896.17283212  45455.07547883  31132.23883836]]\n",
      "loss in cost function:1360760445.835527\n",
      "Training Data****************************************\n",
      "theta:[[165966.70263577  45470.57230168  31138.98186156]]\n",
      "loss in cost function:1166021142.5448558\n",
      "Validation Data****************************************\n",
      "theta:[[165966.70263577  45470.57230168  31138.98186156]]\n",
      "loss in cost function:1359472385.6789792\n",
      "Training Data****************************************\n",
      "theta:[[166036.8797904   45485.98454603  31145.67486536]]\n",
      "loss in cost function:1164982334.5124226\n",
      "Validation Data****************************************\n",
      "theta:[[166036.8797904   45485.98454603  31145.67486536]]\n",
      "loss in cost function:1358196249.1258492\n",
      "Training Data****************************************\n",
      "theta:[[166106.70605925  45501.31268742  31152.31818883]]\n",
      "loss in cost function:1163953974.6387906\n",
      "Validation Data****************************************\n",
      "theta:[[166106.70605925  45501.31268742  31152.31818883]]\n",
      "loss in cost function:1356931921.2437336\n",
      "Training Data****************************************\n",
      "theta:[[166176.18319677  45516.55719863  31158.91216886]]\n",
      "loss in cost function:1162935957.5986311\n",
      "Validation Data****************************************\n",
      "theta:[[166176.18319677  45516.55719863  31158.91216886]]\n",
      "loss in cost function:1355679288.2351048\n",
      "Training Data****************************************\n",
      "theta:[[166245.31294859  45531.71854971  31165.45714016]]\n",
      "loss in cost function:1161928179.1310763\n",
      "Validation Data****************************************\n",
      "theta:[[166245.31294859  45531.71854971  31165.45714016]]\n",
      "loss in cost function:1354438237.425938\n",
      "Training Data****************************************\n",
      "theta:[[166314.09705166  45546.79720804  31171.95343526]]\n",
      "loss in cost function:1160930536.0289576\n",
      "Validation Data****************************************\n",
      "theta:[[166314.09705166  45546.79720804  31171.95343526]]\n",
      "loss in cost function:1353208657.2544565\n",
      "Training Data****************************************\n",
      "theta:[[166382.53723421  45561.79363829  31178.40138455]]\n",
      "loss in cost function:1159942926.128116\n",
      "Validation Data****************************************\n",
      "theta:[[166382.53723421  45561.79363829  31178.40138455]]\n",
      "loss in cost function:1351990437.2599885\n",
      "Training Data****************************************\n",
      "theta:[[166450.63521584  45576.70830247  31184.80131626]]\n",
      "loss in cost function:1158965248.2968254\n",
      "Validation Data****************************************\n",
      "theta:[[166450.63521584  45576.70830247  31184.80131626]]\n",
      "loss in cost function:1350783468.0719326\n",
      "Training Data****************************************\n",
      "theta:[[166518.39270757  45591.54165992  31191.15355651]]\n",
      "loss in cost function:1157997402.4253323\n",
      "Validation Data****************************************\n",
      "theta:[[166518.39270757  45591.54165992  31191.15355651]]\n",
      "loss in cost function:1349587641.3988426\n",
      "Training Data****************************************\n",
      "theta:[[166585.81141184  45606.29416736  31197.45842931]]\n",
      "loss in cost function:1157039289.4155147\n",
      "Validation Data****************************************\n",
      "theta:[[166585.81141184  45606.29416736  31197.45842931]]\n",
      "loss in cost function:1348402850.017618\n",
      "Training Data****************************************\n",
      "theta:[[166652.89302259  45620.96627887  31203.71625655]]\n",
      "loss in cost function:1156090811.1706018\n",
      "Validation Data****************************************\n",
      "theta:[[166652.89302259  45620.96627887  31203.71625655]]\n",
      "loss in cost function:1347228987.7628038\n",
      "Training Data****************************************\n",
      "theta:[[166719.63922529  45635.55844593  31209.92735804]]\n",
      "loss in cost function:1155151870.585047\n",
      "Validation Data****************************************\n",
      "theta:[[166719.63922529  45635.55844593  31209.92735804]]\n",
      "loss in cost function:1346065949.516003\n",
      "Training Data****************************************\n",
      "theta:[[166786.05169697  45650.07111742  31216.09205153]]\n",
      "loss in cost function:1154222371.5344744\n",
      "Validation Data****************************************\n",
      "theta:[[166786.05169697  45650.07111742  31216.09205153]]\n",
      "loss in cost function:1344913631.1953852\n",
      "Training Data****************************************\n",
      "theta:[[166852.13210629  45664.50473964  31222.21065269]]\n",
      "loss in cost function:1153302218.8657355\n",
      "Validation Data****************************************\n",
      "theta:[[166852.13210629  45664.50473964  31222.21065269]]\n",
      "loss in cost function:1343771929.7453187\n",
      "Training Data****************************************\n",
      "theta:[[166917.88211357  45678.85975634  31228.28347514]]\n",
      "loss in cost function:1152391318.3870738\n",
      "Validation Data****************************************\n",
      "theta:[[166917.88211357  45678.85975634  31228.28347514]]\n",
      "loss in cost function:1342640743.1260912\n",
      "Training Data****************************************\n",
      "theta:[[166983.30337081  45693.13660871  31234.3108305 ]]\n",
      "loss in cost function:1151489576.858384\n",
      "Validation Data****************************************\n",
      "theta:[[166983.30337081  45693.13660871  31234.3108305 ]]\n",
      "loss in cost function:1341519970.3037436\n",
      "Training Data****************************************\n",
      "theta:[[167048.39752176  45707.3357354   31240.29302832]]\n",
      "loss in cost function:1150596901.9815598\n",
      "Validation Data****************************************\n",
      "theta:[[167048.39752176  45707.3357354   31240.29302832]]\n",
      "loss in cost function:1340409511.239994\n",
      "Training Data****************************************\n",
      "theta:[[167113.16620196  45721.45757255  31246.23037618]]\n",
      "loss in cost function:1149713202.39096\n",
      "Validation Data****************************************\n",
      "theta:[[167113.16620196  45721.45757255  31246.23037618]]\n",
      "loss in cost function:1339309266.8822956\n",
      "Training Data****************************************\n",
      "theta:[[167177.61103876  45735.5025538   31252.12317963]]\n",
      "loss in cost function:1148838387.643958\n",
      "Validation Data****************************************\n",
      "theta:[[167177.61103876  45735.5025538   31252.12317963]]\n",
      "loss in cost function:1338219139.1539445\n",
      "Training Data****************************************\n",
      "theta:[[167241.73365138  45749.47111029  31257.97174226]]\n",
      "loss in cost function:1147972368.2115893\n",
      "Validation Data****************************************\n",
      "theta:[[167241.73365138  45749.47111029  31257.97174226]]\n",
      "loss in cost function:1337139030.9443407\n",
      "Training Data****************************************\n",
      "theta:[[167305.53565093  45763.3636707   31263.77636569]]\n",
      "loss in cost function:1147115055.4692872\n",
      "Validation Data****************************************\n",
      "theta:[[167305.53565093  45763.3636707   31263.77636569]]\n",
      "loss in cost function:1336068846.0993066\n",
      "Training Data****************************************\n",
      "theta:[[167369.01864048  45777.18066123  31269.53734956]]\n",
      "loss in cost function:1146266361.6877413\n",
      "Validation Data****************************************\n",
      "theta:[[167369.01864048  45777.18066123  31269.53734956]]\n",
      "loss in cost function:1335008489.4115224\n",
      "Training Data****************************************\n",
      "theta:[[167432.18421509  45790.92250564  31275.25499158]]\n",
      "loss in cost function:1145426200.023814\n",
      "Validation Data****************************************\n",
      "theta:[[167432.18421509  45790.92250564  31275.25499158]]\n",
      "loss in cost function:1333957866.6110654\n",
      "Training Data****************************************\n",
      "theta:[[167495.03396182  45804.58962527  31280.92958751]]\n",
      "loss in cost function:1144594484.511557\n",
      "Validation Data****************************************\n",
      "theta:[[167495.03396182  45804.58962527  31280.92958751]]\n",
      "loss in cost function:1332916884.3560207\n",
      "Training Data****************************************\n",
      "theta:[[167557.56945982  45818.18243903  31286.56143122]]\n",
      "loss in cost function:1143771130.053346\n",
      "Validation Data****************************************\n",
      "theta:[[167557.56945982  45818.18243903  31286.56143122]]\n",
      "loss in cost function:1331885450.2232056\n",
      "Training Data****************************************\n",
      "theta:[[167619.79228033  45831.70136342  31292.15081464]]\n",
      "loss in cost function:1142956052.411068\n",
      "Validation Data****************************************\n",
      "theta:[[167619.79228033  45831.70136342  31292.15081464]]\n",
      "loss in cost function:1330863472.6989872\n",
      "Training Data****************************************\n",
      "theta:[[167681.70398673  45845.14681257  31297.6980278 ]]\n",
      "loss in cost function:1142149168.1974263\n",
      "Validation Data****************************************\n",
      "theta:[[167681.70398673  45845.14681257  31297.6980278 ]]\n",
      "loss in cost function:1329850861.1701827\n",
      "Training Data****************************************\n",
      "theta:[[167743.30613461  45858.51919823  31303.20335887]]\n",
      "loss in cost function:1141350394.8673108\n",
      "Validation Data****************************************\n",
      "theta:[[167743.30613461  45858.51919823  31303.20335887]]\n",
      "loss in cost function:1328847525.9150627\n",
      "Training Data****************************************\n",
      "theta:[[167804.60027174  45871.81892976  31308.66709412]]\n",
      "loss in cost function:1140559650.7092857\n",
      "Validation Data****************************************\n",
      "theta:[[167804.60027174  45871.81892976  31308.66709412]]\n",
      "loss in cost function:1327853378.0944335\n",
      "Training Data****************************************\n",
      "theta:[[167865.58793819  45885.04641421  31314.08951797]]\n",
      "loss in cost function:1139776854.837123\n",
      "Validation Data****************************************\n",
      "theta:[[167865.58793819  45885.04641421  31314.08951797]]\n",
      "loss in cost function:1326868329.7428174\n",
      "Training Data****************************************\n",
      "theta:[[167926.27066631  45898.20205628  31319.470913  ]]\n",
      "loss in cost function:1139001927.1814706\n",
      "Validation Data****************************************\n",
      "theta:[[167926.27066631  45898.20205628  31319.470913  ]]\n",
      "loss in cost function:1325892293.7597284\n",
      "Training Data****************************************\n",
      "theta:[[167986.64998079  45911.28625834  31324.81155992]]\n",
      "loss in cost function:1138234788.4815638\n",
      "Validation Data****************************************\n",
      "theta:[[167986.64998079  45911.28625834  31324.81155992]]\n",
      "loss in cost function:1324925183.9010139\n",
      "Training Data****************************************\n",
      "theta:[[168046.72739869  45924.29942047  31330.11173765]]\n",
      "loss in cost function:1137475360.2770314\n",
      "Validation Data****************************************\n",
      "theta:[[168046.72739869  45924.29942047  31330.11173765]]\n",
      "loss in cost function:1323966914.7703137\n",
      "Training Data****************************************\n",
      "theta:[[168106.50442951  45937.24194044  31335.37172328]]\n",
      "loss in cost function:1136723564.8998125\n",
      "Validation Data****************************************\n",
      "theta:[[168106.50442951  45937.24194044  31335.37172328]]\n",
      "loss in cost function:1323017401.810578\n",
      "Training Data****************************************\n",
      "theta:[[168165.98257517  45950.11421375  31340.59179208]]\n",
      "loss in cost function:1135979325.4661152\n",
      "Validation Data****************************************\n",
      "theta:[[168165.98257517  45950.11421375  31340.59179208]]\n",
      "loss in cost function:1322076561.2956884\n",
      "Training Data****************************************\n",
      "theta:[[168225.1633301   45962.91663363  31345.77221754]]\n",
      "loss in cost function:1135242565.8684835\n",
      "Validation Data****************************************\n",
      "theta:[[168225.1633301   45962.91663363  31345.77221754]]\n",
      "loss in cost function:1321144310.3221557\n",
      "Training Data****************************************\n",
      "theta:[[168284.04818126  45975.64959106  31350.91327138]]\n",
      "loss in cost function:1134513210.7679188\n",
      "Validation Data****************************************\n",
      "theta:[[168284.04818126  45975.64959106  31350.91327138]]\n",
      "loss in cost function:1320220566.800906\n",
      "Training Data****************************************\n",
      "theta:[[168342.63860816  45988.31347477  31356.01522354]]\n",
      "loss in cost function:1133791185.5861387\n",
      "Validation Data****************************************\n",
      "theta:[[168342.63860816  45988.31347477  31356.01522354]]\n",
      "loss in cost function:1319305249.449142\n",
      "Training Data****************************************\n",
      "theta:[[168400.93608293  46000.90867127  31361.0783422 ]]\n",
      "loss in cost function:1133076416.4978225\n",
      "Validation Data****************************************\n",
      "theta:[[168400.93608293  46000.90867127  31361.0783422 ]]\n",
      "loss in cost function:1318398277.7822974\n",
      "Training Data****************************************\n",
      "theta:[[168458.94207032  46013.43556485  31366.10289379]]\n",
      "loss in cost function:1132368830.423035\n",
      "Validation Data****************************************\n",
      "theta:[[168458.94207032  46013.43556485  31366.10289379]]\n",
      "loss in cost function:1317499572.1060622\n",
      "Training Data****************************************\n",
      "theta:[[168516.65802778  46025.8945376   31371.089143  ]]\n",
      "loss in cost function:1131668355.019655\n",
      "Validation Data****************************************\n",
      "theta:[[168516.65802778  46025.8945376   31371.089143  ]]\n",
      "loss in cost function:1316609053.508501\n",
      "Training Data****************************************\n",
      "theta:[[168574.08540545  46038.28596944  31376.03735282]]\n",
      "loss in cost function:1130974918.6759048\n",
      "Validation Data****************************************\n",
      "theta:[[168574.08540545  46038.28596944  31376.03735282]]\n",
      "loss in cost function:1315726643.852233\n",
      "Training Data****************************************\n",
      "theta:[[168631.22564623  46050.61023807  31380.94778449]]\n",
      "loss in cost function:1130288450.5029657\n",
      "Validation Data****************************************\n",
      "theta:[[168631.22564623  46050.61023807  31380.94778449]]\n",
      "loss in cost function:1314852265.7667065\n",
      "Training Data****************************************\n",
      "theta:[[168688.0801858   46062.86771907  31385.82069758]]\n",
      "loss in cost function:1129608880.3276613\n",
      "Validation Data****************************************\n",
      "theta:[[168688.0801858   46062.86771907  31385.82069758]]\n",
      "loss in cost function:1313985842.6405425\n",
      "Training Data****************************************\n",
      "theta:[[168744.65045268  46075.05878584  31390.65634993]]\n",
      "loss in cost function:1128936138.6852043\n",
      "Validation Data****************************************\n",
      "theta:[[168744.65045268  46075.05878584  31390.65634993]]\n",
      "loss in cost function:1313127298.6139603\n",
      "Training Data****************************************\n",
      "theta:[[168800.93786823  46087.18380967  31395.45499774]]\n",
      "loss in cost function:1128270156.8120258\n",
      "Validation Data****************************************\n",
      "theta:[[168800.93786823  46087.18380967  31395.45499774]]\n",
      "loss in cost function:1312276558.571285\n",
      "Training Data****************************************\n",
      "theta:[[168856.94384669  46099.24315969  31400.2168955 ]]\n",
      "loss in cost function:1127610866.638688\n",
      "Validation Data****************************************\n",
      "theta:[[168856.94384669  46099.24315969  31400.2168955 ]]\n",
      "loss in cost function:1311433548.1335127\n",
      "Training Data****************************************\n",
      "theta:[[168912.66979527  46111.23720294  31404.94229607]]\n",
      "loss in cost function:1126958200.7828429\n",
      "Validation Data****************************************\n",
      "theta:[[168912.66979527  46111.23720294  31404.94229607]]\n",
      "loss in cost function:1310598193.6509602\n",
      "Training Data****************************************\n",
      "theta:[[168968.1171141   46123.16630436  31409.63145063]]\n",
      "loss in cost function:1126312092.5422926\n",
      "Validation Data****************************************\n",
      "theta:[[168968.1171141   46123.16630436  31409.63145063]]\n",
      "loss in cost function:1309770422.196005\n",
      "Training Data****************************************\n",
      "theta:[[169023.28719634  46135.03082678  31414.28460875]]\n",
      "loss in cost function:1125672475.8881013\n",
      "Validation Data****************************************\n",
      "theta:[[169023.28719634  46135.03082678  31414.28460875]]\n",
      "loss in cost function:1308950161.5558686\n",
      "Training Data****************************************\n",
      "theta:[[169078.18142817  46146.83113098  31418.90201835]]\n",
      "loss in cost function:1125039285.4577672\n",
      "Validation Data****************************************\n",
      "theta:[[169078.18142817  46146.83113098  31418.90201835]]\n",
      "loss in cost function:1308137340.2254932\n",
      "Training Data****************************************\n",
      "theta:[[169132.80118883  46158.56757566  31423.48392573]]\n",
      "loss in cost function:1124412456.5485022\n",
      "Validation Data****************************************\n",
      "theta:[[169132.80118883  46158.56757566  31423.48392573]]\n",
      "loss in cost function:1307331887.400488\n",
      "Training Data****************************************\n",
      "theta:[[169187.1478507   46170.24051746  31428.03057561]]\n",
      "loss in cost function:1123791925.1105294\n",
      "Validation Data****************************************\n",
      "theta:[[169187.1478507   46170.24051746  31428.03057561]]\n",
      "loss in cost function:1306533732.9701347\n",
      "Training Data****************************************\n",
      "theta:[[169241.22277925  46181.850311    31432.54221106]]\n",
      "loss in cost function:1123177627.7405012\n",
      "Validation Data****************************************\n",
      "theta:[[169241.22277925  46181.850311    31432.54221106]]\n",
      "loss in cost function:1305742807.510485\n",
      "Training Data****************************************\n",
      "theta:[[169295.02733316  46193.39730885  31437.01907361]]\n",
      "loss in cost function:1122569501.6749344\n",
      "Validation Data****************************************\n",
      "theta:[[169295.02733316  46193.39730885  31437.01907361]]\n",
      "loss in cost function:1304959042.277498\n",
      "Training Data****************************************\n",
      "theta:[[169348.56286431  46204.88186159  31441.46140319]]\n",
      "loss in cost function:1121967484.783749\n",
      "Validation Data****************************************\n",
      "theta:[[169348.56286431  46204.88186159  31441.46140319]]\n",
      "loss in cost function:1304182369.2002783\n",
      "Training Data****************************************\n",
      "theta:[[169401.83071779  46216.30431776  31445.86943816]]\n",
      "loss in cost function:1121371515.563856\n",
      "Validation Data****************************************\n",
      "theta:[[169401.83071779  46216.30431776  31445.86943816]]\n",
      "loss in cost function:1303412720.8743615\n",
      "Training Data****************************************\n",
      "theta:[[169454.83223201  46227.66502393  31450.24341532]]\n",
      "loss in cost function:1120781533.1328044\n",
      "Validation Data****************************************\n",
      "theta:[[169454.83223201  46227.66502393  31450.24341532]]\n",
      "loss in cost function:1302650030.5550714\n",
      "Training Data****************************************\n",
      "theta:[[169507.56873866  46238.96432469  31454.58356994]]\n",
      "loss in cost function:1120197477.222515\n",
      "Validation Data****************************************\n",
      "theta:[[169507.56873866  46238.96432469  31454.58356994]]\n",
      "loss in cost function:1301894232.150946\n",
      "Training Data****************************************\n",
      "theta:[[169560.04156278  46250.20256264  31458.89013573]]\n",
      "loss in cost function:1119619288.1730602\n",
      "Validation Data****************************************\n",
      "theta:[[169560.04156278  46250.20256264  31458.89013573]]\n",
      "loss in cost function:1301145260.2172384\n",
      "Training Data****************************************\n",
      "theta:[[169612.25202277  46261.38007844  31463.16334488]]\n",
      "loss in cost function:1119046906.9264994\n",
      "Validation Data****************************************\n",
      "theta:[[169612.25202277  46261.38007844  31463.16334488]]\n",
      "loss in cost function:1300403049.9494655\n",
      "Training Data****************************************\n",
      "theta:[[169664.20143046  46272.49721079  31467.40342805]]\n",
      "loss in cost function:1118480275.0208004\n",
      "Validation Data****************************************\n",
      "theta:[[169664.20143046  46272.49721079  31467.40342805]]\n",
      "loss in cost function:1299667537.1770315\n",
      "Training Data****************************************\n",
      "theta:[[169715.89109112  46283.55429647  31471.61061442]]\n",
      "loss in cost function:1117919334.583814\n",
      "Validation Data****************************************\n",
      "theta:[[169715.89109112  46283.55429647  31471.61061442]]\n",
      "loss in cost function:1298938658.3569162\n",
      "Training Data****************************************\n",
      "theta:[[169767.32230347  46294.55167031  31475.78513163]]\n",
      "loss in cost function:1117364028.327294\n",
      "Validation Data****************************************\n",
      "theta:[[169767.32230347  46294.55167031  31475.78513163]]\n",
      "loss in cost function:1298216350.5674312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data****************************************\n",
      "theta:[[169818.49635976  46305.48966526  31479.92720586]]\n",
      "loss in cost function:1116814299.5409913\n",
      "Validation Data****************************************\n",
      "theta:[[169818.49635976  46305.48966526  31479.92720586]]\n",
      "loss in cost function:1297500551.5020247\n",
      "Training Data****************************************\n",
      "theta:[[169869.41454577  46316.36861233  31484.03706179]]\n",
      "loss in cost function:1116270092.0868177\n",
      "Validation Data****************************************\n",
      "theta:[[169869.41454577  46316.36861233  31484.03706179]]\n",
      "loss in cost function:1296791199.463161\n",
      "Training Data****************************************\n",
      "theta:[[169920.07814085  46327.18884066  31488.11492265]]\n",
      "loss in cost function:1115731350.3930562\n",
      "Validation Data****************************************\n",
      "theta:[[169920.07814085  46327.18884066  31488.11492265]]\n",
      "loss in cost function:1296088233.3562596\n",
      "Training Data****************************************\n",
      "theta:[[169970.48841796  46337.95067751  31492.16101017]]\n",
      "loss in cost function:1115198019.4486098\n",
      "Validation Data****************************************\n",
      "theta:[[169970.48841796  46337.95067751  31492.16101017]]\n",
      "loss in cost function:1295391592.6836932\n",
      "Training Data****************************************\n",
      "theta:[[170020.64664367  46348.65444827  31496.17554468]]\n",
      "loss in cost function:1114670044.7973676\n",
      "Validation Data****************************************\n",
      "theta:[[170020.64664367  46348.65444827  31496.17554468]]\n",
      "loss in cost function:1294701217.538841\n",
      "Training Data****************************************\n",
      "theta:[[170070.55407826  46359.30047646  31500.15874501]]\n",
      "loss in cost function:1114147372.5325577\n",
      "Validation Data****************************************\n",
      "theta:[[170070.55407826  46359.30047646  31500.15874501]]\n",
      "loss in cost function:1294017048.6002223\n",
      "Training Data****************************************\n",
      "theta:[[170120.21197568  46369.88908376  31504.1108286 ]]\n",
      "loss in cost function:1113629949.291213\n",
      "Validation Data****************************************\n",
      "theta:[[170120.21197568  46369.88908376  31504.1108286 ]]\n",
      "loss in cost function:1293339027.1256545\n",
      "Training Data****************************************\n",
      "theta:[[170169.62158361  46380.42059001  31508.03201143]]\n",
      "loss in cost function:1113117722.2486558\n",
      "Validation Data****************************************\n",
      "theta:[[170169.62158361  46380.42059001  31508.03201143]]\n",
      "loss in cost function:1292667094.946506\n",
      "Training Data****************************************\n",
      "theta:[[170218.7841435   46390.89531324  31511.9225081 ]]\n",
      "loss in cost function:1112610639.113068\n",
      "Validation Data****************************************\n",
      "theta:[[170218.7841435   46390.89531324  31511.9225081 ]]\n",
      "loss in cost function:1292001194.461979\n",
      "Training Data****************************************\n",
      "theta:[[170267.70089059  46401.31356963  31515.78253177]]\n",
      "loss in cost function:1112108648.1200838\n",
      "Validation Data****************************************\n",
      "theta:[[170267.70089059  46401.31356963  31515.78253177]]\n",
      "loss in cost function:1291341268.6334746\n",
      "Training Data****************************************\n",
      "theta:[[170316.37305395  46411.6756736   31519.61229422]]\n",
      "loss in cost function:1111611698.0274801\n",
      "Validation Data****************************************\n",
      "theta:[[170316.37305395  46411.6756736   31519.61229422]]\n",
      "loss in cost function:1290687260.9789863\n",
      "Training Data****************************************\n",
      "theta:[[170364.80185649  46421.98193772  31523.41200583]]\n",
      "loss in cost function:1111119738.1098614\n",
      "Validation Data****************************************\n",
      "theta:[[170364.80185649  46421.98193772  31523.41200583]]\n",
      "loss in cost function:1290039115.5675814\n",
      "Training Data****************************************\n",
      "theta:[[170412.98851501  46432.23267284  31527.1818756 ]]\n",
      "loss in cost function:1110632718.153477\n",
      "Validation Data****************************************\n",
      "theta:[[170412.98851501  46432.23267284  31527.1818756 ]]\n",
      "loss in cost function:1289396777.0139112\n",
      "Training Data****************************************\n",
      "theta:[[170460.93424024  46442.42818797  31530.92211117]]\n",
      "loss in cost function:1110150588.4510052\n",
      "Validation Data****************************************\n",
      "theta:[[170460.93424024  46442.42818797  31530.92211117]]\n",
      "loss in cost function:1288760190.4727838\n",
      "Training Data****************************************\n",
      "theta:[[170508.64023685  46452.56879041  31534.63291881]]\n",
      "loss in cost function:1109673299.7964551\n",
      "Validation Data****************************************\n",
      "theta:[[170508.64023685  46452.56879041  31534.63291881]]\n",
      "loss in cost function:1288129301.6338072\n",
      "Training Data****************************************\n",
      "theta:[[170556.10770348  46462.65478567  31538.31450342]]\n",
      "loss in cost function:1109200803.4800994\n",
      "Validation Data****************************************\n",
      "theta:[[170556.10770348  46462.65478567  31538.31450342]]\n",
      "loss in cost function:1287504056.7160637\n",
      "Training Data****************************************\n",
      "theta:[[170603.33783277  46472.68647754  31541.96706858]]\n",
      "loss in cost function:1108733051.283443\n",
      "Validation Data****************************************\n",
      "theta:[[170603.33783277  46472.68647754  31541.96706858]]\n",
      "loss in cost function:1286884402.4628532\n",
      "Training Data****************************************\n",
      "theta:[[170650.33181141  46482.66416807  31545.59081651]]\n",
      "loss in cost function:1108269995.4742694\n",
      "Validation Data****************************************\n",
      "theta:[[170650.33181141  46482.66416807  31545.59081651]]\n",
      "loss in cost function:1286270286.1364806\n",
      "Training Data****************************************\n",
      "theta:[[170697.09082016  46492.58815758  31549.18594813]]\n",
      "loss in cost function:1107811588.8017006\n",
      "Validation Data****************************************\n",
      "theta:[[170697.09082016  46492.58815758  31549.18594813]]\n",
      "loss in cost function:1285661655.5131037\n",
      "Training Data****************************************\n",
      "theta:[[170743.61603387  46502.45874468  31552.752663  ]]\n",
      "loss in cost function:1107357784.4913619\n",
      "Validation Data****************************************\n",
      "theta:[[170743.61603387  46502.45874468  31552.752663  ]]\n",
      "loss in cost function:1285058458.8776188\n",
      "Training Data****************************************\n",
      "theta:[[170789.90862151  46512.27622629  31556.2911594 ]]\n",
      "loss in cost function:1106908536.2405515\n",
      "Validation Data****************************************\n",
      "theta:[[170789.90862151  46512.27622629  31556.2911594 ]]\n",
      "loss in cost function:1284460645.0186255\n",
      "Training Data****************************************\n",
      "theta:[[170835.96974621  46522.04089762  31559.80163428]]\n",
      "loss in cost function:1106463798.2134595\n",
      "Validation Data****************************************\n",
      "theta:[[170835.96974621  46522.04089762  31559.80163428]]\n",
      "loss in cost function:1283868163.2233992\n",
      "Training Data****************************************\n",
      "theta:[[170881.80056529  46531.75305221  31563.28428333]]\n",
      "loss in cost function:1106023525.036463\n",
      "Validation Data****************************************\n",
      "theta:[[170881.80056529  46531.75305221  31563.28428333]]\n",
      "loss in cost function:1283280963.2729619\n",
      "Training Data****************************************\n",
      "theta:[[170927.40223027  46541.4129819   31566.73930093]]\n",
      "loss in cost function:1105587671.7934504\n",
      "Validation Data****************************************\n",
      "theta:[[170927.40223027  46541.4129819   31566.73930093]]\n",
      "loss in cost function:1282698995.4371696\n",
      "Training Data****************************************\n",
      "theta:[[170972.77588692  46551.02097691  31570.16688017]]\n",
      "loss in cost function:1105156194.0211918\n",
      "Validation Data****************************************\n",
      "theta:[[170972.77588692  46551.02097691  31570.16688017]]\n",
      "loss in cost function:1282122210.4698508\n",
      "Training Data****************************************\n",
      "theta:[[171017.9226753   46560.57732576  31573.56721289]]\n",
      "loss in cost function:1104729047.7047603\n",
      "Validation Data****************************************\n",
      "theta:[[171017.9226753   46560.57732576  31573.56721289]]\n",
      "loss in cost function:1281550559.6040192\n",
      "Training Data****************************************\n",
      "theta:[[171062.84372973  46570.08231537  31576.94048966]]\n",
      "loss in cost function:1104306189.2730095\n",
      "Validation Data****************************************\n",
      "theta:[[171062.84372973  46570.08231537  31576.94048966]]\n",
      "loss in cost function:1280983994.5471056\n",
      "Training Data****************************************\n",
      "theta:[[171107.54017889  46579.53623098  31580.28689979]]\n",
      "loss in cost function:1103887575.5940819\n",
      "Validation Data****************************************\n",
      "theta:[[171107.54017889  46579.53623098  31580.28689979]]\n",
      "loss in cost function:1280422467.476251\n",
      "Training Data****************************************\n",
      "theta:[[171152.0131458   46588.93935623  31583.60663136]]\n",
      "loss in cost function:1103473163.9709668\n",
      "Validation Data****************************************\n",
      "theta:[[171152.0131458   46588.93935623  31583.60663136]]\n",
      "loss in cost function:1279865931.0336535\n",
      "Training Data****************************************\n",
      "theta:[[171196.26374788  46598.29197315  31586.89987119]]\n",
      "loss in cost function:1103062912.1371028\n",
      "Validation Data****************************************\n",
      "theta:[[171196.26374788  46598.29197315  31586.89987119]]\n",
      "loss in cost function:1279314338.3219404\n",
      "Training Data****************************************\n",
      "theta:[[171240.29309695  46607.59436216  31590.16680488]]\n",
      "loss in cost function:1102656778.2520394\n",
      "Validation Data****************************************\n",
      "theta:[[171240.29309695  46607.59436216  31590.16680488]]\n",
      "loss in cost function:1278767642.8996115\n",
      "Training Data****************************************\n",
      "theta:[[171284.10229927  46616.84680205  31593.40761681]]\n",
      "loss in cost function:1102254720.897111\n",
      "Validation Data****************************************\n",
      "theta:[[171284.10229927  46616.84680205  31593.40761681]]\n",
      "loss in cost function:1278225798.7765117\n",
      "Training Data****************************************\n",
      "theta:[[171327.69245559  46626.04957008  31596.62249013]]\n",
      "loss in cost function:1101856699.0712001\n",
      "Validation Data****************************************\n",
      "theta:[[171327.69245559  46626.04957008  31596.62249013]]\n",
      "loss in cost function:1277688760.4093568\n",
      "Training Data****************************************\n",
      "theta:[[171371.06466112  46635.20294187  31599.81160681]]\n",
      "loss in cost function:1101462672.1864889\n",
      "Validation Data****************************************\n",
      "theta:[[171371.06466112  46635.20294187  31599.81160681]]\n",
      "loss in cost function:1277156482.6973023\n",
      "Training Data****************************************\n",
      "theta:[[171414.22000562  46644.30719152  31602.97514759]]\n",
      "loss in cost function:1101072600.0643032\n",
      "Validation Data****************************************\n",
      "theta:[[171414.22000562  46644.30719152  31602.97514759]]\n",
      "loss in cost function:1276628920.977554\n",
      "Training Data****************************************\n",
      "theta:[[171457.1595734   46653.36259155  31606.11329203]]\n",
      "loss in cost function:1100686442.9309683\n",
      "Validation Data****************************************\n",
      "theta:[[171457.1595734   46653.36259155  31606.11329203]]\n",
      "loss in cost function:1276106031.0210216\n",
      "Training Data****************************************\n",
      "theta:[[171499.88444334  46662.36941291  31609.22621852]]\n",
      "loss in cost function:1100304161.413714\n",
      "Validation Data****************************************\n",
      "theta:[[171499.88444334  46662.36941291  31609.22621852]]\n",
      "loss in cost function:1275587769.028035\n",
      "Training Data****************************************\n",
      "theta:[[171542.39568893  46671.32792504  31612.31410425]]\n",
      "loss in cost function:1099925716.5366323\n",
      "Validation Data****************************************\n",
      "theta:[[171542.39568893  46671.32792504  31612.31410425]]\n",
      "loss in cost function:1275074091.6240633\n",
      "Training Data****************************************\n",
      "theta:[[171584.6943783   46680.23839583  31615.37712524]]\n",
      "loss in cost function:1099551069.7166653\n",
      "Validation Data****************************************\n",
      "theta:[[171584.6943783   46680.23839583  31615.37712524]]\n",
      "loss in cost function:1274564955.8555272\n",
      "Training Data****************************************\n",
      "theta:[[171626.78157421  46689.10109164  31618.41545638]]\n",
      "loss in cost function:1099180182.7596247\n",
      "Validation Data****************************************\n",
      "theta:[[171626.78157421  46689.10109164  31618.41545638]]\n",
      "loss in cost function:1274060319.1856\n",
      "Training Data****************************************\n",
      "theta:[[171668.65833415  46697.91627733  31621.42927138]]\n",
      "loss in cost function:1098813017.8562794\n",
      "Validation Data****************************************\n",
      "theta:[[171668.65833415  46697.91627733  31621.42927138]]\n",
      "loss in cost function:1273560139.4901097\n",
      "Training Data****************************************\n",
      "theta:[[171710.32571029  46706.68421624  31624.41874279]]\n",
      "loss in cost function:1098449537.5784478\n",
      "Validation Data****************************************\n",
      "theta:[[171710.32571029  46706.68421624  31624.41874279]]\n",
      "loss in cost function:1273064375.053424\n",
      "Training Data****************************************\n",
      "theta:[[171751.78474954  46715.40517022  31627.38404205]]\n",
      "loss in cost function:1098089704.8751721\n",
      "Validation Data****************************************\n",
      "theta:[[171751.78474954  46715.40517022  31627.38404205]]\n",
      "loss in cost function:1272572984.5644195\n",
      "Training Data****************************************\n",
      "theta:[[171793.0364936   46724.07939961  31630.32533945]]\n",
      "loss in cost function:1097733483.0688717\n",
      "Validation Data****************************************\n",
      "theta:[[171793.0364936   46724.07939961  31630.32533945]]\n",
      "loss in cost function:1272085927.1124685\n",
      "Training Data****************************************\n",
      "theta:[[171834.08197894  46732.70716331  31633.24280415]]\n",
      "loss in cost function:1097380835.8516011\n",
      "Validation Data****************************************\n",
      "theta:[[171834.08197894  46732.70716331  31633.24280415]]\n",
      "loss in cost function:1271603162.1834738\n",
      "Training Data****************************************\n",
      "theta:[[171874.92223686  46741.28871871  31636.13660422]]\n",
      "loss in cost function:1097031727.281319\n",
      "Validation Data****************************************\n",
      "theta:[[171874.92223686  46741.28871871  31636.13660422]]\n",
      "loss in cost function:1271124649.6559422\n",
      "Training Data****************************************\n",
      "theta:[[171915.55829348  46749.82432174  31639.00690658]]\n",
      "loss in cost function:1096686121.7781482\n",
      "Validation Data****************************************\n",
      "theta:[[171915.55829348  46749.82432174  31639.00690658]]\n",
      "loss in cost function:1270650349.7971046\n",
      "Training Data****************************************\n",
      "theta:[[171955.99116982  46758.31422691  31641.85387708]]\n",
      "loss in cost function:1096343984.1207745\n",
      "Validation Data****************************************\n",
      "theta:[[171955.99116982  46758.31422691  31641.85387708]]\n",
      "loss in cost function:1270180223.2590554\n",
      "Training Data****************************************\n",
      "theta:[[171996.22188178  46766.75868723  31644.67768045]]\n",
      "loss in cost function:1096005279.4427834\n",
      "Validation Data****************************************\n",
      "theta:[[171996.22188178  46766.75868723  31644.67768045]]\n",
      "loss in cost function:1269714231.0749521\n",
      "Training Data****************************************\n",
      "theta:[[172036.25144018  46775.15795431  31647.47848034]]\n",
      "loss in cost function:1095669973.229094\n",
      "Validation Data****************************************\n",
      "theta:[[172036.25144018  46775.15795431  31647.47848034]]\n",
      "loss in cost function:1269252334.6552413\n",
      "Training Data****************************************\n",
      "theta:[[172076.08085079  46783.51227832  31650.25643931]]\n",
      "loss in cost function:1095338031.3123999\n",
      "Validation Data****************************************\n",
      "theta:[[172076.08085079  46783.51227832  31650.25643931]]\n",
      "loss in cost function:1268794495.7839315\n",
      "Training Data****************************************\n",
      "theta:[[172115.71111434  46791.821908    31653.01171886]]\n",
      "loss in cost function:1095009419.869668\n",
      "Validation Data****************************************\n",
      "theta:[[172115.71111434  46791.821908    31653.01171886]]\n",
      "loss in cost function:1268340676.614886\n",
      "Training Data****************************************\n",
      "theta:[[172155.14322658  46800.08709069  31655.74447941]]\n",
      "loss in cost function:1094684105.4186463\n",
      "Validation Data****************************************\n",
      "theta:[[172155.14322658  46800.08709069  31655.74447941]]\n",
      "loss in cost function:1267890839.6681685\n",
      "Training Data****************************************\n",
      "theta:[[172194.37817825  46808.30807231  31658.45488031]]\n",
      "loss in cost function:1094362054.8144352\n",
      "Validation Data****************************************\n",
      "theta:[[172194.37817825  46808.30807231  31658.45488031]]\n",
      "loss in cost function:1267444947.8264313\n",
      "Training Data****************************************\n",
      "theta:[[172233.41695517  46816.4850974   31661.14307986]]\n",
      "loss in cost function:1094043235.2460675\n",
      "Validation Data****************************************\n",
      "theta:[[172233.41695517  46816.4850974   31661.14307986]]\n",
      "loss in cost function:1267002964.3313134\n",
      "Training Data****************************************\n",
      "theta:[[172272.2605382   46824.61840909  31663.80923532]]\n",
      "loss in cost function:1093727614.2331543\n",
      "Validation Data****************************************\n",
      "theta:[[172272.2605382   46824.61840909  31663.80923532]]\n",
      "loss in cost function:1266564852.7798996\n",
      "Training Data****************************************\n",
      "theta:[[172310.90990332  46832.70824915  31666.45350291]]\n",
      "loss in cost function:1093415159.6225295\n",
      "Validation Data****************************************\n",
      "theta:[[172310.90990332  46832.70824915  31666.45350291]]\n",
      "loss in cost function:1266130577.1212053\n",
      "Training Data****************************************\n",
      "theta:[[172349.36602161  46840.75485796  31669.07603779]]\n",
      "loss in cost function:1093105839.584943\n",
      "Validation Data****************************************\n",
      "theta:[[172349.36602161  46840.75485796  31669.07603779]]\n",
      "loss in cost function:1265700101.652699\n",
      "Training Data****************************************\n",
      "theta:[[172387.62985931  46848.75847454  31671.67699411]]\n",
      "loss in cost function:1092799622.6118064\n",
      "Validation Data****************************************\n",
      "theta:[[172387.62985931  46848.75847454  31671.67699411]]\n",
      "loss in cost function:1265273391.016853\n",
      "Training Data****************************************\n",
      "theta:[[172425.70237782  46856.71933656  31674.25652501]]\n",
      "loss in cost function:1092496477.5119448\n",
      "Validation Data****************************************\n",
      "theta:[[172425.70237782  46856.71933656  31674.25652501]]\n",
      "loss in cost function:1264850410.197739\n",
      "Training Data****************************************\n",
      "theta:[[172463.58453374  46864.63768033  31676.81478258]]\n",
      "loss in cost function:1092196373.408385\n",
      "Validation Data****************************************\n",
      "theta:[[172463.58453374  46864.63768033  31676.81478258]]\n",
      "loss in cost function:1264431124.517646\n",
      "Training Data****************************************\n",
      "theta:[[172501.27727888  46872.51374082  31679.35191793]]\n",
      "loss in cost function:1091899279.7351904\n",
      "Validation Data****************************************\n",
      "theta:[[172501.27727888  46872.51374082  31679.35191793]]\n",
      "loss in cost function:1264015499.6337519\n",
      "Training Data****************************************\n",
      "theta:[[172538.7815603   46880.34775168  31681.86808116]]\n",
      "loss in cost function:1091605166.2343285\n",
      "Validation Data****************************************\n",
      "theta:[[172538.7815603   46880.34775168  31681.86808116]]\n",
      "loss in cost function:1263603501.5347993\n",
      "Training Data****************************************\n",
      "theta:[[172576.0983203   46888.13994522  31684.36342137]]\n",
      "loss in cost function:1091314002.9525378\n",
      "Validation Data****************************************\n",
      "theta:[[172576.0983203   46888.13994522  31684.36342137]]\n",
      "loss in cost function:1263195096.53783\n",
      "Training Data****************************************\n",
      "theta:[[172613.22849651  46895.89055242  31686.83808667]]\n",
      "loss in cost function:1091025760.238277\n",
      "Validation Data****************************************\n",
      "theta:[[172613.22849651  46895.89055242  31686.83808667]]\n",
      "loss in cost function:1262790251.284941\n",
      "Training Data****************************************\n",
      "theta:[[172650.17302184  46903.59980298  31689.29222418]]\n",
      "loss in cost function:1090740408.7386584\n",
      "Validation Data****************************************\n",
      "theta:[[172650.17302184  46903.59980298  31689.29222418]]\n",
      "loss in cost function:1262388932.7400718\n",
      "Training Data****************************************\n",
      "theta:[[172686.93282454  46911.26792528  31691.72598006]]\n",
      "loss in cost function:1090457919.3964472\n",
      "Validation Data****************************************\n",
      "theta:[[172686.93282454  46911.26792528  31691.72598006]]\n",
      "loss in cost function:1261991108.1858342\n",
      "Training Data****************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta:[[172723.50882822  46918.89514639  31694.13949949]]\n",
      "loss in cost function:1090178263.447068\n",
      "Validation Data****************************************\n",
      "theta:[[172723.50882822  46918.89514639  31694.13949949]]\n",
      "loss in cost function:1261596745.2203534\n",
      "Training Data****************************************\n",
      "theta:[[172759.90195189  46926.48169213  31696.53292668]]\n",
      "loss in cost function:1089901412.4156435\n",
      "Validation Data****************************************\n",
      "theta:[[172759.90195189  46926.48169213  31696.53292668]]\n",
      "loss in cost function:1261205811.754173\n",
      "Training Data****************************************\n",
      "theta:[[172796.11310994  46934.027787    31698.90640489]]\n",
      "loss in cost function:1089627338.1140885\n",
      "Validation Data****************************************\n",
      "theta:[[172796.11310994  46934.027787    31698.90640489]]\n",
      "loss in cost function:1260818276.0071511\n",
      "Training Data****************************************\n",
      "theta:[[172832.1432122   46941.53365424  31701.26007642]]\n",
      "loss in cost function:1089356012.638199\n",
      "Validation Data****************************************\n",
      "theta:[[172832.1432122   46941.53365424  31701.26007642]]\n",
      "loss in cost function:1260434106.505421\n",
      "Training Data****************************************\n",
      "theta:[[172867.99316394  46948.99951584  31703.59408262]]\n",
      "loss in cost function:1089087408.3647962\n",
      "Validation Data****************************************\n",
      "theta:[[172867.99316394  46948.99951584  31703.59408262]]\n",
      "loss in cost function:1260053272.0783668\n",
      "Training Data****************************************\n",
      "theta:[[172903.66386593  46956.4255925   31705.90856393]]\n",
      "loss in cost function:1088821497.948877\n",
      "Validation Data****************************************\n",
      "theta:[[172903.66386593  46956.4255925   31705.90856393]]\n",
      "loss in cost function:1259675741.8556306\n",
      "Training Data****************************************\n",
      "theta:[[172939.15621441  46963.81210371  31708.20365981]]\n",
      "loss in cost function:1088558254.3208184\n",
      "Validation Data****************************************\n",
      "theta:[[172939.15621441  46963.81210371  31708.20365981]]\n",
      "loss in cost function:1259301485.264153\n",
      "Training Data****************************************\n",
      "theta:[[172974.47110115  46971.15926768  31710.47950882]]\n",
      "loss in cost function:1088297650.6835873\n",
      "Validation Data****************************************\n",
      "theta:[[172974.47110115  46971.15926768  31710.47950882]]\n",
      "loss in cost function:1258930472.025239\n",
      "Training Data****************************************\n",
      "theta:[[173009.60941345  46978.46730139  31712.73624859]]\n",
      "loss in cost function:1088039660.5100036\n",
      "Validation Data****************************************\n",
      "theta:[[173009.60941345  46978.46730139  31712.73624859]]\n",
      "loss in cost function:1258562672.1516511\n",
      "Training Data****************************************\n",
      "theta:[[173044.57203419  46985.73642061  31714.97401584]]\n",
      "loss in cost function:1087784257.5400038\n",
      "Validation Data****************************************\n",
      "theta:[[173044.57203419  46985.73642061  31714.97401584]]\n",
      "loss in cost function:1258198055.944751\n",
      "Training Data****************************************\n",
      "theta:[[173079.35984183  46992.96683986  31717.19294638]]\n",
      "loss in cost function:1087531415.7779417\n",
      "Validation Data****************************************\n",
      "theta:[[173079.35984183  46992.96683986  31717.19294638]]\n",
      "loss in cost function:1257836593.991639\n",
      "Training Data****************************************\n",
      "theta:[[173113.97371043  47000.15877247  31719.3931751 ]]\n",
      "loss in cost function:1087281109.4899313\n",
      "Validation Data****************************************\n",
      "theta:[[173113.97371043  47000.15877247  31719.3931751 ]]\n",
      "loss in cost function:1257478257.1623437\n",
      "Training Data****************************************\n",
      "theta:[[173148.41450968  47007.31243054  31721.57483601]]\n",
      "loss in cost function:1087033313.201195\n",
      "Validation Data****************************************\n",
      "theta:[[173148.41450968  47007.31243054  31721.57483601]]\n",
      "loss in cost function:1257123016.607033\n",
      "Training Data****************************************\n",
      "theta:[[173182.68310494  47014.42802499  31723.7380622 ]]\n",
      "loss in cost function:1086788001.6934688\n",
      "Validation Data****************************************\n",
      "theta:[[173182.68310494  47014.42802499  31723.7380622 ]]\n",
      "loss in cost function:1256770843.7532578\n",
      "Training Data****************************************\n",
      "theta:[[173216.78035723  47021.50576552  31725.88298591]]\n",
      "loss in cost function:1086545150.0023708\n",
      "Validation Data****************************************\n",
      "theta:[[173216.78035723  47021.50576552  31725.88298591]]\n",
      "loss in cost function:1256421710.3032148\n",
      "Training Data****************************************\n",
      "theta:[[173250.70712325  47028.54586066  31728.00973848]]\n",
      "loss in cost function:1086304733.4148862\n",
      "Validation Data****************************************\n",
      "theta:[[173250.70712325  47028.54586066  31728.00973848]]\n",
      "loss in cost function:1256075588.2310388\n",
      "Training Data****************************************\n",
      "theta:[[173284.46425544  47035.54851776  31730.11845035]]\n",
      "loss in cost function:1086066727.4668033\n",
      "Validation Data****************************************\n",
      "theta:[[173284.46425544  47035.54851776  31730.11845035]]\n",
      "loss in cost function:1255732449.7801337\n",
      "Training Data****************************************\n",
      "theta:[[173318.05260197  47042.51394298  31732.20925114]]\n",
      "loss in cost function:1085831107.9402072\n",
      "Validation Data****************************************\n",
      "theta:[[173318.05260197  47042.51394298  31732.20925114]]\n",
      "loss in cost function:1255392267.4605098\n",
      "Training Data****************************************\n",
      "theta:[[173351.47300677  47049.44234134  31734.28226955]]\n",
      "loss in cost function:1085597850.8609986\n",
      "Validation Data****************************************\n",
      "theta:[[173351.47300677  47049.44234134  31734.28226955]]\n",
      "loss in cost function:1255055014.0461688\n",
      "Training Data****************************************\n",
      "theta:[[173384.72630954  47056.33391667  31736.33763347]]\n",
      "loss in cost function:1085366932.4964302\n",
      "Validation Data****************************************\n",
      "theta:[[173384.72630954  47056.33391667  31736.33763347]]\n",
      "loss in cost function:1254720662.5725026\n",
      "Training Data****************************************\n",
      "theta:[[173417.8133458   47063.18887166  31738.37546991]]\n",
      "loss in cost function:1085138329.352678\n",
      "Validation Data****************************************\n",
      "theta:[[173417.8133458   47063.18887166  31738.37546991]]\n",
      "loss in cost function:1254389186.33371\n",
      "Training Data****************************************\n",
      "theta:[[173450.73494688  47070.00740786  31740.39590502]]\n",
      "loss in cost function:1084912018.172412\n",
      "Validation Data****************************************\n",
      "theta:[[173450.73494688  47070.00740786  31740.39590502]]\n",
      "loss in cost function:1254060558.880265\n",
      "Training Data****************************************\n",
      "theta:[[173483.49193996  47076.78972567  31742.39906415]]\n",
      "loss in cost function:1084687975.9324334\n",
      "Validation Data****************************************\n",
      "theta:[[173483.49193996  47076.78972567  31742.39906415]]\n",
      "loss in cost function:1253734754.016387\n",
      "Training Data****************************************\n",
      "theta:[[173516.08514806  47083.53602436  31744.38507176]]\n",
      "loss in cost function:1084466179.8412957\n",
      "Validation Data****************************************\n",
      "theta:[[173516.08514806  47083.53602436  31744.38507176]]\n",
      "loss in cost function:1253411745.7975433\n",
      "Training Data****************************************\n",
      "theta:[[173548.51539013  47090.24650206  31746.35405152]]\n",
      "loss in cost function:1084246607.3369718\n",
      "Validation Data****************************************\n",
      "theta:[[173548.51539013  47090.24650206  31746.35405152]]\n",
      "loss in cost function:1253091508.5279837\n",
      "Training Data****************************************\n",
      "theta:[[173580.78348099  47096.9213558   31748.30612624]]\n",
      "loss in cost function:1084029236.0845456\n",
      "Validation Data****************************************\n",
      "theta:[[173580.78348099  47096.9213558   31748.30612624]]\n",
      "loss in cost function:1252774016.758286\n",
      "Training Data****************************************\n",
      "theta:[[173612.89023139  47103.56078148  31750.24141795]]\n",
      "loss in cost function:1083814043.973903\n",
      "Validation Data****************************************\n",
      "theta:[[173612.89023139  47103.56078148  31750.24141795]]\n",
      "loss in cost function:1252459245.2829416\n",
      "Training Data****************************************\n",
      "theta:[[173644.83644804  47110.1649739   31752.16004782]]\n",
      "loss in cost function:1083601009.1174867\n",
      "Validation Data****************************************\n",
      "theta:[[173644.83644804  47110.1649739   31752.16004782]]\n",
      "loss in cost function:1252147169.137951\n",
      "Training Data****************************************\n",
      "theta:[[173676.62293361  47116.73412676  31754.06213623]]\n",
      "loss in cost function:1083390109.8480334\n",
      "Validation Data****************************************\n",
      "theta:[[173676.62293361  47116.73412676  31754.06213623]]\n",
      "loss in cost function:1251837763.5984523\n",
      "Training Data****************************************\n",
      "theta:[[173708.25048675  47123.26843266  31755.94780276]]\n",
      "loss in cost function:1083181324.7163587\n",
      "Validation Data****************************************\n",
      "theta:[[173708.25048675  47123.26843266  31755.94780276]]\n",
      "loss in cost function:1251531004.1763678\n",
      "Training Data****************************************\n",
      "theta:[[173739.71990213  47129.7680831   31757.81716618]]\n",
      "loss in cost function:1082974632.4891515\n",
      "Validation Data****************************************\n",
      "theta:[[173739.71990213  47129.7680831   31757.81716618]]\n",
      "loss in cost function:1251226866.6180851\n",
      "Training Data****************************************\n",
      "theta:[[173771.03197043  47136.23326853  31759.67034446]]\n",
      "loss in cost function:1082770012.146809\n",
      "Validation Data****************************************\n",
      "theta:[[173771.03197043  47136.23326853  31759.67034446]]\n",
      "loss in cost function:1250925326.9021432\n",
      "Training Data****************************************\n",
      "theta:[[173802.18747838  47142.66417829  31761.50745479]]\n",
      "loss in cost function:1082567442.88126\n",
      "Validation Data****************************************\n",
      "theta:[[173802.18747838  47142.66417829  31761.50745479]]\n",
      "loss in cost function:1250626361.2369547\n",
      "Training Data****************************************\n",
      "theta:[[173833.1872088   47149.06100067  31763.32861358]]\n",
      "loss in cost function:1082366904.093857\n",
      "Validation Data****************************************\n",
      "theta:[[173833.1872088   47149.06100067  31763.32861358]]\n",
      "loss in cost function:1250329946.0585542\n",
      "Training Data****************************************\n",
      "theta:[[173864.03194056  47155.42392289  31765.13393644]]\n",
      "loss in cost function:1082168375.3932478\n",
      "Validation Data****************************************\n",
      "theta:[[173864.03194056  47155.42392289  31765.13393644]]\n",
      "loss in cost function:1250036058.0283546\n",
      "Training Data****************************************\n",
      "theta:[[173894.72244867  47161.7531311   31766.92353822]]\n",
      "loss in cost function:1081971836.5932693\n",
      "Validation Data****************************************\n",
      "theta:[[173894.72244867  47161.7531311   31766.92353822]]\n",
      "loss in cost function:1249744674.0309358\n",
      "Training Data****************************************\n",
      "theta:[[173925.25950423  47168.04881042  31768.697533  ]]\n",
      "loss in cost function:1081777267.7109208\n",
      "Validation Data****************************************\n",
      "theta:[[173925.25950423  47168.04881042  31768.697533  ]]\n",
      "loss in cost function:1249455771.1718585\n",
      "Training Data****************************************\n",
      "theta:[[173955.64387452  47174.31114492  31770.45603408]]\n",
      "loss in cost function:1081584648.9642692\n",
      "Validation Data****************************************\n",
      "theta:[[173955.64387452  47174.31114492  31770.45603408]]\n",
      "loss in cost function:1249169326.7754855\n",
      "Training Data****************************************\n",
      "theta:[[173985.87632295  47180.54031762  31772.19915402]]\n",
      "loss in cost function:1081393960.7704577\n",
      "Validation Data****************************************\n",
      "theta:[[173985.87632295  47180.54031762  31772.19915402]]\n",
      "loss in cost function:1248885318.3828483\n",
      "Training Data****************************************\n",
      "theta:[[174015.95760915  47186.7365105   31773.92700462]]\n",
      "loss in cost function:1081205183.7436678\n",
      "Validation Data****************************************\n",
      "theta:[[174015.95760915  47186.7365105   31773.92700462]]\n",
      "loss in cost function:1248603723.749502\n",
      "Training Data****************************************\n",
      "theta:[[174045.88848891  47192.89990455  31775.63969691]]\n",
      "loss in cost function:1081018298.6931536\n",
      "Validation Data****************************************\n",
      "theta:[[174045.88848891  47192.89990455  31775.63969691]]\n",
      "loss in cost function:1248324520.8434374\n",
      "Training Data****************************************\n",
      "theta:[[174075.66971427  47199.03067968  31777.33734121]]\n",
      "loss in cost function:1080833286.6212635\n",
      "Validation Data****************************************\n",
      "theta:[[174075.66971427  47199.03067968  31777.33734121]]\n",
      "loss in cost function:1248047687.8429875\n",
      "Training Data****************************************\n",
      "theta:[[174105.30203351  47205.12901484  31779.02004706]]\n",
      "loss in cost function:1080650128.7214956\n",
      "Validation Data****************************************\n",
      "theta:[[174105.30203351  47205.12901484  31779.02004706]]\n",
      "loss in cost function:1247773203.13477\n",
      "Training Data****************************************\n",
      "theta:[[174134.78619115  47211.19508795  31780.68792328]]\n",
      "loss in cost function:1080468806.376572\n",
      "Validation Data****************************************\n",
      "theta:[[174134.78619115  47211.19508795  31780.68792328]]\n",
      "loss in cost function:1247501045.3116374\n",
      "Training Data****************************************\n",
      "theta:[[174164.122928    47217.2290759   31782.34107798]]\n",
      "loss in cost function:1080289301.156531\n",
      "Validation Data****************************************\n",
      "theta:[[174164.122928    47217.2290759   31782.34107798]]\n",
      "loss in cost function:1247231193.170664\n",
      "Training Data****************************************\n",
      "theta:[[174193.31298117  47223.23115462  31783.97961851]]\n",
      "loss in cost function:1080111594.8168278\n",
      "Validation Data****************************************\n",
      "theta:[[174193.31298117  47223.23115462  31783.97961851]]\n",
      "loss in cost function:1246963625.7111316\n",
      "Training Data****************************************\n",
      "theta:[[174222.35708407  47229.20149903  31785.60365152]]\n",
      "loss in cost function:1079935669.2964864\n",
      "Validation Data****************************************\n",
      "theta:[[174222.35708407  47229.20149903  31785.60365152]]\n",
      "loss in cost function:1246698322.13256\n",
      "Training Data****************************************\n",
      "theta:[[174251.25596646  47235.14028307  31787.21328293]]\n",
      "loss in cost function:1079761506.716219\n",
      "Validation Data****************************************\n",
      "theta:[[174251.25596646  47235.14028307  31787.21328293]]\n",
      "loss in cost function:1246435261.8327382\n",
      "Training Data****************************************\n",
      "theta:[[174280.01035444  47241.04767968  31788.80861796]]\n",
      "loss in cost function:1079589089.376625\n",
      "Validation Data****************************************\n",
      "theta:[[174280.01035444  47241.04767968  31788.80861796]]\n",
      "loss in cost function:1246174424.4057813\n",
      "Training Data****************************************\n",
      "theta:[[174308.62097047  47246.92386085  31790.38976111]]\n",
      "loss in cost function:1079418399.7563517\n",
      "Validation Data****************************************\n",
      "theta:[[174308.62097047  47246.92386085  31790.38976111]]\n",
      "loss in cost function:1245915789.6402042\n",
      "Training Data****************************************\n",
      "theta:[[174337.08853343  47252.76899758  31791.95681618]]\n",
      "loss in cost function:1079249420.510314\n",
      "Validation Data****************************************\n",
      "theta:[[174337.08853343  47252.76899758  31791.95681618]]\n",
      "loss in cost function:1245659337.5170317\n",
      "Training Data****************************************\n",
      "theta:[[174365.41375857  47258.58325992  31793.50988628]]\n",
      "loss in cost function:1079082134.4679186\n",
      "Validation Data****************************************\n",
      "theta:[[174365.41375857  47258.58325992  31793.50988628]]\n",
      "loss in cost function:1245405048.20789\n",
      "Training Data****************************************\n",
      "theta:[[174393.59735759  47264.36681696  31795.0490738 ]]\n",
      "loss in cost function:1078916524.6312838\n",
      "Validation Data****************************************\n",
      "theta:[[174393.59735759  47264.36681696  31795.0490738 ]]\n",
      "loss in cost function:1245152902.0731606\n",
      "Training Data****************************************\n",
      "theta:[[174421.64003861  47270.11983682  31796.57448048]]\n",
      "loss in cost function:1078752574.1735265\n",
      "Validation Data****************************************\n",
      "theta:[[174421.64003861  47270.11983682  31796.57448048]]\n",
      "loss in cost function:1244902879.6601224\n",
      "Training Data****************************************\n",
      "theta:[[174449.54250622  47275.8424867   31798.08620734]]\n",
      "loss in cost function:1078590266.437023\n",
      "Validation Data****************************************\n",
      "theta:[[174449.54250622  47275.8424867   31798.08620734]]\n",
      "loss in cost function:1244654961.7011218\n",
      "Training Data****************************************\n",
      "theta:[[174477.3054615   47281.53493283  31799.58435473]]\n",
      "loss in cost function:1078429584.9317026\n",
      "Validation Data****************************************\n",
      "theta:[[174477.3054615   47281.53493283  31799.58435473]]\n",
      "loss in cost function:1244409129.1117642\n",
      "Training Data****************************************\n",
      "theta:[[174504.929602    47287.19734052  31801.06902234]]\n",
      "loss in cost function:1078270513.333358\n",
      "Validation Data****************************************\n",
      "theta:[[174504.929602    47287.19734052  31801.06902234]]\n",
      "loss in cost function:1244165362.9891236\n",
      "Training Data****************************************\n",
      "theta:[[174532.4156218   47292.82987414  31802.54030915]]\n",
      "loss in cost function:1078113035.48198\n",
      "Validation Data****************************************\n",
      "theta:[[174532.4156218   47292.82987414  31802.54030915]]\n",
      "loss in cost function:1243923644.6099584\n",
      "Training Data****************************************\n",
      "theta:[[174559.7642115   47298.43269715  31803.99831351]]\n",
      "loss in cost function:1077957135.3800967\n",
      "Validation Data****************************************\n",
      "theta:[[174559.7642115   47298.43269715  31803.99831351]]\n",
      "loss in cost function:1243683955.428963\n",
      "Training Data****************************************\n",
      "theta:[[174586.97605825  47304.00597207  31805.44313309]]\n",
      "loss in cost function:1077802797.1911318\n",
      "Validation Data****************************************\n",
      "theta:[[174586.97605825  47304.00597207  31805.44313309]]\n",
      "loss in cost function:1243446277.0770183\n",
      "Training Data****************************************\n",
      "theta:[[174614.05184577  47309.54986051  31806.8748649 ]]\n",
      "loss in cost function:1077650005.2377954\n",
      "Validation Data****************************************\n",
      "theta:[[174614.05184577  47309.54986051  31806.8748649 ]]\n",
      "loss in cost function:1243210591.3594778\n",
      "Training Data****************************************\n",
      "theta:[[174640.99225434  47315.06452318  31808.29360529]]\n",
      "loss in cost function:1077498744.000464\n",
      "Validation Data****************************************\n",
      "theta:[[174640.99225434  47315.06452318  31808.29360529]]\n",
      "loss in cost function:1242976880.2544558\n",
      "Training Data****************************************\n",
      "theta:[[174667.79796088  47320.55011987  31809.69944998]]\n",
      "loss in cost function:1077348998.1155999\n",
      "Validation Data****************************************\n",
      "theta:[[174667.79796088  47320.55011987  31809.69944998]]\n",
      "loss in cost function:1242745125.911139\n",
      "Training Data****************************************\n",
      "theta:[[174694.46963888  47326.00680947  31811.09249401]]\n",
      "loss in cost function:1077200752.374177\n",
      "Validation Data****************************************\n",
      "theta:[[174694.46963888  47326.00680947  31811.09249401]]\n",
      "loss in cost function:1242515310.648118\n",
      "Training Data****************************************\n",
      "theta:[[174721.0079585   47331.43475     31812.47283181]]\n",
      "loss in cost function:1077053991.72012\n",
      "Validation Data****************************************\n",
      "theta:[[174721.0079585   47331.43475     31812.47283181]]\n",
      "loss in cost function:1242287416.9517262\n",
      "Training Data****************************************\n",
      "theta:[[174747.41358651  47336.83409857  31813.84055716]]\n",
      "loss in cost function:1076908701.2487617\n",
      "Validation Data****************************************\n",
      "theta:[[174747.41358651  47336.83409857  31813.84055716]]\n",
      "loss in cost function:1242061427.4744084\n",
      "Training Data****************************************\n",
      "theta:[[174773.68718639  47342.2050114   31815.1957632 ]]\n",
      "loss in cost function:1076764866.2053256\n",
      "Validation Data****************************************\n",
      "theta:[[174773.68718639  47342.2050114   31815.1957632 ]]\n",
      "loss in cost function:1241837325.0330892\n",
      "Training Data****************************************\n",
      "theta:[[174799.82941827  47347.54764385  31816.53854243]]\n",
      "loss in cost function:1076622471.9834049\n",
      "Validation Data****************************************\n",
      "theta:[[174799.82941827  47347.54764385  31816.53854243]]\n",
      "loss in cost function:1241615092.6075754\n",
      "Training Data****************************************\n",
      "theta:[[174825.84093898  47352.8621504   31817.86898676]]\n",
      "loss in cost function:1076481504.1234756\n",
      "Validation Data****************************************\n",
      "theta:[[174825.84093898  47352.8621504   31817.86898676]]\n",
      "loss in cost function:1241394713.3389578\n",
      "Training Data****************************************\n",
      "theta:[[174851.7224021   47358.14868464  31819.18718745]]\n",
      "loss in cost function:1076341948.3114135\n",
      "Validation Data****************************************\n",
      "theta:[[174851.7224021   47358.14868464  31819.18718745]]\n",
      "loss in cost function:1241176170.5280392\n",
      "Training Data****************************************\n",
      "theta:[[174877.47445789  47363.40739932  31820.49323514]]\n",
      "loss in cost function:1076203790.377022\n",
      "Validation Data****************************************\n",
      "theta:[[174877.47445789  47363.40739932  31820.49323514]]\n",
      "loss in cost function:1240959447.6337798\n",
      "Training Data****************************************\n",
      "theta:[[174903.09775341  47368.63844633  31821.78721987]]\n",
      "loss in cost function:1076067016.2925925\n",
      "Validation Data****************************************\n",
      "theta:[[174903.09775341  47368.63844633  31821.78721987]]\n",
      "loss in cost function:1240744528.271745\n",
      "Training Data****************************************\n",
      "theta:[[174928.59293245  47373.8419767   31823.06923106]]\n",
      "loss in cost function:1075931612.1714635\n",
      "Validation Data****************************************\n",
      "theta:[[174928.59293245  47373.8419767   31823.06923106]]\n",
      "loss in cost function:1240531396.2125847\n",
      "Training Data****************************************\n",
      "theta:[[174953.9606356   47379.0181406   31824.33935754]]\n",
      "loss in cost function:1075797564.2665937\n",
      "Validation Data****************************************\n",
      "theta:[[174953.9606356   47379.0181406   31824.33935754]]\n",
      "loss in cost function:1240320035.380516\n",
      "Training Data****************************************\n",
      "theta:[[174979.20150023  47384.16708738  31825.59768751]]\n",
      "loss in cost function:1075664858.969172\n",
      "Validation Data****************************************\n",
      "theta:[[174979.20150023  47384.16708738  31825.59768751]]\n",
      "loss in cost function:1240110429.8518271\n",
      "Training Data****************************************\n",
      "theta:[[175004.31616054  47389.28896552  31826.84430859]]\n",
      "loss in cost function:1075533482.8072066\n",
      "Validation Data****************************************\n",
      "theta:[[175004.31616054  47389.28896552  31826.84430859]]\n",
      "loss in cost function:1239902563.8533962\n",
      "Training Data****************************************\n",
      "theta:[[175029.30524754  47394.3839227   31828.07930782]]\n",
      "loss in cost function:1075403422.4441528\n",
      "Validation Data****************************************\n",
      "theta:[[175029.30524754  47394.3839227   31828.07930782]]\n",
      "loss in cost function:1239696421.7612119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data****************************************\n",
      "theta:[[175054.16938911  47399.45210575  31829.30277161]]\n",
      "loss in cost function:1075274664.6775432\n",
      "Validation Data****************************************\n",
      "theta:[[175054.16938911  47399.45210575  31829.30277161]]\n",
      "loss in cost function:1239491988.098937\n",
      "Training Data****************************************\n",
      "theta:[[175078.90920998  47404.49366067  31830.51478583]]\n",
      "loss in cost function:1075147196.4376533\n",
      "Validation Data****************************************\n",
      "theta:[[175078.90920998  47404.49366067  31830.51478583]]\n",
      "loss in cost function:1239289247.5364542\n",
      "Training Data****************************************\n",
      "theta:[[175103.52533173  47409.50873264  31831.71543572]]\n",
      "loss in cost function:1075021004.7861488\n",
      "Validation Data****************************************\n",
      "theta:[[175103.52533173  47409.50873264  31831.71543572]]\n",
      "loss in cost function:1239088184.8884506\n",
      "Training Data****************************************\n",
      "theta:[[175128.01837288  47414.49746605  31832.90480598]]\n",
      "loss in cost function:1074896076.9147654\n",
      "Validation Data****************************************\n",
      "theta:[[175128.01837288  47414.49746605  31832.90480598]]\n",
      "loss in cost function:1238888785.112999\n",
      "Training Data****************************************\n",
      "theta:[[175152.38894883  47419.46000444  31834.08298072]]\n",
      "loss in cost function:1074772400.1439955\n",
      "Validation Data****************************************\n",
      "theta:[[175152.38894883  47419.46000444  31834.08298072]]\n",
      "loss in cost function:1238691033.310168\n",
      "Training Data****************************************\n",
      "theta:[[175176.63767189  47424.39649058  31835.25004347]]\n",
      "loss in cost function:1074649961.9218047\n",
      "Validation Data****************************************\n",
      "theta:[[175176.63767189  47424.39649058  31835.25004347]]\n",
      "loss in cost function:1238494914.720633\n",
      "Training Data****************************************\n",
      "theta:[[175200.76515134  47429.30706641  31836.4060772 ]]\n",
      "loss in cost function:1074528749.8223252\n",
      "Validation Data****************************************\n",
      "theta:[[175200.76515134  47429.30706641  31836.4060772 ]]\n",
      "loss in cost function:1238300414.7243137\n",
      "Training Data****************************************\n",
      "theta:[[175224.77199339  47434.1918731   31837.55116432]]\n",
      "loss in cost function:1074408751.5446029\n",
      "Validation Data****************************************\n",
      "theta:[[175224.77199339  47434.1918731   31837.55116432]]\n",
      "loss in cost function:1238107518.83901\n",
      "Training Data****************************************\n",
      "theta:[[175248.65880123  47439.051051    31838.68538667]]\n",
      "loss in cost function:1074289954.9113343\n",
      "Validation Data****************************************\n",
      "theta:[[175248.65880123  47439.051051    31838.68538667]]\n",
      "loss in cost function:1237916212.7190647\n",
      "Training Data****************************************\n",
      "theta:[[175272.42617504  47443.88473969  31839.80882554]]\n",
      "loss in cost function:1074172347.8676171\n",
      "Validation Data****************************************\n",
      "theta:[[175272.42617504  47443.88473969  31839.80882554]]\n",
      "loss in cost function:1237726482.154035\n",
      "Training Data****************************************\n",
      "theta:[[175296.07471197  47448.69307796  31840.92156166]]\n",
      "loss in cost function:1074055918.479721\n",
      "Validation Data****************************************\n",
      "theta:[[175296.07471197  47448.69307796  31840.92156166]]\n",
      "loss in cost function:1237538313.0673773\n",
      "Training Data****************************************\n",
      "theta:[[175319.60500622  47453.47620382  31842.02367523]]\n",
      "loss in cost function:1073940654.9338655\n",
      "Validation Data****************************************\n",
      "theta:[[175319.60500622  47453.47620382  31842.02367523]]\n",
      "loss in cost function:1237351691.5151367\n",
      "Training Data****************************************\n",
      "theta:[[175343.01764899  47458.23425451  31843.11524588]]\n",
      "loss in cost function:1073826545.535008\n",
      "Validation Data****************************************\n",
      "theta:[[175343.01764899  47458.23425451  31843.11524588]]\n",
      "loss in cost function:1237166603.6846678\n",
      "Training Data****************************************\n",
      "theta:[[175366.31322856  47462.96736651  31844.1963527 ]]\n",
      "loss in cost function:1073713578.7056621\n",
      "Validation Data****************************************\n",
      "theta:[[175366.31322856  47462.96736651  31844.1963527 ]]\n",
      "loss in cost function:1236983035.893352\n",
      "Training Data****************************************\n",
      "theta:[[175389.49233022  47467.6756755   31845.26707425]]\n",
      "loss in cost function:1073601742.9846913\n",
      "Validation Data****************************************\n",
      "theta:[[175389.49233022  47467.6756755   31845.26707425]]\n",
      "loss in cost function:1236800974.5873344\n",
      "Training Data****************************************\n",
      "theta:[[175412.55553638  47472.35931644  31846.32748856]]\n",
      "loss in cost function:1073491027.0261594\n",
      "Validation Data****************************************\n",
      "theta:[[175412.55553638  47472.35931644  31846.32748856]]\n",
      "loss in cost function:1236620406.3402703\n",
      "Training Data****************************************\n",
      "theta:[[175435.50342651  47477.0184235   31847.37767311]]\n",
      "loss in cost function:1073381419.5981501\n",
      "Validation Data****************************************\n",
      "theta:[[175435.50342651  47477.0184235   31847.37767311]]\n",
      "loss in cost function:1236441317.8520873\n",
      "Training Data****************************************\n",
      "theta:[[175458.33657718  47481.65313013  31848.41770487]]\n",
      "loss in cost function:1073272909.5816356\n",
      "Validation Data****************************************\n",
      "theta:[[175458.33657718  47481.65313013  31848.41770487]]\n",
      "loss in cost function:1236263695.94776\n",
      "Training Data****************************************\n",
      "theta:[[175481.0555621   47486.263569    31849.44766027]]\n",
      "loss in cost function:1073165485.9693284\n",
      "Validation Data****************************************\n",
      "theta:[[175481.0555621   47486.263569    31849.44766027]]\n",
      "loss in cost function:1236087527.576091\n",
      "Training Data****************************************\n",
      "theta:[[175503.6609521   47490.84987205  31850.46761522]]\n",
      "loss in cost function:1073059137.8645632\n",
      "Validation Data****************************************\n",
      "theta:[[175503.6609521   47490.84987205  31850.46761522]]\n",
      "loss in cost function:1235912799.8085103\n",
      "Training Data****************************************\n",
      "theta:[[175526.15331515  47495.41217048  31851.47764512]]\n",
      "loss in cost function:1072953854.4801763\n",
      "Validation Data****************************************\n",
      "theta:[[175526.15331515  47495.41217048  31851.47764512]]\n",
      "loss in cost function:1235739499.837883\n",
      "Training Data****************************************\n",
      "theta:[[175548.53321638  47499.95059477  31852.47782486]]\n",
      "loss in cost function:1072849625.1374093\n",
      "Validation Data****************************************\n",
      "theta:[[175548.53321638  47499.95059477  31852.47782486]]\n",
      "loss in cost function:1235567614.977336\n",
      "Training Data****************************************\n",
      "theta:[[175570.80121811  47504.46527464  31853.4682288 ]]\n",
      "loss in cost function:1072746439.2648098\n",
      "Validation Data****************************************\n",
      "theta:[[175570.80121811  47504.46527464  31853.4682288 ]]\n",
      "loss in cost function:1235397132.6590748\n",
      "Training Data****************************************\n",
      "theta:[[175592.95787983  47508.9563391   31854.44893079]]\n",
      "loss in cost function:1072644286.3971605\n",
      "Validation Data****************************************\n",
      "theta:[[175592.95787983  47508.9563391   31854.44893079]]\n",
      "loss in cost function:1235228040.4332504\n",
      "Training Data****************************************\n",
      "theta:[[175615.00375824  47513.42391644  31855.4200042 ]]\n",
      "loss in cost function:1072543156.1743994\n",
      "Validation Data****************************************\n",
      "theta:[[175615.00375824  47513.42391644  31855.4200042 ]]\n",
      "loss in cost function:1235060325.9667969\n",
      "Training Data****************************************\n",
      "theta:[[175636.93940725  47517.86813422  31856.38152189]]\n",
      "loss in cost function:1072443038.340569\n",
      "Validation Data****************************************\n",
      "theta:[[175636.93940725  47517.86813422  31856.38152189]]\n",
      "loss in cost function:1234893977.0423043\n",
      "Training Data****************************************\n",
      "theta:[[175658.76537802  47522.28911929  31857.3335562 ]]\n",
      "loss in cost function:1072343922.7427602\n",
      "Validation Data****************************************\n",
      "theta:[[175658.76537802  47522.28911929  31857.3335562 ]]\n",
      "loss in cost function:1234728981.5568936\n",
      "Training Data****************************************\n",
      "theta:[[175680.48221894  47526.6869978   31858.27617899]]\n",
      "loss in cost function:1072245799.3300871\n",
      "Validation Data****************************************\n",
      "theta:[[175680.48221894  47526.6869978   31858.27617899]]\n",
      "loss in cost function:1234565327.5211155\n",
      "Training Data****************************************\n",
      "theta:[[175702.09047566  47531.06189517  31859.20946164]]\n",
      "loss in cost function:1072148658.15264\n",
      "Validation Data****************************************\n",
      "theta:[[175702.09047566  47531.06189517  31859.20946164]]\n",
      "loss in cost function:1234403003.0578368\n",
      "Training Data****************************************\n",
      "theta:[[175723.59069109  47535.41393615  31860.13347502]]\n",
      "loss in cost function:1072052489.3604931\n",
      "Validation Data****************************************\n",
      "theta:[[175723.59069109  47535.41393615  31860.13347502]]\n",
      "loss in cost function:1234241996.4011614\n",
      "Training Data****************************************\n",
      "theta:[[175744.98340544  47539.74324476  31861.04828953]]\n",
      "loss in cost function:1071957283.2026798\n",
      "Validation Data****************************************\n",
      "theta:[[175744.98340544  47539.74324476  31861.04828953]]\n",
      "loss in cost function:1234082295.8953497\n",
      "Training Data****************************************\n",
      "theta:[[175766.26915622  47544.04994435  31861.95397508]]\n",
      "loss in cost function:1071863030.0261962\n",
      "Validation Data****************************************\n",
      "theta:[[175766.26915622  47544.04994435  31861.95397508]]\n",
      "loss in cost function:1233923889.9937484\n",
      "Training Data****************************************\n",
      "theta:[[175787.44847825  47548.33415756  31862.85060109]]\n",
      "loss in cost function:1071769720.2750242\n",
      "Validation Data****************************************\n",
      "theta:[[175787.44847825  47548.33415756  31862.85060109]]\n",
      "loss in cost function:1233766767.2577372\n",
      "Training Data****************************************\n",
      "theta:[[175808.52190366  47552.59600636  31863.73823654]]\n",
      "loss in cost function:1071677344.4891587\n",
      "Validation Data****************************************\n",
      "theta:[[175808.52190366  47552.59600636  31863.73823654]]\n",
      "loss in cost function:1233610916.355679\n",
      "Training Data****************************************\n",
      "theta:[[175829.48996195  47556.83561204  31864.61694989]]\n",
      "loss in cost function:1071585893.3036205\n",
      "Validation Data****************************************\n",
      "theta:[[175829.48996195  47556.83561204  31864.61694989]]\n",
      "loss in cost function:1233456326.0618882\n",
      "Training Data****************************************\n",
      "theta:[[175850.35317995  47561.05309518  31865.48680916]]\n",
      "loss in cost function:1071495357.447524\n",
      "Validation Data****************************************\n",
      "theta:[[175850.35317995  47561.05309518  31865.48680916]]\n",
      "loss in cost function:1233302985.255597\n",
      "Training Data****************************************\n",
      "theta:[[175871.11208186  47565.24857572  31866.3478819 ]]\n",
      "loss in cost function:1071405727.7431089\n",
      "Validation Data****************************************\n",
      "theta:[[175871.11208186  47565.24857572  31866.3478819 ]]\n",
      "loss in cost function:1233150882.91995\n",
      "Training Data****************************************\n",
      "theta:[[175891.76718926  47569.42217293  31867.20023518]]\n",
      "loss in cost function:1071316995.104819\n",
      "Validation Data****************************************\n",
      "theta:[[175891.76718926  47569.42217293  31867.20023518]]\n",
      "loss in cost function:1233000008.140989\n",
      "Training Data****************************************\n",
      "theta:[[175912.31902112  47573.57400538  31868.04393564]]\n",
      "loss in cost function:1071229150.5383629\n",
      "Validation Data****************************************\n",
      "theta:[[175912.31902112  47573.57400538  31868.04393564]]\n",
      "loss in cost function:1232850350.1066628\n",
      "Training Data****************************************\n",
      "theta:[[175932.76809382  47577.704191    31868.87904942]]\n",
      "loss in cost function:1071142185.1398057\n",
      "Validation Data****************************************\n",
      "theta:[[175932.76809382  47577.704191    31868.87904942]]\n",
      "loss in cost function:1232701898.1058404\n",
      "Training Data****************************************\n",
      "theta:[[175953.11492116  47581.81284707  31869.70564225]]\n",
      "loss in cost function:1071056090.094644\n",
      "Validation Data****************************************\n",
      "theta:[[175953.11492116  47581.81284707  31869.70564225]]\n",
      "loss in cost function:1232554641.5273352\n",
      "Training Data****************************************\n",
      "theta:[[175973.36001437  47585.9000902   31870.52377937]]\n",
      "loss in cost function:1070970856.6769273\n",
      "Validation Data****************************************\n",
      "theta:[[175973.36001437  47585.9000902   31870.52377937]]\n",
      "loss in cost function:1232408569.8589318\n",
      "Training Data****************************************\n",
      "theta:[[175993.5038821   47589.96603635  31871.3335256 ]]\n",
      "loss in cost function:1070886476.2483432\n",
      "Validation Data****************************************\n",
      "theta:[[175993.5038821   47589.96603635  31871.3335256 ]]\n",
      "loss in cost function:1232263672.686443\n",
      "Training Data****************************************\n",
      "theta:[[176013.5470305   47594.01080082  31872.13494529]]\n",
      "loss in cost function:1070802940.2573514\n",
      "Validation Data****************************************\n",
      "theta:[[176013.5470305   47594.01080082  31872.13494529]]\n",
      "loss in cost function:1232119939.6927464\n",
      "Training Data****************************************\n",
      "theta:[[176033.48996316  47598.0344983   31872.92810237]]\n",
      "loss in cost function:1070720240.238309\n",
      "Validation Data****************************************\n",
      "theta:[[176033.48996316  47598.0344983   31872.92810237]]\n",
      "loss in cost function:1231977360.6568587\n",
      "Training Data****************************************\n",
      "theta:[[176053.33318115  47602.0372428   31873.7130603 ]]\n",
      "loss in cost function:1070638367.8106004\n",
      "Validation Data****************************************\n",
      "theta:[[176053.33318115  47602.0372428   31873.7130603 ]]\n",
      "loss in cost function:1231835925.4529967\n",
      "Training Data****************************************\n",
      "theta:[[176073.07718305  47606.01914773  31874.48988214]]\n",
      "loss in cost function:1070557314.6777735\n",
      "Validation Data****************************************\n",
      "theta:[[176073.07718305  47606.01914773  31874.48988214]]\n",
      "loss in cost function:1231695624.04967\n",
      "Training Data****************************************\n",
      "theta:[[176092.72246494  47609.98032583  31875.25863048]]\n",
      "loss in cost function:1070477072.6267221\n",
      "Validation Data****************************************\n",
      "theta:[[176092.72246494  47609.98032583  31875.25863048]]\n",
      "loss in cost function:1231556446.5087557\n",
      "Training Data****************************************\n",
      "theta:[[176112.26952043  47613.92088923  31876.0193675 ]]\n",
      "loss in cost function:1070397633.5268075\n",
      "Validation Data****************************************\n",
      "theta:[[176112.26952043  47613.92088923  31876.0193675 ]]\n",
      "loss in cost function:1231418382.9846094\n",
      "Training Data****************************************\n",
      "theta:[[176131.71884063  47617.84094944  31876.77215495]]\n",
      "loss in cost function:1070318989.3290664\n",
      "Validation Data****************************************\n",
      "theta:[[176131.71884063  47617.84094944  31876.77215495]]\n",
      "loss in cost function:1231281423.7231624\n",
      "Training Data****************************************\n",
      "theta:[[176151.07091424  47621.74061734  31877.51705415]]\n",
      "loss in cost function:1070241132.0653592\n",
      "Validation Data****************************************\n",
      "theta:[[176151.07091424  47621.74061734  31877.51705415]]\n",
      "loss in cost function:1231145559.061048\n",
      "Training Data****************************************\n",
      "theta:[[176170.32622747  47625.62000319  31878.25412598]]\n",
      "loss in cost function:1070164053.8475747\n",
      "Validation Data****************************************\n",
      "theta:[[176170.32622747  47625.62000319  31878.25412598]]\n",
      "loss in cost function:1231010779.4247158\n",
      "Training Data****************************************\n",
      "theta:[[176189.48526415  47629.47921663  31878.98343094]]\n",
      "loss in cost function:1070087746.8668127\n",
      "Validation Data****************************************\n",
      "theta:[[176189.48526415  47629.47921663  31878.98343094]]\n",
      "loss in cost function:1230877075.3295715\n",
      "Training Data****************************************\n",
      "theta:[[176208.54850563  47633.3183667   31879.70502907]]\n",
      "loss in cost function:1070012203.3926022\n",
      "Validation Data****************************************\n",
      "theta:[[176208.54850563  47633.3183667   31879.70502907]]\n",
      "loss in cost function:1230744437.3791187\n",
      "Training Data****************************************\n",
      "theta:[[176227.51643091  47637.13756182  31880.41898002]]\n",
      "loss in cost function:1069937415.7720933\n",
      "Validation Data****************************************\n",
      "theta:[[176227.51643091  47637.13756182  31880.41898002]]\n",
      "loss in cost function:1230612856.2641075\n",
      "Training Data****************************************\n",
      "theta:[[176246.38951657  47640.93690981  31881.12534302]]\n",
      "loss in cost function:1069863376.4292933\n",
      "Validation Data****************************************\n",
      "theta:[[176246.38951657  47640.93690981  31881.12534302]]\n",
      "loss in cost function:1230482322.7616937\n",
      "Training Data****************************************\n",
      "theta:[[176265.16823679  47644.71651789  31881.82417689]]\n",
      "loss in cost function:1069790077.864282\n",
      "Validation Data****************************************\n",
      "theta:[[176265.16823679  47644.71651789  31881.82417689]]\n",
      "loss in cost function:1230352827.7346025\n",
      "Training Data****************************************\n",
      "theta:[[176283.85306342  47648.47649269  31882.51554004]]\n",
      "loss in cost function:1069717512.6524514\n",
      "Validation Data****************************************\n",
      "theta:[[176283.85306342  47648.47649269  31882.51554004]]\n",
      "loss in cost function:1230224362.1303139\n",
      "Training Data****************************************\n",
      "theta:[[176302.44446591  47652.21694022  31883.19949049]]\n",
      "loss in cost function:1069645673.4437447\n",
      "Validation Data****************************************\n",
      "theta:[[176302.44446591  47652.21694022  31883.19949049]]\n",
      "loss in cost function:1230096916.9802356\n",
      "Training Data****************************************\n",
      "theta:[[176320.94291139  47655.93796593  31883.87608584]]\n",
      "loss in cost function:1069574552.9619113\n",
      "Validation Data****************************************\n",
      "theta:[[176320.94291139  47655.93796593  31883.87608584]]\n",
      "loss in cost function:1229970483.3988976\n",
      "Training Data****************************************\n",
      "theta:[[176339.34886464  47659.63967467  31884.5453833 ]]\n",
      "loss in cost function:1069504144.003757\n",
      "Validation Data****************************************\n",
      "theta:[[176339.34886464  47659.63967467  31884.5453833 ]]\n",
      "loss in cost function:1229845052.5831573\n",
      "Training Data****************************************\n",
      "theta:[[176357.66278812  47663.32217068  31885.20743968]]\n",
      "loss in cost function:1069434439.4384166\n",
      "Validation Data****************************************\n",
      "theta:[[176357.66278812  47663.32217068  31885.20743968]]\n",
      "loss in cost function:1229720615.8113937\n",
      "Training Data****************************************\n",
      "theta:[[176375.88514199  47666.98555767  31885.86231141]]\n",
      "loss in cost function:1069365432.2066171\n",
      "Validation Data****************************************\n",
      "theta:[[176375.88514199  47666.98555767  31885.86231141]]\n",
      "loss in cost function:1229597164.4427414\n",
      "Training Data****************************************\n",
      "theta:[[176394.01638409  47670.62993872  31886.51005451]]\n",
      "loss in cost function:1069297115.3199682\n",
      "Validation Data****************************************\n",
      "theta:[[176394.01638409  47670.62993872  31886.51005451]]\n",
      "loss in cost function:1229474689.916294\n",
      "Training Data****************************************\n",
      "theta:[[176412.05696998  47674.25541638  31887.15072462]]\n",
      "loss in cost function:1069229481.8602461\n",
      "Validation Data****************************************\n",
      "theta:[[176412.05696998  47674.25541638  31887.15072462]]\n",
      "loss in cost function:1229353183.750351\n",
      "Training Data****************************************\n",
      "theta:[[176430.00735293  47677.86209259  31887.784377  ]]\n",
      "loss in cost function:1069162524.9786782\n",
      "Validation Data****************************************\n",
      "theta:[[176430.00735293  47677.86209259  31887.784377  ]]\n",
      "loss in cost function:1229232637.5416436\n",
      "Training Data****************************************\n",
      "theta:[[176447.86798398  47681.45006875  31888.41106651]]\n",
      "loss in cost function:1069096237.8952497\n",
      "Validation Data****************************************\n",
      "theta:[[176447.86798398  47681.45006875  31888.41106651]]\n",
      "loss in cost function:1229113042.9645839\n",
      "Training Data****************************************\n",
      "theta:[[176465.63931187  47685.01944569  31889.03084765]]\n",
      "loss in cost function:1069030613.898026\n",
      "Validation Data****************************************\n",
      "theta:[[176465.63931187  47685.01944569  31889.03084765]]\n",
      "loss in cost function:1228994391.7705262\n",
      "Training Data****************************************\n",
      "theta:[[176483.32178312  47688.57032367  31889.64377451]]\n",
      "loss in cost function:1068965646.3424447\n",
      "Validation Data****************************************\n",
      "theta:[[176483.32178312  47688.57032367  31889.64377451]]\n",
      "loss in cost function:1228876675.787016\n",
      "Training Data****************************************\n",
      "theta:[[176500.91584201  47692.1028024   31890.24990085]]\n",
      "loss in cost function:1068901328.6506559\n",
      "Validation Data****************************************\n",
      "theta:[[176500.91584201  47692.1028024   31890.24990085]]\n",
      "loss in cost function:1228759886.9170628\n",
      "Training Data****************************************\n",
      "theta:[[176518.42193061  47695.61698102  31890.84928001]]\n",
      "loss in cost function:1068837654.3108379\n",
      "Validation Data****************************************\n",
      "theta:[[176518.42193061  47695.61698102  31890.84928001]]\n",
      "loss in cost function:1228644017.138415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data****************************************\n",
      "theta:[[176535.84048876  47699.11295814  31891.44196499]]\n",
      "loss in cost function:1068774616.8765398\n",
      "Validation Data****************************************\n",
      "theta:[[176535.84048876  47699.11295814  31891.44196499]]\n",
      "loss in cost function:1228529058.5028422\n",
      "Training Data****************************************\n",
      "theta:[[176553.17195413  47702.59083181  31892.02800841]]\n",
      "loss in cost function:1068712209.9660256\n",
      "Validation Data****************************************\n",
      "theta:[[176553.17195413  47702.59083181  31892.02800841]]\n",
      "loss in cost function:1228415003.1354232\n",
      "Training Data****************************************\n",
      "theta:[[176570.41676216  47706.05069953  31892.60746252]]\n",
      "loss in cost function:1068650427.2616191\n",
      "Validation Data****************************************\n",
      "theta:[[176570.41676216  47706.05069953  31892.60746252]]\n",
      "loss in cost function:1228301843.2338414\n",
      "Training Data****************************************\n",
      "theta:[[176587.57534616  47709.49265827  31893.18037922]]\n",
      "loss in cost function:1068589262.5090662\n",
      "Validation Data****************************************\n",
      "theta:[[176587.57534616  47709.49265827  31893.18037922]]\n",
      "loss in cost function:1228189571.0676901\n",
      "Training Data****************************************\n",
      "theta:[[176604.64813724  47712.91680444  31893.74681003]]\n",
      "loss in cost function:1068528709.5168859\n",
      "Validation Data****************************************\n",
      "theta:[[176604.64813724  47712.91680444  31893.74681003]]\n",
      "loss in cost function:1228078178.977783\n",
      "Training Data****************************************\n",
      "theta:[[176621.63556436  47716.32323393  31894.30680612]]\n",
      "loss in cost function:1068468762.1557511\n",
      "Validation Data****************************************\n",
      "theta:[[176621.63556436  47716.32323393  31894.30680612]]\n",
      "loss in cost function:1227967659.375462\n",
      "Training Data****************************************\n",
      "theta:[[176638.53805435  47719.71204211  31894.8604183 ]]\n",
      "loss in cost function:1068409414.3578502\n",
      "Validation Data****************************************\n",
      "theta:[[176638.53805435  47719.71204211  31894.8604183 ]]\n",
      "loss in cost function:1227858004.7419376\n",
      "Training Data****************************************\n",
      "theta:[[176655.35603188  47723.08332378  31895.40769705]]\n",
      "loss in cost function:1068350660.1162814\n",
      "Validation Data****************************************\n",
      "theta:[[176655.35603188  47723.08332378  31895.40769705]]\n",
      "loss in cost function:1227749207.6276016\n",
      "Training Data****************************************\n",
      "theta:[[176672.08991953  47726.43717325  31895.94869245]]\n",
      "loss in cost function:1068292493.4844314\n",
      "Validation Data****************************************\n",
      "theta:[[176672.08991953  47726.43717325  31895.94869245]]\n",
      "loss in cost function:1227641260.6513746\n",
      "Training Data****************************************\n",
      "theta:[[176688.74013774  47729.77368429  31896.48345428]]\n",
      "loss in cost function:1068234908.575378\n",
      "Validation Data****************************************\n",
      "theta:[[176688.74013774  47729.77368429  31896.48345428]]\n",
      "loss in cost function:1227534156.5000443\n",
      "Training Data****************************************\n",
      "theta:[[176705.30710486  47733.09295015  31897.01203193]]\n",
      "loss in cost function:1068177899.5612779\n",
      "Validation Data****************************************\n",
      "theta:[[176705.30710486  47733.09295015  31897.01203193]]\n",
      "loss in cost function:1227427887.9276183\n",
      "Training Data****************************************\n",
      "theta:[[176721.79123715  47736.39506356  31897.53447448]]\n",
      "loss in cost function:1068121460.6727761\n",
      "Validation Data****************************************\n",
      "theta:[[176721.79123715  47736.39506356  31897.53447448]]\n",
      "loss in cost function:1227322447.7546742\n",
      "Training Data****************************************\n",
      "theta:[[176738.19294877  47739.68011674  31898.05083064]]\n",
      "loss in cost function:1068065586.1984334\n",
      "Validation Data****************************************\n",
      "theta:[[176738.19294877  47739.68011674  31898.05083064]]\n",
      "loss in cost function:1227217828.8677309\n",
      "Training Data****************************************\n",
      "theta:[[176754.51265183  47742.94820139  31898.5611488 ]]\n",
      "loss in cost function:1068010270.4841179\n",
      "Validation Data****************************************\n",
      "theta:[[176754.51265183  47742.94820139  31898.5611488 ]]\n",
      "loss in cost function:1227114024.2186084\n",
      "Training Data****************************************\n",
      "theta:[[176770.75075638  47746.19940871  31899.065477  ]]\n",
      "loss in cost function:1067955507.9324479\n",
      "Validation Data****************************************\n",
      "theta:[[176770.75075638  47746.19940871  31899.065477  ]]\n",
      "loss in cost function:1227011026.8238106\n",
      "Training Data****************************************\n",
      "theta:[[176786.90767041  47749.43382939  31899.56386295]]\n",
      "loss in cost function:1067901293.0022064\n",
      "Validation Data****************************************\n",
      "theta:[[176786.90767041  47749.43382939  31899.56386295]]\n",
      "loss in cost function:1226908829.7638967\n",
      "Training Data****************************************\n",
      "theta:[[176802.98379986  47752.6515536   31900.05635401]]\n",
      "loss in cost function:1067847620.2078017\n",
      "Validation Data****************************************\n",
      "theta:[[176802.98379986  47752.6515536   31900.05635401]]\n",
      "loss in cost function:1226807426.1828783\n",
      "Training Data****************************************\n",
      "theta:[[176818.97954867  47755.85267105  31900.54299724]]\n",
      "loss in cost function:1067794484.1186793\n",
      "Validation Data****************************************\n",
      "theta:[[176818.97954867  47755.85267105  31900.54299724]]\n",
      "loss in cost function:1226706809.287602\n",
      "Training Data****************************************\n",
      "theta:[[176834.89531874  47759.03727091  31901.02383933]]\n",
      "loss in cost function:1067741879.3587794\n",
      "Validation Data****************************************\n",
      "theta:[[176834.89531874  47759.03727091  31901.02383933]]\n",
      "loss in cost function:1226606972.3471646\n",
      "Training Data****************************************\n",
      "theta:[[176850.73150995  47762.20544187  31901.49892668]]\n",
      "loss in cost function:1067689800.6059984\n",
      "Validation Data****************************************\n",
      "theta:[[176850.73150995  47762.20544187  31901.49892668]]\n",
      "loss in cost function:1226507908.6922925\n",
      "Training Data****************************************\n",
      "theta:[[176866.48852021  47765.35727213  31901.96830534]]\n",
      "loss in cost function:1067638242.5916307\n",
      "Validation Data****************************************\n",
      "theta:[[176866.48852021  47765.35727213  31901.96830534]]\n",
      "loss in cost function:1226409611.7147806\n",
      "Training Data****************************************\n",
      "theta:[[176882.16674542  47768.49284941  31902.43202105]]\n",
      "loss in cost function:1067587200.0998422\n",
      "Validation Data****************************************\n",
      "theta:[[176882.16674542  47768.49284941  31902.43202105]]\n",
      "loss in cost function:1226312074.8668888\n",
      "Training Data****************************************\n",
      "theta:[[176897.7665795   47771.61226092  31902.89011923]]\n",
      "loss in cost function:1067536667.9671397\n",
      "Validation Data****************************************\n",
      "theta:[[176897.7665795   47771.61226092  31902.89011923]]\n",
      "loss in cost function:1226215291.6607714\n",
      "Training Data****************************************\n",
      "theta:[[176913.28841441  47774.71559341  31903.34264496]]\n",
      "loss in cost function:1067486641.0818368\n",
      "Validation Data****************************************\n",
      "theta:[[176913.28841441  47774.71559341  31903.34264496]]\n",
      "loss in cost function:1226119255.667905\n",
      "Training Data****************************************\n",
      "theta:[[176928.73264015  47777.80293313  31903.78964303]]\n",
      "loss in cost function:1067437114.3835328\n",
      "Validation Data****************************************\n",
      "theta:[[176928.73264015  47777.80293313  31903.78964303]]\n",
      "loss in cost function:1226023960.5185242\n",
      "Training Data****************************************\n",
      "theta:[[176944.09964475  47780.87436586  31904.2311579 ]]\n",
      "loss in cost function:1067388082.8626059\n",
      "Validation Data****************************************\n",
      "theta:[[176944.09964475  47780.87436586  31904.2311579 ]]\n",
      "loss in cost function:1225929399.9010553\n",
      "Training Data****************************************\n",
      "theta:[[176959.38981434  47783.92997691  31904.66723373]]\n",
      "loss in cost function:1067339541.5597068\n",
      "Validation Data****************************************\n",
      "theta:[[176959.38981434  47783.92997691  31904.66723373]]\n",
      "loss in cost function:1225835567.5615613\n",
      "Training Data****************************************\n",
      "theta:[[176974.60353307  47786.96985111  31905.09791436]]\n",
      "loss in cost function:1067291485.5652274\n",
      "Validation Data****************************************\n",
      "theta:[[176974.60353307  47786.96985111  31905.09791436]]\n",
      "loss in cost function:1225742457.303198\n",
      "Training Data****************************************\n",
      "theta:[[176989.74118322  47789.99407281  31905.52324331]]\n",
      "loss in cost function:1067243910.0188304\n",
      "Validation Data****************************************\n",
      "theta:[[176989.74118322  47789.99407281  31905.52324331]]\n",
      "loss in cost function:1225650062.9856627\n",
      "Training Data****************************************\n",
      "theta:[[177004.80314511  47793.00272592  31905.94326382]]\n",
      "loss in cost function:1067196810.1089389\n",
      "Validation Data****************************************\n",
      "theta:[[177004.80314511  47793.00272592  31905.94326382]]\n",
      "loss in cost function:1225558378.5246608\n",
      "Training Data****************************************\n",
      "theta:[[177019.78979719  47795.99589385  31906.3580188 ]]\n",
      "loss in cost function:1067150181.0722457\n",
      "Validation Data****************************************\n",
      "theta:[[177019.78979719  47795.99589385  31906.3580188 ]]\n",
      "loss in cost function:1225467397.891364\n",
      "Training Data****************************************\n",
      "theta:[[177034.70151601  47798.97365958  31906.76755088]]\n",
      "loss in cost function:1067104018.1932373\n",
      "Validation Data****************************************\n",
      "theta:[[177034.70151601  47798.97365958  31906.76755088]]\n",
      "loss in cost function:1225377115.111891\n",
      "Training Data****************************************\n",
      "theta:[[177049.53867624  47801.93610561  31907.17190238]]\n",
      "loss in cost function:1067058316.8037024\n",
      "Validation Data****************************************\n",
      "theta:[[177049.53867624  47801.93610561  31907.17190238]]\n",
      "loss in cost function:1225287524.2667747\n",
      "Training Data****************************************\n",
      "theta:[[177064.30165067  47804.88331398  31907.57111531]]\n",
      "loss in cost function:1067013072.2822634\n",
      "Validation Data****************************************\n",
      "theta:[[177064.30165067  47804.88331398  31907.57111531]]\n",
      "loss in cost function:1225198619.4904513\n",
      "Training Data****************************************\n",
      "theta:[[177078.99081022  47807.8153663   31907.96523141]]\n",
      "loss in cost function:1066968280.0539055\n",
      "Validation Data****************************************\n",
      "theta:[[177078.99081022  47807.8153663   31907.96523141]]\n",
      "loss in cost function:1225110394.9707432\n",
      "Training Data****************************************\n",
      "theta:[[177093.60652398  47810.73234371  31908.35429209]]\n",
      "loss in cost function:1066923935.5895087\n",
      "Validation Data****************************************\n",
      "theta:[[177093.60652398  47810.73234371  31908.35429209]]\n",
      "loss in cost function:1225022844.94835\n",
      "Training Data****************************************\n",
      "theta:[[177108.14915917  47813.6343269   31908.73833851]]\n",
      "loss in cost function:1066880034.4053813\n",
      "Validation Data****************************************\n",
      "theta:[[177108.14915917  47813.6343269   31908.73833851]]\n",
      "loss in cost function:1224935963.7163453\n",
      "Training Data****************************************\n",
      "theta:[[177122.61908118  47816.52139611  31909.1174115 ]]\n",
      "loss in cost function:1066836572.0628158\n",
      "Validation Data****************************************\n",
      "theta:[[177122.61908118  47816.52139611  31909.1174115 ]]\n",
      "loss in cost function:1224849745.6196797\n",
      "Training Data****************************************\n",
      "theta:[[177137.01665358  47819.39363116  31909.49155162]]\n",
      "loss in cost function:1066793544.1676322\n",
      "Validation Data****************************************\n",
      "theta:[[177137.01665358  47819.39363116  31909.49155162]]\n",
      "loss in cost function:1224764185.0546901\n",
      "Training Data****************************************\n",
      "theta:[[177151.34223812  47822.25111139  31909.86079915]]\n",
      "loss in cost function:1066750946.3697157\n",
      "Validation Data****************************************\n",
      "theta:[[177151.34223812  47822.25111139  31909.86079915]]\n",
      "loss in cost function:1224679276.4686034\n",
      "Training Data****************************************\n",
      "theta:[[177165.59619474  47825.09391574  31910.22519407]]\n",
      "loss in cost function:1066708774.362599\n",
      "Validation Data****************************************\n",
      "theta:[[177165.59619474  47825.09391574  31910.22519407]]\n",
      "loss in cost function:1224595014.3590586\n",
      "Training Data****************************************\n",
      "theta:[[177179.77888158  47827.92212268  31910.58477609]]\n",
      "loss in cost function:1066667023.8830079\n",
      "Validation Data****************************************\n",
      "theta:[[177179.77888158  47827.92212268  31910.58477609]]\n",
      "loss in cost function:1224511393.2736235\n",
      "Training Data****************************************\n",
      "theta:[[177193.89065498  47830.73581027  31910.93958462]]\n",
      "loss in cost function:1066625690.7104217\n",
      "Validation Data****************************************\n",
      "theta:[[177193.89065498  47830.73581027  31910.93958462]]\n",
      "loss in cost function:1224428407.809324\n",
      "Training Data****************************************\n",
      "theta:[[177207.93186951  47833.53505612  31911.28965882]]\n",
      "loss in cost function:1066584770.6666684\n",
      "Validation Data****************************************\n",
      "theta:[[177207.93186951  47833.53505612  31911.28965882]]\n",
      "loss in cost function:1224346052.6121724\n",
      "Training Data****************************************\n",
      "theta:[[177221.90287797  47836.31993743  31911.63503754]]\n",
      "loss in cost function:1066544259.6154662\n",
      "Validation Data****************************************\n",
      "theta:[[177221.90287797  47836.31993743  31911.63503754]]\n",
      "loss in cost function:1224264322.3767002\n",
      "Training Data****************************************\n",
      "theta:[[177235.80403139  47839.09053096  31911.97575938]]\n",
      "loss in cost function:1066504153.4620286\n",
      "Validation Data****************************************\n",
      "theta:[[177235.80403139  47839.09053096  31911.97575938]]\n",
      "loss in cost function:1224183211.845502\n",
      "Training Data****************************************\n",
      "theta:[[177249.63567904  47841.84691305  31912.31186265]]\n",
      "loss in cost function:1066464448.1526366\n",
      "Validation Data****************************************\n",
      "theta:[[177249.63567904  47841.84691305  31912.31186265]]\n",
      "loss in cost function:1224102715.8087711\n",
      "Training Data****************************************\n",
      "theta:[[177263.39816845  47844.5891596   31912.64338541]]\n",
      "loss in cost function:1066425139.6742351\n",
      "Validation Data****************************************\n",
      "theta:[[177263.39816845  47844.5891596   31912.64338541]]\n",
      "loss in cost function:1224022829.1038625\n",
      "Training Data****************************************\n",
      "theta:[[177277.09184542  47847.31734613  31912.97036541]]\n",
      "loss in cost function:1066386224.0539936\n",
      "Validation Data****************************************\n",
      "theta:[[177277.09184542  47847.31734613  31912.97036541]]\n",
      "loss in cost function:1223943546.6148264\n",
      "Training Data****************************************\n",
      "theta:[[177290.717054    47850.03154771  31913.29284018]]\n",
      "loss in cost function:1066347697.358948\n",
      "Validation Data****************************************\n",
      "theta:[[177290.717054    47850.03154771  31913.29284018]]\n",
      "loss in cost function:1223864863.2719772\n",
      "Training Data****************************************\n",
      "theta:[[177304.27413654  47852.73183901  31913.61084695]]\n",
      "loss in cost function:1066309555.6955667\n",
      "Validation Data****************************************\n",
      "theta:[[177304.27413654  47852.73183901  31913.61084695]]\n",
      "loss in cost function:1223786774.0514565\n",
      "Training Data****************************************\n",
      "theta:[[177317.76343366  47855.41829427  31913.92442269]]\n",
      "loss in cost function:1066271795.2093709\n",
      "Validation Data****************************************\n",
      "theta:[[177317.76343366  47855.41829427  31913.92442269]]\n",
      "loss in cost function:1223709273.974788\n",
      "Training Data****************************************\n",
      "theta:[[177331.1852843   47858.09098734  31914.23360411]]\n",
      "loss in cost function:1066234412.0845299\n",
      "Validation Data****************************************\n",
      "theta:[[177331.1852843   47858.09098734  31914.23360411]]\n",
      "loss in cost function:1223632358.1084583\n",
      "Training Data****************************************\n",
      "theta:[[177344.54002569  47860.74999165  31914.53842768]]\n",
      "loss in cost function:1066197402.5434817\n",
      "Validation Data****************************************\n",
      "theta:[[177344.54002569  47860.74999165  31914.53842768]]\n",
      "loss in cost function:1223556021.5634823\n",
      "Training Data****************************************\n",
      "theta:[[177357.82799337  47863.39538024  31914.83892957]]\n",
      "loss in cost function:1066160762.8465457\n",
      "Validation Data****************************************\n",
      "theta:[[177357.82799337  47863.39538024  31914.83892957]]\n",
      "loss in cost function:1223480259.4949906\n",
      "Training Data****************************************\n",
      "theta:[[177371.04952121  47866.02722572  31915.13514573]]\n",
      "loss in cost function:1066124489.2915422\n",
      "Validation Data****************************************\n",
      "theta:[[177371.04952121  47866.02722572  31915.13514573]]\n",
      "loss in cost function:1223405067.101803\n",
      "Training Data****************************************\n",
      "theta:[[177384.20494141  47868.64560032  31915.42711183]]\n",
      "loss in cost function:1066088578.2134157\n",
      "Validation Data****************************************\n",
      "theta:[[177384.20494141  47868.64560032  31915.42711183]]\n",
      "loss in cost function:1223330439.6260157\n",
      "Training Data****************************************\n",
      "theta:[[177397.29458452  47871.25057588  31915.71486329]]\n",
      "loss in cost function:1066053025.9838617\n",
      "Validation Data****************************************\n",
      "theta:[[177397.29458452  47871.25057588  31915.71486329]]\n",
      "loss in cost function:1223256372.3526022\n",
      "Training Data****************************************\n",
      "theta:[[177410.3187794   47873.84222381  31915.99843529]]\n",
      "loss in cost function:1066017829.01096\n",
      "Validation Data****************************************\n",
      "theta:[[177410.3187794   47873.84222381  31915.99843529]]\n",
      "loss in cost function:1223182860.6089947\n",
      "Training Data****************************************\n",
      "theta:[[177423.27785331  47876.42061515  31916.27786273]]\n",
      "loss in cost function:1065982983.7388017\n",
      "Validation Data****************************************\n",
      "theta:[[177423.27785331  47876.42061515  31916.27786273]]\n",
      "loss in cost function:1223109899.7646835\n",
      "Training Data****************************************\n",
      "theta:[[177436.17213185  47878.98582055  31916.5531803 ]]\n",
      "loss in cost function:1065948486.6471382\n",
      "Validation Data****************************************\n",
      "theta:[[177436.17213185  47878.98582055  31916.5531803 ]]\n",
      "loss in cost function:1223037485.230832\n",
      "Training Data****************************************\n",
      "theta:[[177449.001939    47881.53791025  31916.82442242]]\n",
      "loss in cost function:1065914334.2510107\n",
      "Validation Data****************************************\n",
      "theta:[[177449.001939    47881.53791025  31916.82442242]]\n",
      "loss in cost function:1222965612.4598699\n",
      "Training Data****************************************\n",
      "theta:[[177461.76759712  47884.07695414  31917.09162326]]\n",
      "loss in cost function:1065880523.1004139\n",
      "Validation Data****************************************\n",
      "theta:[[177461.76759712  47884.07695414  31917.09162326]]\n",
      "loss in cost function:1222894276.945104\n",
      "Training Data****************************************\n",
      "theta:[[177474.46942694  47886.60302168  31917.35481675]]\n",
      "loss in cost function:1065847049.77992\n",
      "Validation Data****************************************\n",
      "theta:[[177474.46942694  47886.60302168  31917.35481675]]\n",
      "loss in cost function:1222823474.2203414\n",
      "Training Data****************************************\n",
      "theta:[[177487.10774761  47889.11618198  31917.61403658]]\n",
      "loss in cost function:1065813910.9083505\n",
      "Validation Data****************************************\n",
      "theta:[[177487.10774761  47889.11618198  31917.61403658]]\n",
      "loss in cost function:1222753199.8594975\n",
      "Training Data****************************************\n",
      "theta:[[177499.68287668  47891.61650375  31917.86931621]]\n",
      "loss in cost function:1065781103.138429\n",
      "Validation Data****************************************\n",
      "theta:[[177499.68287668  47891.61650375  31917.86931621]]\n",
      "loss in cost function:1222683449.47622\n",
      "Training Data****************************************\n",
      "theta:[[177512.19513011  47894.10405532  31918.12068883]]\n",
      "loss in cost function:1065748623.1564336\n",
      "Validation Data****************************************\n",
      "theta:[[177512.19513011  47894.10405532  31918.12068883]]\n",
      "loss in cost function:1222614218.723517\n",
      "Training Data****************************************\n",
      "theta:[[177524.64482227  47896.57890467  31918.36818743]]\n",
      "loss in cost function:1065716467.6818718\n",
      "Validation Data****************************************\n",
      "theta:[[177524.64482227  47896.57890467  31918.36818743]]\n",
      "loss in cost function:1222545503.2933815\n",
      "Training Data****************************************\n",
      "theta:[[177537.03226596  47899.04111938  31918.61184473]]\n",
      "loss in cost function:1065684633.4671282\n",
      "Validation Data****************************************\n",
      "theta:[[177537.03226596  47899.04111938  31918.61184473]]\n",
      "loss in cost function:1222477298.9164233\n",
      "Training Data****************************************\n",
      "theta:[[177549.35777244  47901.49076666  31918.85169323]]\n",
      "loss in cost function:1065653117.2971535\n",
      "Validation Data****************************************\n",
      "theta:[[177549.35777244  47901.49076666  31918.85169323]]\n",
      "loss in cost function:1222409601.3615124\n",
      "Training Data****************************************\n",
      "theta:[[177561.62165139  47903.92791336  31919.08776521]]\n",
      "loss in cost function:1065621915.9891235\n",
      "Validation Data****************************************\n",
      "theta:[[177561.62165139  47903.92791336  31919.08776521]]\n",
      "loss in cost function:1222342406.4354053\n",
      "Training Data****************************************\n",
      "theta:[[177573.82421094  47906.35262596  31919.32009269]]\n",
      "loss in cost function:1065591026.3921295\n",
      "Validation Data****************************************\n",
      "theta:[[177573.82421094  47906.35262596  31919.32009269]]\n",
      "loss in cost function:1222275709.9823995\n",
      "Training Data****************************************\n",
      "theta:[[177585.96575769  47908.76497056  31919.54870747]]\n",
      "loss in cost function:1065560445.3868417\n",
      "Validation Data****************************************\n",
      "theta:[[177585.96575769  47908.76497056  31919.54870747]]\n",
      "loss in cost function:1222209507.8839757\n",
      "Training Data****************************************\n",
      "theta:[[177598.04659671  47911.1650129   31919.77364115]]\n",
      "loss in cost function:1065530169.8852006\n",
      "Validation Data****************************************\n",
      "theta:[[177598.04659671  47911.1650129   31919.77364115]]\n",
      "loss in cost function:1222143796.058444\n",
      "Training Data****************************************\n",
      "theta:[[177610.06703154  47913.55281839  31919.99492505]]\n",
      "loss in cost function:1065500196.830108\n",
      "Validation Data****************************************\n",
      "theta:[[177610.06703154  47913.55281839  31919.99492505]]\n",
      "loss in cost function:1222078570.460603\n",
      "Training Data****************************************\n",
      "theta:[[177622.02736419  47915.92845203  31920.21259032]]\n",
      "loss in cost function:1065470523.195108\n",
      "Validation Data****************************************\n",
      "theta:[[177622.02736419  47915.92845203  31920.21259032]]\n",
      "loss in cost function:1222013827.0813932\n",
      "Training Data****************************************\n",
      "theta:[[177633.92789517  47918.29197849  31920.42666784]]\n",
      "loss in cost function:1065441145.9840761\n",
      "Validation Data****************************************\n",
      "theta:[[177633.92789517  47918.29197849  31920.42666784]]\n",
      "loss in cost function:1221949561.947556\n",
      "Training Data****************************************\n",
      "theta:[[177645.76892351  47920.64346209  31920.63718829]]\n",
      "loss in cost function:1065412062.230921\n",
      "Validation Data****************************************\n",
      "theta:[[177645.76892351  47920.64346209  31920.63718829]]\n",
      "loss in cost function:1221885771.1212983\n",
      "Training Data****************************************\n",
      "theta:[[177657.5507467   47922.98296679  31920.84418214]]\n",
      "loss in cost function:1065383268.9992929\n",
      "Validation Data****************************************\n",
      "theta:[[177657.5507467   47922.98296679  31920.84418214]]\n",
      "loss in cost function:1221822450.6999588\n",
      "Training Data****************************************\n",
      "theta:[[177669.27366077  47925.31055618  31921.0476796 ]]\n",
      "loss in cost function:1065354763.3822637\n",
      "Validation Data****************************************\n",
      "theta:[[177669.27366077  47925.31055618  31921.0476796 ]]\n",
      "loss in cost function:1221759596.8156765\n",
      "Training Data****************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta:[[177680.93796028  47927.62629352  31921.24771071]]\n",
      "loss in cost function:1065326542.5020374\n",
      "Validation Data****************************************\n",
      "theta:[[177680.93796028  47927.62629352  31921.24771071]]\n",
      "loss in cost function:1221697205.6350682\n",
      "Training Data****************************************\n",
      "theta:[[177692.54393828  47929.93024172  31921.44430527]]\n",
      "loss in cost function:1065298603.5096749\n",
      "Validation Data****************************************\n",
      "theta:[[177692.54393828  47929.93024172  31921.44430527]]\n",
      "loss in cost function:1221635273.3588984\n",
      "Training Data****************************************\n",
      "theta:[[177704.0918864   47932.22246334  31921.63749286]]\n",
      "loss in cost function:1065270943.5847862\n",
      "Validation Data****************************************\n",
      "theta:[[177704.0918864   47932.22246334  31921.63749286]]\n",
      "loss in cost function:1221573796.2217593\n",
      "Training Data****************************************\n",
      "theta:[[177715.58209478  47934.50302061  31921.82730284]]\n",
      "loss in cost function:1065243559.9352423\n",
      "Validation Data****************************************\n",
      "theta:[[177715.58209478  47934.50302061  31921.82730284]]\n",
      "loss in cost function:1221512770.4917562\n",
      "Training Data****************************************\n",
      "theta:[[177727.01485211  47936.77197538  31922.0137644 ]]\n",
      "loss in cost function:1065216449.7969103\n",
      "Validation Data****************************************\n",
      "theta:[[177727.01485211  47936.77197538  31922.0137644 ]]\n",
      "loss in cost function:1221452192.4701967\n",
      "Training Data****************************************\n",
      "theta:[[177738.39044566  47939.0293892   31922.19690646]]\n",
      "loss in cost function:1065189610.4333458\n",
      "Validation Data****************************************\n",
      "theta:[[177738.39044566  47939.0293892   31922.19690646]]\n",
      "loss in cost function:1221392058.4912617\n",
      "Training Data****************************************\n",
      "theta:[[177749.70916124  47941.27532327  31922.37675778]]\n",
      "loss in cost function:1065163039.1355432\n",
      "Validation Data****************************************\n",
      "theta:[[177749.70916124  47941.27532327  31922.37675778]]\n",
      "loss in cost function:1221332364.921724\n",
      "Training Data****************************************\n",
      "theta:[[177760.97128324  47943.50983845  31922.55334688]]\n",
      "loss in cost function:1065136733.2216339\n",
      "Validation Data****************************************\n",
      "theta:[[177760.97128324  47943.50983845  31922.55334688]]\n",
      "loss in cost function:1221273108.1606185\n",
      "Training Data****************************************\n",
      "theta:[[177772.17709463  47945.73299526  31922.72670209]]\n",
      "loss in cost function:1065110690.0366359\n",
      "Validation Data****************************************\n",
      "theta:[[177772.17709463  47945.73299526  31922.72670209]]\n",
      "loss in cost function:1221214284.6389475\n",
      "Training Data****************************************\n",
      "theta:[[177783.32687697  47947.9448539   31922.89685153]]\n",
      "loss in cost function:1065084906.9521708\n",
      "Validation Data****************************************\n",
      "theta:[[177783.32687697  47947.9448539   31922.89685153]]\n",
      "loss in cost function:1221155890.8193917\n",
      "Training Data****************************************\n",
      "theta:[[177794.42091039  47950.14547423  31923.06382311]]\n",
      "loss in cost function:1065059381.3661902\n",
      "Validation Data****************************************\n",
      "theta:[[177794.42091039  47950.14547423  31923.06382311]]\n",
      "loss in cost function:1221097923.1959958\n",
      "Training Data****************************************\n",
      "theta:[[177805.45947365  47952.33491579  31923.22764455]]\n",
      "loss in cost function:1065034110.7027363\n",
      "Validation Data****************************************\n",
      "theta:[[177805.45947365  47952.33491579  31923.22764455]]\n",
      "loss in cost function:1221040378.2938852\n",
      "Training Data****************************************\n",
      "theta:[[177816.44284409  47954.51323778  31923.38834336]]\n",
      "loss in cost function:1065009092.4116594\n",
      "Validation Data****************************************\n",
      "theta:[[177816.44284409  47954.51323778  31923.38834336]]\n",
      "loss in cost function:1220983252.6689773\n",
      "Training Data****************************************\n",
      "theta:[[177827.37129767  47956.68049909  31923.54594685]]\n",
      "loss in cost function:1064984323.9683583\n",
      "Validation Data****************************************\n",
      "theta:[[177827.37129767  47956.68049909  31923.54594685]]\n",
      "loss in cost function:1220926542.9076846\n",
      "Training Data****************************************\n",
      "theta:[[177838.24510899  47958.83675827  31923.70048213]]\n",
      "loss in cost function:1064959802.8735377\n",
      "Validation Data****************************************\n",
      "theta:[[177838.24510899  47958.83675827  31923.70048213]]\n",
      "loss in cost function:1220870245.6266322\n",
      "Training Data****************************************\n",
      "theta:[[177849.06455126  47960.98207358  31923.85197613]]\n",
      "loss in cost function:1064935526.6529393\n",
      "Validation Data****************************************\n",
      "theta:[[177849.06455126  47960.98207358  31923.85197613]]\n",
      "loss in cost function:1220814357.4723804\n",
      "Training Data****************************************\n",
      "theta:[[177859.82989631  47963.11650292  31924.00045556]]\n",
      "loss in cost function:1064911492.8571012\n",
      "Validation Data****************************************\n",
      "theta:[[177859.82989631  47963.11650292  31924.00045556]]\n",
      "loss in cost function:1220758875.1211398\n",
      "Training Data****************************************\n",
      "theta:[[177870.54141464  47965.2401039   31924.14594695]]\n",
      "loss in cost function:1064887699.0611106\n",
      "Validation Data****************************************\n",
      "theta:[[177870.54141464  47965.2401039   31924.14594695]]\n",
      "loss in cost function:1220703795.2784958\n",
      "Training Data****************************************\n",
      "theta:[[177881.19937537  47967.3529338   31924.28847662]]\n",
      "loss in cost function:1064864142.8643368\n",
      "Validation Data****************************************\n",
      "theta:[[177881.19937537  47967.3529338   31924.28847662]]\n",
      "loss in cost function:1220649114.6791296\n",
      "Training Data****************************************\n",
      "theta:[[177891.8040463   47969.45504959  31924.42807073]]\n",
      "loss in cost function:1064840821.8902161\n",
      "Validation Data****************************************\n",
      "theta:[[177891.8040463   47969.45504959  31924.42807073]]\n",
      "loss in cost function:1220594830.0865567\n",
      "Training Data****************************************\n",
      "theta:[[177902.35569388  47971.54650794  31924.56475523]]\n",
      "loss in cost function:1064817733.785994\n",
      "Validation Data****************************************\n",
      "theta:[[177902.35569388  47971.54650794  31924.56475523]]\n",
      "loss in cost function:1220540938.292846\n",
      "Training Data****************************************\n",
      "theta:[[177912.85458322  47973.62736519  31924.69855586]]\n",
      "loss in cost function:1064794876.222482\n",
      "Validation Data****************************************\n",
      "theta:[[177912.85458322  47973.62736519  31924.69855586]]\n",
      "loss in cost function:1220487436.11836\n",
      "Training Data****************************************\n",
      "theta:[[177923.30097811  47975.69767738  31924.82949821]]\n",
      "loss in cost function:1064772246.8938373\n",
      "Validation Data****************************************\n",
      "theta:[[177923.30097811  47975.69767738  31924.82949821]]\n",
      "loss in cost function:1220434320.4114914\n",
      "Training Data****************************************\n",
      "theta:[[177933.69514103  47977.75750024  31924.95760766]]\n",
      "loss in cost function:1064749843.5173099\n",
      "Validation Data****************************************\n",
      "theta:[[177933.69514103  47977.75750024  31924.95760766]]\n",
      "loss in cost function:1220381588.0483937\n",
      "Training Data****************************************\n",
      "theta:[[177944.03733313  47979.80688921  31925.08290941]]\n",
      "loss in cost function:1064727663.833031\n",
      "Validation Data****************************************\n",
      "theta:[[177944.03733313  47979.80688921  31925.08290941]]\n",
      "loss in cost function:1220329235.9327323\n",
      "Training Data****************************************\n",
      "theta:[[177954.32781427  47981.8458994   31925.20542848]]\n",
      "loss in cost function:1064705705.60376\n",
      "Validation Data****************************************\n",
      "theta:[[177954.32781427  47981.8458994   31925.20542848]]\n",
      "loss in cost function:1220277260.9954238\n",
      "Training Data****************************************\n",
      "theta:[[177964.56684301  47983.87458563  31925.3251897 ]]\n",
      "loss in cost function:1064683966.6146786\n",
      "Validation Data****************************************\n",
      "theta:[[177964.56684301  47983.87458563  31925.3251897 ]]\n",
      "loss in cost function:1220225660.1943808\n",
      "Training Data****************************************\n",
      "theta:[[177974.7546766   47985.89300244  31925.44221773]]\n",
      "loss in cost function:1064662444.6731554\n",
      "Validation Data****************************************\n",
      "theta:[[177974.7546766   47985.89300244  31925.44221773]]\n",
      "loss in cost function:1220174430.5142603\n",
      "Training Data****************************************\n",
      "theta:[[177984.89157103  47987.90120403  31925.55653702]]\n",
      "loss in cost function:1064641137.6085207\n",
      "Validation Data****************************************\n",
      "theta:[[177984.89157103  47987.90120403  31925.55653702]]\n",
      "loss in cost function:1220123568.9662151\n",
      "Training Data****************************************\n",
      "theta:[[177994.97778098  47989.89924434  31925.66817188]]\n",
      "loss in cost function:1064620043.2718589\n",
      "Validation Data****************************************\n",
      "theta:[[177994.97778098  47989.89924434  31925.66817188]]\n",
      "loss in cost function:1220073072.5876508\n",
      "Training Data****************************************\n",
      "theta:[[178005.01355988  47991.887177    31925.77714642]]\n",
      "loss in cost function:1064599159.5357683\n",
      "Validation Data****************************************\n",
      "theta:[[178005.01355988  47991.887177    31925.77714642]]\n",
      "loss in cost function:1220022938.441974\n",
      "Training Data****************************************\n",
      "theta:[[178014.99915989  47993.86505534  31925.88348457]]\n",
      "loss in cost function:1064578484.2941719\n",
      "Validation Data****************************************\n",
      "theta:[[178014.99915989  47993.86505534  31925.88348457]]\n",
      "loss in cost function:1219973163.6183581\n",
      "Training Data****************************************\n",
      "theta:[[178024.9348319   47995.83293241  31925.98721009]]\n",
      "loss in cost function:1064558015.4620781\n",
      "Validation Data****************************************\n",
      "theta:[[178024.9348319   47995.83293241  31925.98721009]]\n",
      "loss in cost function:1219923745.2315001\n",
      "Training Data****************************************\n",
      "theta:[[178034.82082555  47997.79086096  31926.08834657]]\n",
      "loss in cost function:1064537750.9753903\n",
      "Validation Data****************************************\n",
      "theta:[[178034.82082555  47997.79086096  31926.08834657]]\n",
      "loss in cost function:1219874680.4213793\n",
      "Training Data****************************************\n",
      "theta:[[178044.65738923  47999.73889347  31926.18691742]]\n",
      "loss in cost function:1064517688.7906824\n",
      "Validation Data****************************************\n",
      "theta:[[178044.65738923  47999.73889347  31926.18691742]]\n",
      "loss in cost function:1219825966.3530324\n",
      "Training Data****************************************\n",
      "theta:[[178054.44477009  48001.67708211  31926.28294588]]\n",
      "loss in cost function:1064497826.8849995\n",
      "Validation Data****************************************\n",
      "theta:[[178054.44477009  48001.67708211  31926.28294588]]\n",
      "loss in cost function:1219777600.2163072\n",
      "Training Data****************************************\n",
      "theta:[[178064.18321405  48003.60547877  31926.37645501]]\n",
      "loss in cost function:1064478163.2556441\n",
      "Validation Data****************************************\n",
      "theta:[[178064.18321405  48003.60547877  31926.37645501]]\n",
      "loss in cost function:1219729579.2256448\n",
      "Training Data****************************************\n",
      "theta:[[178073.87296579  48005.52413506  31926.46746773]]\n",
      "loss in cost function:1064458695.9199882\n",
      "Validation Data****************************************\n",
      "theta:[[178073.87296579  48005.52413506  31926.46746773]]\n",
      "loss in cost function:1219681900.619847\n",
      "Training Data****************************************\n",
      "theta:[[178083.51426877  48007.43310232  31926.55600675]]\n",
      "loss in cost function:1064439422.9152541\n",
      "Validation Data****************************************\n",
      "theta:[[178083.51426877  48007.43310232  31926.55600675]]\n",
      "loss in cost function:1219634561.6618445\n",
      "Training Data****************************************\n",
      "theta:[[178093.10736523  48009.33243159  31926.64209465]]\n",
      "loss in cost function:1064420342.2983221\n",
      "Validation Data****************************************\n",
      "theta:[[178093.10736523  48009.33243159  31926.64209465]]\n",
      "loss in cost function:1219587559.63848\n",
      "Training Data****************************************\n",
      "theta:[[178102.65249621  48011.22217364  31926.72575381]]\n",
      "loss in cost function:1064401452.1455488\n",
      "Validation Data****************************************\n",
      "theta:[[178102.65249621  48011.22217364  31926.72575381]]\n",
      "loss in cost function:1219540891.860285\n",
      "Training Data****************************************\n",
      "theta:[[178112.14990154  48013.10237897  31926.80700646]]\n",
      "loss in cost function:1064382750.5525354\n",
      "Validation Data****************************************\n",
      "theta:[[178112.14990154  48013.10237897  31926.80700646]]\n",
      "loss in cost function:1219494555.6612568\n",
      "Training Data****************************************\n",
      "theta:[[178121.59981984  48014.97309777  31926.88587468]]\n",
      "loss in cost function:1064364235.6339732\n",
      "Validation Data****************************************\n",
      "theta:[[178121.59981984  48014.97309777  31926.88587468]]\n",
      "loss in cost function:1219448548.398645\n",
      "Training Data****************************************\n",
      "theta:[[178131.00248855  48016.83438001  31926.96238036]]\n",
      "loss in cost function:1064345905.523435\n",
      "Validation Data****************************************\n",
      "theta:[[178131.00248855  48016.83438001  31926.96238036]]\n",
      "loss in cost function:1219402867.4527364\n",
      "Training Data****************************************\n",
      "theta:[[178140.35814392  48018.68627535  31927.03654525]]\n",
      "loss in cost function:1064327758.3731687\n",
      "Validation Data****************************************\n",
      "theta:[[178140.35814392  48018.68627535  31927.03654525]]\n",
      "loss in cost function:1219357510.2266343\n",
      "Training Data****************************************\n",
      "theta:[[178149.66702101  48020.52883318  31927.10839093]]\n",
      "loss in cost function:1064309792.3539509\n",
      "Validation Data****************************************\n",
      "theta:[[178149.66702101  48020.52883318  31927.10839093]]\n",
      "loss in cost function:1219312474.146059\n",
      "Training Data****************************************\n",
      "theta:[[178158.92935371  48022.36210263  31927.17793881]]\n",
      "loss in cost function:1064292005.6548612\n",
      "Validation Data****************************************\n",
      "theta:[[178158.92935371  48022.36210263  31927.17793881]]\n",
      "loss in cost function:1219267756.6591246\n",
      "Training Data****************************************\n",
      "theta:[[178168.14537475  48024.18613256  31927.24521016]]\n",
      "loss in cost function:1064274396.4831249\n",
      "Validation Data****************************************\n",
      "theta:[[178168.14537475  48024.18613256  31927.24521016]]\n",
      "loss in cost function:1219223355.2361472\n",
      "Training Data****************************************\n",
      "theta:[[178177.31531568  48026.00097156  31927.31022608]]\n",
      "loss in cost function:1064256963.0639085\n",
      "Validation Data****************************************\n",
      "theta:[[178177.31531568  48026.00097156  31927.31022608]]\n",
      "loss in cost function:1219179267.3694282\n",
      "Training Data****************************************\n",
      "theta:[[178186.43940691  48027.80666797  31927.37300752]]\n",
      "loss in cost function:1064239703.640165\n",
      "Validation Data****************************************\n",
      "theta:[[178186.43940691  48027.80666797  31927.37300752]]\n",
      "loss in cost function:1219135490.573051\n",
      "Training Data****************************************\n",
      "theta:[[178195.51787769  48029.60326984  31927.43357527]]\n",
      "loss in cost function:1064222616.4724365\n",
      "Validation Data****************************************\n",
      "theta:[[178195.51787769  48029.60326984  31927.43357527]]\n",
      "loss in cost function:1219092022.3826895\n",
      "Training Data****************************************\n",
      "theta:[[178204.55095611  48031.39082498  31927.49194997]]\n",
      "loss in cost function:1064205699.8386838\n",
      "Validation Data****************************************\n",
      "theta:[[178204.55095611  48031.39082498  31927.49194997]]\n",
      "loss in cost function:1219048860.3553984\n",
      "Training Data****************************************\n",
      "theta:[[178213.53886913  48033.16938092  31927.54815209]]\n",
      "loss in cost function:1064188952.0341126\n",
      "Validation Data****************************************\n",
      "theta:[[178213.53886913  48033.16938092  31927.54815209]]\n",
      "loss in cost function:1219006002.0694206\n",
      "Training Data****************************************\n",
      "theta:[[178222.4818426   48034.93898496  31927.60220198]]\n",
      "loss in cost function:1064172371.3709972\n",
      "Validation Data****************************************\n",
      "theta:[[178222.4818426   48034.93898496  31927.60220198]]\n",
      "loss in cost function:1218963445.1239898\n",
      "Training Data****************************************\n",
      "theta:[[178231.38010119  48036.69968411  31927.65411981]]\n",
      "loss in cost function:1064155956.1785178\n",
      "Validation Data****************************************\n",
      "theta:[[178231.38010119  48036.69968411  31927.65411981]]\n",
      "loss in cost function:1218921187.1391406\n",
      "Training Data****************************************\n",
      "theta:[[178240.23386849  48038.45152514  31927.7039256 ]]\n",
      "loss in cost function:1064139704.8025782\n",
      "Validation Data****************************************\n",
      "theta:[[178240.23386849  48038.45152514  31927.7039256 ]]\n",
      "loss in cost function:1218879225.7555091\n",
      "Training Data****************************************\n",
      "theta:[[178249.04336696  48040.19455457  31927.75163925]]\n",
      "loss in cost function:1064123615.6056439\n",
      "Validation Data****************************************\n",
      "theta:[[178249.04336696  48040.19455457  31927.75163925]]\n",
      "loss in cost function:1218837558.6341524\n",
      "Training Data****************************************\n",
      "theta:[[178257.80881793  48041.92881864  31927.79728048]]\n",
      "loss in cost function:1064107686.9665893\n",
      "Validation Data****************************************\n",
      "theta:[[178257.80881793  48041.92881864  31927.79728048]]\n",
      "loss in cost function:1218796183.4563522\n",
      "Training Data****************************************\n",
      "theta:[[178266.53044165  48043.65436338  31927.84086887]]\n",
      "loss in cost function:1064091917.2805015\n",
      "Validation Data****************************************\n",
      "theta:[[178266.53044165  48043.65436338  31927.84086887]]\n",
      "loss in cost function:1218755097.9234314\n",
      "Training Data****************************************\n",
      "theta:[[178275.20845725  48045.37123454  31927.88242386]]\n",
      "loss in cost function:1064076304.9585502\n",
      "Validation Data****************************************\n",
      "theta:[[178275.20845725  48045.37123454  31927.88242386]]\n",
      "loss in cost function:1218714299.7565703\n",
      "Training Data****************************************\n",
      "theta:[[178283.84308277  48047.07947762  31927.92196475]]\n",
      "loss in cost function:1064060848.4278109\n",
      "Validation Data****************************************\n",
      "theta:[[178283.84308277  48047.07947762  31927.92196475]]\n",
      "loss in cost function:1218673786.6966252\n",
      "Training Data****************************************\n",
      "theta:[[178292.43453517  48048.77913788  31927.95951069]]\n",
      "loss in cost function:1064045546.1311041\n",
      "Validation Data****************************************\n",
      "theta:[[178292.43453517  48048.77913788  31927.95951069]]\n",
      "loss in cost function:1218633556.5039463\n",
      "Training Data****************************************\n",
      "theta:[[178300.9830303   48050.47026034  31927.99508067]]\n",
      "loss in cost function:1064030396.5268424\n",
      "Validation Data****************************************\n",
      "theta:[[178300.9830303   48050.47026034  31927.99508067]]\n",
      "loss in cost function:1218593606.9581914\n",
      "Training Data****************************************\n",
      "theta:[[178309.48878296  48052.15288976  31928.02869356]]\n",
      "loss in cost function:1064015398.0888699\n",
      "Validation Data****************************************\n",
      "theta:[[178309.48878296  48052.15288976  31928.02869356]]\n",
      "loss in cost function:1218553935.858159\n",
      "Training Data****************************************\n",
      "theta:[[178317.95200685  48053.82707067  31928.06036809]]\n",
      "loss in cost function:1064000549.3063174\n",
      "Validation Data****************************************\n",
      "theta:[[178317.95200685  48053.82707067  31928.06036809]]\n",
      "loss in cost function:1218514541.0216074\n",
      "Training Data****************************************\n",
      "theta:[[178326.37291462  48055.49284735  31928.09012283]]\n",
      "loss in cost function:1063985848.6834334\n",
      "Validation Data****************************************\n",
      "theta:[[178326.37291462  48055.49284735  31928.09012283]]\n",
      "loss in cost function:1218475420.285079\n",
      "Training Data****************************************\n",
      "theta:[[178334.75171786  48057.15026385  31928.11797622]]\n",
      "loss in cost function:1063971294.7394404\n",
      "Validation Data****************************************\n",
      "theta:[[178334.75171786  48057.15026385  31928.11797622]]\n",
      "loss in cost function:1218436571.5037303\n",
      "Training Data****************************************\n",
      "theta:[[178343.08862708  48058.79936395  31928.14394657]]\n",
      "loss in cost function:1063956886.0083921\n",
      "Validation Data****************************************\n",
      "theta:[[178343.08862708  48058.79936395  31928.14394657]]\n",
      "loss in cost function:1218397992.5511615\n",
      "Training Data****************************************\n",
      "theta:[[178351.38385175  48060.44019124  31928.16805203]]\n",
      "loss in cost function:1063942621.0390154\n",
      "Validation Data****************************************\n",
      "theta:[[178351.38385175  48060.44019124  31928.16805203]]\n",
      "loss in cost function:1218359681.319243\n",
      "Training Data****************************************\n",
      "theta:[[178359.6376003   48062.07278902  31928.19031065]]\n",
      "loss in cost function:1063928498.3945637\n",
      "Validation Data****************************************\n",
      "theta:[[178359.6376003   48062.07278902  31928.19031065]]\n",
      "loss in cost function:1218321635.717955\n",
      "Training Data****************************************\n",
      "theta:[[178367.85008011  48063.6972004   31928.21074031]]\n",
      "loss in cost function:1063914516.6526713\n",
      "Validation Data****************************************\n",
      "theta:[[178367.85008011  48063.6972004   31928.21074031]]\n",
      "loss in cost function:1218283853.6752107\n",
      "Training Data****************************************\n",
      "theta:[[178376.02149752  48065.31346823  31928.22935877]]\n",
      "loss in cost function:1063900674.4052105\n",
      "Validation Data****************************************\n",
      "theta:[[178376.02149752  48065.31346823  31928.22935877]]\n",
      "loss in cost function:1218246333.1367035\n",
      "Training Data****************************************\n",
      "theta:[[178384.15205784  48066.92163512  31928.24618365]]\n",
      "loss in cost function:1063886970.258157\n",
      "Validation Data****************************************\n",
      "theta:[[178384.15205784  48066.92163512  31928.24618365]]\n",
      "loss in cost function:1218209072.0657365\n",
      "Training Data****************************************\n",
      "theta:[[178392.24196535  48068.52174349  31928.26123245]]\n",
      "loss in cost function:1063873402.8314317\n",
      "Validation Data****************************************\n",
      "theta:[[178392.24196535  48068.52174349  31928.26123245]]\n",
      "loss in cost function:1218172068.4430645\n",
      "Training Data****************************************\n",
      "theta:[[178400.29142334  48070.11383547  31928.27452252]]\n",
      "loss in cost function:1063859970.7587736\n",
      "Validation Data****************************************\n",
      "theta:[[178400.29142334  48070.11383547  31928.27452252]]\n",
      "loss in cost function:1218135320.2667313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data****************************************\n",
      "theta:[[178408.30063403  48071.69795302  31928.28607109]]\n",
      "loss in cost function:1063846672.6875982\n",
      "Validation Data****************************************\n",
      "theta:[[178408.30063403  48071.69795302  31928.28607109]]\n",
      "loss in cost function:1218098825.551915\n",
      "Training Data****************************************\n",
      "theta:[[178416.26979867  48073.27413783  31928.29589526]]\n",
      "loss in cost function:1063833507.2788569\n",
      "Validation Data****************************************\n",
      "theta:[[178416.26979867  48073.27413783  31928.29589526]]\n",
      "loss in cost function:1218062582.3307657\n",
      "Training Data****************************************\n",
      "theta:[[178424.19911748  48074.84243137  31928.304012  ]]\n",
      "loss in cost function:1063820473.2069128\n",
      "Validation Data****************************************\n",
      "theta:[[178424.19911748  48074.84243137  31928.304012  ]]\n",
      "loss in cost function:1218026588.6522615\n",
      "Training Data****************************************\n",
      "theta:[[178432.0887897   48076.40287491  31928.31043814]]\n",
      "loss in cost function:1063807569.1593795\n",
      "Validation Data****************************************\n",
      "theta:[[178432.0887897   48076.40287491  31928.31043814]]\n",
      "loss in cost function:1217990842.5820389\n",
      "Training Data****************************************\n",
      "theta:[[178439.93901356  48077.95550947  31928.3151904 ]]\n",
      "loss in cost function:1063794793.8370245\n",
      "Validation Data****************************************\n",
      "theta:[[178439.93901356  48077.95550947  31928.3151904 ]]\n",
      "loss in cost function:1217955342.2022543\n",
      "Training Data****************************************\n",
      "theta:[[178447.7499863   48079.50037586  31928.31828536]]\n",
      "loss in cost function:1063782145.9536138\n",
      "Validation Data****************************************\n",
      "theta:[[178447.7499863   48079.50037586  31928.31828536]]\n",
      "loss in cost function:1217920085.6114254\n",
      "Training Data****************************************\n",
      "theta:[[178455.52190418  48081.03751466  31928.31973949]]\n",
      "loss in cost function:1063769624.2357769\n",
      "Validation Data****************************************\n",
      "theta:[[178455.52190418  48081.03751466  31928.31973949]]\n",
      "loss in cost function:1217885070.9242847\n",
      "Training Data****************************************\n",
      "theta:[[178463.25496247  48082.56696624  31928.31956911]]\n",
      "loss in cost function:1063757227.4229045\n",
      "Validation Data****************************************\n",
      "theta:[[178463.25496247  48082.56696624  31928.31956911]]\n",
      "loss in cost function:1217850296.2716277\n",
      "Training Data****************************************\n",
      "theta:[[178470.94935546  48084.08877074  31928.31779044]]\n",
      "loss in cost function:1063744954.266988\n",
      "Validation Data****************************************\n",
      "theta:[[178470.94935546  48084.08877074  31928.31779044]]\n",
      "loss in cost function:1217815759.8001745\n",
      "Training Data****************************************\n",
      "theta:[[178478.60527649  48085.60296808  31928.31441957]]\n",
      "loss in cost function:1063732803.532522\n",
      "Validation Data****************************************\n",
      "theta:[[178478.60527649  48085.60296808  31928.31441957]]\n",
      "loss in cost function:1217781459.6724167\n",
      "Training Data****************************************\n",
      "theta:[[178486.22291792  48087.10959799  31928.30947245]]\n",
      "loss in cost function:1063720773.996354\n",
      "Validation Data****************************************\n",
      "theta:[[178486.22291792  48087.10959799  31928.30947245]]\n",
      "loss in cost function:1217747394.066475\n",
      "Training Data****************************************\n",
      "theta:[[178493.80247114  48088.60869995  31928.30296494]]\n",
      "loss in cost function:1063708864.447583\n",
      "Validation Data****************************************\n",
      "theta:[[178493.80247114  48088.60869995  31928.30296494]]\n",
      "loss in cost function:1217713561.1759615\n",
      "Training Data****************************************\n",
      "theta:[[178501.34412659  48090.10031325  31928.29491275]]\n",
      "loss in cost function:1063697073.6874204\n",
      "Validation Data****************************************\n",
      "theta:[[178501.34412659  48090.10031325  31928.29491275]]\n",
      "loss in cost function:1217679959.209833\n",
      "Training Data****************************************\n",
      "theta:[[178508.84807376  48091.58447695  31928.28533148]]\n",
      "loss in cost function:1063685400.5290747\n",
      "Validation Data****************************************\n",
      "theta:[[178508.84807376  48091.58447695  31928.28533148]]\n",
      "loss in cost function:1217646586.392253\n",
      "Training Data****************************************\n",
      "theta:[[178516.3145012   48093.06122992  31928.27423663]]\n",
      "loss in cost function:1063673843.7976319\n",
      "Validation Data****************************************\n",
      "theta:[[178516.3145012   48093.06122992  31928.27423663]]\n",
      "loss in cost function:1217613440.9624557\n",
      "Training Data****************************************\n",
      "theta:[[178523.74359651  48094.53061079  31928.26164355]]\n",
      "loss in cost function:1063662402.3299375\n",
      "Validation Data****************************************\n",
      "theta:[[178523.74359651  48094.53061079  31928.26164355]]\n",
      "loss in cost function:1217580521.174604\n",
      "Training Data****************************************\n",
      "theta:[[178531.13554633  48095.99265801  31928.24756749]]\n",
      "loss in cost function:1063651074.9744731\n",
      "Validation Data****************************************\n",
      "theta:[[178531.13554633  48095.99265801  31928.24756749]]\n",
      "loss in cost function:1217547825.2976606\n",
      "Training Data****************************************\n",
      "theta:[[178538.49053641  48097.4474098   31928.23202359]]\n",
      "loss in cost function:1063639860.5912428\n",
      "Validation Data****************************************\n",
      "theta:[[178538.49053641  48097.4474098   31928.23202359]]\n",
      "loss in cost function:1217515351.615246\n",
      "Training Data****************************************\n",
      "theta:[[178545.80875153  48098.89490419  31928.21502685]]\n",
      "loss in cost function:1063628758.0516618\n",
      "Validation Data****************************************\n",
      "theta:[[178545.80875153  48098.89490419  31928.21502685]]\n",
      "loss in cost function:1217483098.4255116\n",
      "Training Data****************************************\n",
      "theta:[[178553.09037559  48100.33517899  31928.19659218]]\n",
      "loss in cost function:1063617766.2384361\n",
      "Validation Data****************************************\n",
      "theta:[[178553.09037559  48100.33517899  31928.19659218]]\n",
      "loss in cost function:1217451064.0410013\n",
      "Training Data****************************************\n",
      "theta:[[178560.33559152  48101.7682718   31928.17673436]]\n",
      "loss in cost function:1063606884.0454534\n",
      "Validation Data****************************************\n",
      "theta:[[178560.33559152  48101.7682718   31928.17673436]]\n",
      "loss in cost function:1217419246.788531\n",
      "Training Data****************************************\n",
      "theta:[[178567.54458137  48103.19422005  31928.15546807]]\n",
      "loss in cost function:1063596110.3776642\n",
      "Validation Data****************************************\n",
      "theta:[[178567.54458137  48103.19422005  31928.15546807]]\n",
      "loss in cost function:1217387645.0090442\n",
      "Training Data****************************************\n",
      "theta:[[178574.71752627  48104.61306093  31928.13280786]]\n",
      "loss in cost function:1063585444.1509827\n",
      "Validation Data****************************************\n",
      "theta:[[178574.71752627  48104.61306093  31928.13280786]]\n",
      "loss in cost function:1217356257.0574996\n",
      "Training Data****************************************\n",
      "theta:[[178581.85460644  48106.02483145  31928.10876818]]\n",
      "loss in cost function:1063574884.292165\n",
      "Validation Data****************************************\n",
      "theta:[[178581.85460644  48106.02483145  31928.10876818]]\n",
      "loss in cost function:1217325081.3027315\n",
      "Training Data****************************************\n",
      "theta:[[178588.95600122  48107.4295684   31928.08336336]]\n",
      "loss in cost function:1063564429.7387087\n",
      "Validation Data****************************************\n",
      "theta:[[178588.95600122  48107.4295684   31928.08336336]]\n",
      "loss in cost function:1217294116.1273339\n",
      "Training Data****************************************\n",
      "theta:[[178596.02188902  48108.8273084   31928.05660764]]\n",
      "loss in cost function:1063554079.4387385\n",
      "Validation Data****************************************\n",
      "theta:[[178596.02188902  48108.8273084   31928.05660764]]\n",
      "loss in cost function:1217263359.9275258\n",
      "Training Data****************************************\n",
      "theta:[[178603.05244739  48110.21808785  31928.02851513]]\n",
      "loss in cost function:1063543832.3509068\n",
      "Validation Data****************************************\n",
      "theta:[[178603.05244739  48110.21808785  31928.02851513]]\n",
      "loss in cost function:1217232811.1130378\n",
      "Training Data****************************************\n",
      "theta:[[178610.04785296  48111.60194296  31927.99909983]]\n",
      "loss in cost function:1063533687.4442866\n",
      "Validation Data****************************************\n",
      "theta:[[178610.04785296  48111.60194296  31927.99909983]]\n",
      "loss in cost function:1217202468.1069815\n",
      "Training Data****************************************\n",
      "theta:[[178617.0082815   48112.97890974  31927.96837565]]\n",
      "loss in cost function:1063523643.6982579\n",
      "Validation Data****************************************\n",
      "theta:[[178617.0082815   48112.97890974  31927.96837565]]\n",
      "loss in cost function:1217172329.3457346\n",
      "Training Data****************************************\n",
      "theta:[[178623.9339079   48114.34902402  31927.93635638]]\n",
      "loss in cost function:1063513700.1024148\n",
      "Validation Data****************************************\n",
      "theta:[[178623.9339079   48114.34902402  31927.93635638]]\n",
      "loss in cost function:1217142393.2788172\n",
      "Training Data****************************************\n",
      "theta:[[178630.82490617  48115.71232141  31927.90305569]]\n",
      "loss in cost function:1063503855.6564666\n",
      "Validation Data****************************************\n",
      "theta:[[178630.82490617  48115.71232141  31927.90305569]]\n",
      "loss in cost function:1217112658.368777\n",
      "Training Data****************************************\n",
      "theta:[[178637.68144945  48117.06883734  31927.86848718]]\n",
      "loss in cost function:1063494109.3701241\n",
      "Validation Data****************************************\n",
      "theta:[[178637.68144945  48117.06883734  31927.86848718]]\n",
      "loss in cost function:1217083123.0910633\n",
      "Training Data****************************************\n",
      "theta:[[178644.50371001  48118.41860707  31927.83266431]]\n",
      "loss in cost function:1063484460.2629966\n",
      "Validation Data****************************************\n",
      "theta:[[178644.50371001  48118.41860707  31927.83266431]]\n",
      "loss in cost function:1217053785.9339213\n",
      "Training Data****************************************\n",
      "theta:[[178651.29185927  48119.76166564  31927.79560046]]\n",
      "loss in cost function:1063474907.3645172\n",
      "Validation Data****************************************\n",
      "theta:[[178651.29185927  48119.76166564  31927.79560046]]\n",
      "loss in cost function:1217024645.398266\n",
      "Training Data****************************************\n",
      "theta:[[178658.04606778  48121.09804791  31927.75730889]]\n",
      "loss in cost function:1063465449.7138193\n",
      "Validation Data****************************************\n",
      "theta:[[178658.04606778  48121.09804791  31927.75730889]]\n",
      "loss in cost function:1216995699.9975793\n",
      "Training Data****************************************\n",
      "theta:[[178664.76650525  48122.42778856  31927.71780276]]\n",
      "loss in cost function:1063456086.3596488\n",
      "Validation Data****************************************\n",
      "theta:[[178664.76650525  48122.42778856  31927.71780276]]\n",
      "loss in cost function:1216966948.2577872\n",
      "Training Data****************************************\n",
      "theta:[[178671.45334053  48123.75092206  31927.67709513]]\n",
      "loss in cost function:1063446816.3602744\n",
      "Validation Data****************************************\n",
      "theta:[[178671.45334053  48123.75092206  31927.67709513]]\n",
      "loss in cost function:1216938388.7171476\n",
      "Training Data****************************************\n",
      "theta:[[178678.10674164  48125.06748273  31927.63519896]]\n",
      "loss in cost function:1063437638.7833732\n",
      "Validation Data****************************************\n",
      "theta:[[178678.10674164  48125.06748273  31927.63519896]]\n",
      "loss in cost function:1216910019.9261487\n",
      "Training Data****************************************\n",
      "theta:[[178684.72687574  48126.37750468  31927.59212712]]\n",
      "loss in cost function:1063428552.7059568\n",
      "Validation Data****************************************\n",
      "theta:[[178684.72687574  48126.37750468  31927.59212712]]\n",
      "loss in cost function:1216881840.447389\n",
      "Training Data****************************************\n",
      "theta:[[178691.31390916  48127.68102183  31927.54789235]]\n",
      "loss in cost function:1063419557.2142708\n",
      "Validation Data****************************************\n",
      "theta:[[178691.31390916  48127.68102183  31927.54789235]]\n",
      "loss in cost function:1216853848.8554723\n",
      "Training Data****************************************\n",
      "theta:[[178697.86800743  48128.97806793  31927.50250732]]\n",
      "loss in cost function:1063410651.403692\n",
      "Validation Data****************************************\n",
      "theta:[[178697.86800743  48128.97806793  31927.50250732]]\n",
      "loss in cost function:1216826043.7368977\n",
      "Training Data****************************************\n",
      "theta:[[178704.3893352   48130.26867656  31927.45598459]]\n",
      "loss in cost function:1063401834.3786588\n",
      "Validation Data****************************************\n",
      "theta:[[178704.3893352   48130.26867656  31927.45598459]]\n",
      "loss in cost function:1216798423.689959\n",
      "Training Data****************************************\n",
      "theta:[[178710.87805633  48131.55288109  31927.40833661]]\n",
      "loss in cost function:1063393105.2525524\n",
      "Validation Data****************************************\n",
      "theta:[[178710.87805633  48131.55288109  31927.40833661]]\n",
      "loss in cost function:1216770987.3246264\n",
      "Training Data****************************************\n",
      "theta:[[178717.33433386  48132.83071472  31927.35957575]]\n",
      "loss in cost function:1063384463.1476369\n",
      "Validation Data****************************************\n",
      "theta:[[178717.33433386  48132.83071472  31927.35957575]]\n",
      "loss in cost function:1216743733.2624564\n",
      "Training Data****************************************\n",
      "theta:[[178723.75833     48134.10221049  31927.30971428]]\n",
      "loss in cost function:1063375907.1949427\n",
      "Validation Data****************************************\n",
      "theta:[[178723.75833     48134.10221049  31927.30971428]]\n",
      "loss in cost function:1216716660.136473\n",
      "Training Data****************************************\n",
      "theta:[[178730.15020615  48135.36740124  31927.25876437]]\n",
      "loss in cost function:1063367436.5341994\n",
      "Validation Data****************************************\n",
      "theta:[[178730.15020615  48135.36740124  31927.25876437]]\n",
      "loss in cost function:1216689766.5910788\n",
      "Training Data****************************************\n",
      "theta:[[178736.51012293  48136.62631964  31927.2067381 ]]\n",
      "loss in cost function:1063359050.3137414\n",
      "Validation Data****************************************\n",
      "theta:[[178736.51012293  48136.62631964  31927.2067381 ]]\n",
      "loss in cost function:1216663051.2819393\n",
      "Training Data****************************************\n",
      "theta:[[178742.83824013  48137.87899819  31927.15364744]]\n",
      "loss in cost function:1063350747.6904188\n",
      "Validation Data****************************************\n",
      "theta:[[178742.83824013  48137.87899819  31927.15364744]]\n",
      "loss in cost function:1216636512.8758943\n",
      "Training Data****************************************\n",
      "theta:[[178749.13471673  48139.1254692   31927.09950429]]\n",
      "loss in cost function:1063342527.8295116\n",
      "Validation Data****************************************\n",
      "theta:[[178749.13471673  48139.1254692   31927.09950429]]\n",
      "loss in cost function:1216610150.05085\n",
      "Training Data****************************************\n",
      "theta:[[178755.39971096  48140.36576482  31927.04432043]]\n",
      "loss in cost function:1063334389.9046509\n",
      "Validation Data****************************************\n",
      "theta:[[178755.39971096  48140.36576482  31927.04432043]]\n",
      "loss in cost function:1216583961.4956782\n",
      "Training Data****************************************\n",
      "theta:[[178761.63338021  48141.59991702  31926.98810758]]\n",
      "loss in cost function:1063326333.0977336\n",
      "Validation Data****************************************\n",
      "theta:[[178761.63338021  48141.59991702  31926.98810758]]\n",
      "loss in cost function:1216557945.910129\n",
      "Training Data****************************************\n",
      "theta:[[178767.83588112  48142.8279576   31926.93087732]]\n",
      "loss in cost function:1063318356.5988387\n",
      "Validation Data****************************************\n",
      "theta:[[178767.83588112  48142.8279576   31926.93087732]]\n",
      "loss in cost function:1216532102.0047193\n",
      "Training Data****************************************\n",
      "theta:[[178774.00736952  48144.04991819  31926.87264119]]\n",
      "loss in cost function:1063310459.606141\n",
      "Validation Data****************************************\n",
      "theta:[[178774.00736952  48144.04991819  31926.87264119]]\n",
      "loss in cost function:1216506428.5006452\n",
      "Training Data****************************************\n",
      "theta:[[178780.14800048  48145.26583025  31926.81341061]]\n",
      "loss in cost function:1063302641.32584\n",
      "Validation Data****************************************\n",
      "theta:[[178780.14800048  48145.26583025  31926.81341061]]\n",
      "loss in cost function:1216480924.1296842\n",
      "Training Data****************************************\n",
      "theta:[[178786.25792829  48146.47572507  31926.75319692]]\n",
      "loss in cost function:1063294900.9720647\n",
      "Validation Data****************************************\n",
      "theta:[[178786.25792829  48146.47572507  31926.75319692]]\n",
      "loss in cost function:1216455587.634102\n",
      "Training Data****************************************\n",
      "theta:[[178792.33730645  48147.67963376  31926.69201135]]\n",
      "loss in cost function:1063287237.766815\n",
      "Validation Data****************************************\n",
      "theta:[[178792.33730645  48147.67963376  31926.69201135]]\n",
      "loss in cost function:1216430417.7665582\n",
      "Training Data****************************************\n",
      "theta:[[178798.38628773  48148.87758728  31926.62986508]]\n",
      "loss in cost function:1063279650.9398553\n",
      "Validation Data****************************************\n",
      "theta:[[178798.38628773  48148.87758728  31926.62986508]]\n",
      "loss in cost function:1216405413.2900095\n",
      "Training Data****************************************\n",
      "theta:[[178804.4050241   48150.06961642  31926.56676916]]\n",
      "loss in cost function:1063272139.7286716\n",
      "Validation Data****************************************\n",
      "theta:[[178804.4050241   48150.06961642  31926.56676916]]\n",
      "loss in cost function:1216380572.9776247\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数値大きくないか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "散布図で表示してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel(\"test\")\n",
    "plt.ylabel(\"predict\")\n",
    "plt.scatter(y_test_scaled, y_pred, \n",
    "            marker=\"x\", alpha=0.5, c=\"r\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できているように見える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearRegressionと比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train_scaled)\n",
    "y_pred_lr= model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel(\"test\")\n",
    "plt.ylabel(\"predict\")\n",
    "plt.scatter(y_test_scaled, y_pred, \n",
    "            marker=\"x\", alpha=0.5, c=\"r\")\n",
    "plt.scatter(y_test_scaled, y_pred_lr, \n",
    "            marker=\"o\", alpha=0.5, c=\"b\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "あっている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題7】学習曲線のプロット\n",
    "\n",
    "学習曲線を表示する関数を作成し、実行してください。グラフを見て損失が適切に下がっているかどうか確認してください。\n",
    "\n",
    "\n",
    "線形回帰クラスの雛形ではself.loss, self.val_lossに損失を記録しておくようになっているため、入力にはこれを利用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x  = np.arange(0,model.iter)\n",
    "\n",
    "plt.plot(x, model.loss, 'o-', color=\"b\", label=\"train loss\")\n",
    "plt.plot(x, model.val_loss, 'x', color=\"r\", label=\"val loss\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
