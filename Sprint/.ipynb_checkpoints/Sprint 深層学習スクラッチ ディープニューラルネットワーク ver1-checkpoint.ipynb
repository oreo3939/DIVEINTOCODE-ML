{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ディープニューラルネットワークスクラッチ\n",
    "\n",
    "前回は3層のニューラルネットワークを作成しましたが、今回はこれを任意の層数に拡張しやすいものに書き換えていきます。その上で、活性化関数や初期値、最適化手法について発展的なものを扱えるようにしていきます。\n",
    "\n",
    "\n",
    "このようなスクラッチを行うことで、今後各種フレームワークを利用していくにあたり、内部の動きが想像できることを目指します。\n",
    "\n",
    "\n",
    "名前は新しくScratchDeepNeuralNetrowkClassifierクラスとしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##層などのクラス化\n",
    "\n",
    "クラスにまとめて行くことで、構成を変更しやすい実装にしていきます。\n",
    "\n",
    "\n",
    "手を加える箇所\n",
    "\n",
    "\n",
    "・層の数\n",
    "\n",
    "・層の種類（今後畳み込み層など他のタイプの層が登場する）\n",
    "\n",
    "・活性化関数の種類\n",
    "\n",
    "・重みやバイアスの初期化方法\n",
    "\n",
    "・最適化手法\n",
    "\n",
    "そのために、全結合層、各種活性化関数、重みやバイアスの初期化、最適化手法それぞれのクラスを作成します。\n",
    "\n",
    "\n",
    "**実装方法は自由**ですが、簡単な例を紹介します。サンプルコード1のように全結合層と活性化関数のインスタンスを作成し、サンプルコード2,3のようにして使用します。それぞれのクラスについてはこのあと解説します。\n",
    "\n",
    "《サンプルコード1》\n",
    "\n",
    "\n",
    "ScratchDeepNeuralNetrowkClassifierのfitメソッド内\n",
    "\n",
    "    # self.sigma : ガウス分布の標準偏差\n",
    "    # self.lr : 学習率\n",
    "    # self.n_nodes1 : 1層目のノード数\n",
    "    # self.n_nodes2 : 2層目のノード数\n",
    "    # self.n_output : 出力層のノード数\n",
    "    optimizer = SGD(self.lr)\n",
    "    self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "    self.activation1 = Tanh()\n",
    "    self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "    self.activation2 = Tanh()\n",
    "    self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "    self.activation3 = Softmax()\n",
    "    \n",
    " \n",
    "《サンプルコード2》\n",
    "\n",
    "\n",
    "イテレーションごとのフォワード\n",
    "\n",
    "    A1 = self.FC1.forward(X)\n",
    "    Z1 = self.activation1.forward(A1)\n",
    "    A2 = self.FC2.forward(Z1)\n",
    "    Z2 = self.activation2.forward(A2)\n",
    "    A3 = self.FC3.forward(Z2)\n",
    "    Z3 = self.activation3.forward(A3)\n",
    "    \n",
    "    \n",
    " 《サンプルコード3》\n",
    "\n",
    "\n",
    "イテレーションごとのバックワード\n",
    "\n",
    "    dA3 = self.activation3.backward(Z3, Y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "    dZ2 = self.FC3.backward(dA3)\n",
    "    dA2 = self.activation2.backward(dZ2)\n",
    "    dZ1 = self.FC2.backward(dA2)\n",
    "    dA1 = self.activation1.backward(dZ1)\n",
    "    dZ0 = self.FC1.backward(dA1) # dZ0は使用しない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題1】全結合層のクラス化\n",
    "\n",
    "全結合層のクラス化を行なってください。\n",
    "\n",
    "\n",
    "以下に雛形を載せました。コンストラクタで重みやバイアスの初期化をして、あとはフォワードとバックワードのメソッドを用意します。重みW、バイアスB、およびフォワード時の入力Xをインスタンス変数として保持しておくことで、煩雑な入出力は不要になります。\n",
    "\n",
    "\n",
    "なお、インスタンスも引数として渡すことができます。そのため、初期化方法のインスタンスinitializerをコンストラクタで受け取れば、それにより初期化が行われます。渡すインスタンスを変えれば、初期化方法が変えられます。\n",
    "\n",
    "\n",
    "また、引数として自身のインスタンスselfを渡すこともできます。これを利用してself.optimizer.update(self)という風に層の重みの更新が可能です。更新に必要な値は複数ありますが、全て全結合層が持つインスタンス変数にすることができます。\n",
    "\n",
    "\n",
    "初期化方法と最適化手法のクラスについては後述します。\n",
    "\n",
    "\n",
    "《雛形》\n",
    "\n",
    "    class FC:\n",
    "        \"\"\"\n",
    "        ノード数n_nodes1からn_nodes2への全結合層\n",
    "        Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        pass\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        pass\n",
    "        return A\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        pass\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.b = initializer.B(n_nodes2)\n",
    "        self.X = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        out : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        self.X = X # 保持\n",
    "        out = np.dot(X, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"        \n",
    "        # 入力値の微分 = (逆伝播の)入力値 * 重みの転置\n",
    "        dX = np.dot(dA, self.W.T)\n",
    "        # 重みパラメータの微分 = 順伝播時の入力値の転置 * 逆伝播の入力値\n",
    "        self.dW = np.dot(self.X.T, dA)\n",
    "        # バイアスパラメータの微分 = 逆伝播の入力値の総和\n",
    "        self.db = np.sum(dA, axis=0)\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題2】初期化方法のクラス化\n",
    "\n",
    "初期化を行うコードをクラス化してください。\n",
    "\n",
    "\n",
    "前述のように、全結合層のコンストラクタに初期化方法のインスタンスを渡せるようにします。以下の雛形に必要なコードを書き加えていってください。標準偏差の値（sigma）はコンストラクタで受け取るようにすることで、全結合層のクラス内にこの値（sigma）を渡さなくてすむようになります。\n",
    "\n",
    "\n",
    "これまで扱ってきた初期化方法はSimpleInitializerクラスと名付けることにします。\n",
    "\n",
    "\n",
    "《雛形》\n",
    "\n",
    "    class SimpleInitializer:\n",
    "        \"\"\"\n",
    "        ガウス分布によるシンプルな初期化\n",
    "        Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        pass\n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        pass\n",
    "        return B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題3】最適化手法のクラス化\n",
    "\n",
    "最適化手法のクラス化を行なってください。\n",
    "\n",
    "\n",
    "最適化手法に関しても初期化方法同様に全結合層にインスタンスとして渡します。バックワードのときにself.optimizer.update(self)のように更新できるようにします。以下の雛形に必要なコードを書き加えていってください。\n",
    "\n",
    "\n",
    "これまで扱ってきた最適化手法はSGDクラス（Stochastic Gradient Descent、確率的勾配降下法）として作成します。\n",
    "\n",
    "\n",
    "雛形\n",
    "\n",
    "    class SGD:\n",
    "        \"\"\"\n",
    "        確率的勾配降下法\n",
    "        Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # dW = 重みパラメータの微分\n",
    "        # db = バイアスパラメータの微分\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.b -= self.lr * layer.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題4】活性化関数のクラス化\n",
    "\n",
    "活性化関数のクラス化を行なってください。\n",
    "\n",
    "\n",
    "ソフトマックス関数のバックプロパゲーションには交差エントロピー誤差の計算も含む実装を行うことで計算が簡略化されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##発展的要素\n",
    "\n",
    "活性化関数や重みの初期値、最適化手法に関してこれまで見てきた以外のものを実装していきます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "code_folding": [
     0,
     16,
     32,
     50
    ]
   },
   "outputs": [],
   "source": [
    "class Tangent():\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = np.tanh(X)\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dX = dout*(1 - self.out**2)\n",
    "        \n",
    "        return dX\n",
    "\n",
    "    \n",
    "class Sigmoid():\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = 1 / (1 + np.exp(-X))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dX = dout*(1.0 - self.out) * self.out\n",
    "        \n",
    "        return dX\n",
    "\n",
    "    \n",
    "class Relu():\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.mask = (X <= 0)\n",
    "        out = X.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "    \n",
    "    \n",
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "    \n",
    "    def forward(self, X, t):\n",
    "        self.t = t\n",
    "        self.y = self.softmax(X)\n",
    "        self.loss = self.cross_entropy_error(self.y, self.t)\n",
    "        return self.loss \n",
    "    \n",
    "    def cross_entropy_error(self, y, t):\n",
    "        if t.size == y.size:\n",
    "            t = t.argmax(axis=1)\n",
    "            \n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "\n",
    "    def softmax(self, X):\n",
    "        X = X - np.max(X, axis=-1, keepdims=True)\n",
    "        y = np.exp(X) / np.sum(np.exp(X), axis=-1, keepdims=True)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dX = (self.y - self.t) / batch_size\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題5】ReLUクラスの作成\n",
    "\n",
    "現在一般的に使われている活性化関数であるReLU（Rectified Linear Unit）をReLUクラスとして実装してください。\n",
    "\n",
    "\n",
    "ReLUは以下の数式です。\n",
    "\n",
    "$$f(x) = ReLU(x) = \\begin{cases}\n",
    "x  & \\text{if $x>0$,}\\\\\n",
    "0 & \\text{if $x\\leqq0$.}\n",
    "\\end{cases}$$\n",
    "\n",
    "$\n",
    "x\n",
    " $ : ある特徴量。スカラー\n",
    "\n",
    "\n",
    "実装上はnp.maximumを使い配列に対してまとめて計算が可能です。\n",
    "\n",
    "\n",
    "numpy.maximum — NumPy v1.15 Manual\n",
    "\n",
    "\n",
    "一方、バックプロパゲーションのための $\n",
    "x\n",
    "$ に関する $\n",
    "f\n",
    "(\n",
    "x\n",
    ")\n",
    " $の微分は以下のようになります。\n",
    " \n",
    " $$\\frac{\\partial f(x)}{\\partial x} = \\begin{cases}\n",
    "1  & \\text{if $x>0$,}\\\\\n",
    "0 & \\text{if $x\\leqq0$.}\n",
    "\\end{cases}$$\n",
    "\n",
    "数学的には微分可能ではないですが、 $\n",
    "x\n",
    "=$\n",
    "0\n",
    " のとき \n",
    "0\n",
    " とすることで対応しています。\n",
    "\n",
    "\n",
    "フォワード時の $\n",
    "x\n",
    "$ の正負により、勾配を逆伝播するかどうかが決まるということになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __init__(self):\n",
    "        self.re = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.re = (X <= 0) # Xが0以下True, その他False\n",
    "        out = X.copy() # X配列コピー\n",
    "        out[self.re] = 0 # Trueを0に\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.re] = 0 # X <= 0がtrueはdoutも0で流す。それ以外はそのまま\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 3.]])"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = Relu()\n",
    "X = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(X)\n",
    "r.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 3.]])"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.backward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題6】重みの初期値\n",
    "\n",
    "ここまでは重みやバイアスの初期値は単純にガウス分布で、標準偏差をハイパーパラメータとして扱ってきました。しかし、どのような値にすると良いかが知られています。**シグモイド関数やハイパボリックタンジェント関数のときは Xavierの初期値 （またはGlorotの初期値）、ReLUのときは Heの初期値**が使われます。\n",
    "\n",
    "\n",
    "XavierInitializerクラスと、HeInitializerクラスを作成してください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Xavierの初期値\n",
    "\n",
    "Xavierの初期値における標準偏差 $\n",
    "σ\n",
    " $は次の式で求められます。\n",
    "\n",
    "$$\\sigma = \\frac{1}{\\sqrt{n}}$$\n",
    "\n",
    "$\n",
    "n\n",
    "$ : 前の層のノード数\n",
    "\n",
    "\n",
    "《論文》\n",
    "\n",
    "\n",
    "[Glorot, X., & Bengio, Y. (n.d.). Understanding the difficulty of training deep feedforward neural networks.](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02545473 -0.05222828 -0.05924382 ...  0.05443918 -0.03569927\n",
      "  -0.08462216]\n",
      " [ 0.0163612   0.10750198  0.09432884 ... -0.04194449  0.0650574\n",
      "   0.08135153]\n",
      " [-0.19610319  0.05757095  0.01815883 ...  0.01689613  0.11391467\n",
      "   0.03102198]\n",
      " ...\n",
      " [-0.08915606  0.00340409 -0.03324871 ...  0.06041643 -0.07339099\n",
      "  -0.04247029]\n",
      " [ 0.10694668 -0.08118332  0.10670233 ... -0.01727104  0.10587765\n",
      "  -0.08071005]\n",
      " [ 0.05163821 -0.13084516 -0.08043983 ... -0.08180163  0.00622774\n",
      "   0.04760624]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "n_nodes1 = 100\n",
    "n_nodes2 = 50\n",
    "\n",
    "W = np.random.randn(n_nodes1, n_nodes1) * np.sqrt(1.0 / n_nodes1)\n",
    "B = np.zeros(n_nodes2)\n",
    "\n",
    "print(W)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Xa:\n",
    "    def __init__(self):\n",
    "        self.sigma = None\n",
    "    # 重みXavierの初期値\n",
    "    def W(self,n_nodes1, n_nodes2):\n",
    "        self.sigma = np.sqrt(1.0 /n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    # バイアスXavierの初期値\n",
    "    def B(self,n_nodes2):\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.07595512 -0.03773669 -0.06574965 ...  0.12712779  0.03825179\n",
      "   0.07517853]\n",
      " [-0.23610904  0.1757041  -0.04954988 ...  0.07222684  0.08624101\n",
      "  -0.03895452]\n",
      " [-0.05515574  0.16588395  0.13044192 ... -0.0472475   0.04422224\n",
      "   0.06078262]\n",
      " ...\n",
      " [-0.17066118 -0.01252444  0.09609024 ...  0.01219337  0.01020999\n",
      "  -0.09391865]\n",
      " [ 0.08884957 -0.14953992  0.20709231 ...  0.01993056 -0.02562784\n",
      "  -0.03058842]\n",
      " [-0.13063644 -0.08104448  0.05122479 ...  0.10737502 -0.00518379\n",
      "   0.06052356]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_nodes1 = 100\n",
    "n_nodes2 = 50\n",
    "x = Xavier()\n",
    "print(x.W(n_nodes1,n_nodes2))\n",
    "x.B(n_nodes2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Heの初期値\n",
    "\n",
    "Heの初期値における標準偏差 $\n",
    "σ\n",
    "$ は次の式で求められます。\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{2}{n}}$$\n",
    "\n",
    "$\n",
    "n\n",
    "$ : 前の層のノード数\n",
    "\n",
    "\n",
    "《論文》\n",
    "\n",
    "\n",
    "[He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.](https://arxiv.org/pdf/1502.01852.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1137009 , -0.05312567, -0.04245601, ..., -0.08429376,\n",
       "        -0.08493217,  0.21158113],\n",
       "       [-0.03571319,  0.08022964, -0.01614997, ...,  0.04313805,\n",
       "        -0.13537523, -0.03444894],\n",
       "       [ 0.04626939,  0.16924931, -0.01502614, ..., -0.02038165,\n",
       "        -0.15700654,  0.06888967],\n",
       "       ...,\n",
       "       [-0.26096115, -0.14325132,  0.16042525, ..., -0.00221963,\n",
       "        -0.17699754,  0.06666529],\n",
       "       [ 0.08330453, -0.20195569, -0.02559002, ...,  0.14721267,\n",
       "         0.1156399 ,  0.09380904],\n",
       "       [-0.10081599, -0.10709278,  0.11828361, ..., -0.02549647,\n",
       "        -0.17547366, -0.0115739 ]])"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_nodes1 = 100\n",
    "n_nodes2 = 50\n",
    "\n",
    "\n",
    "W = np.random.randn(n_nodes1, n_nodes1) * np.sqrt(2.0 / n_nodes1)\n",
    "\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class He:\n",
    "    def __init__(self):\n",
    "        self.sigma = None\n",
    "    \n",
    "    def W(self,n_nodes1, n_nodes2):\n",
    "        self.sigma = np.sqrt(2.0 / n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self,n_nodes2):\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.16945174  0.25662846 -0.01315376 ... -0.2452924   0.06994103\n",
      "   0.0981642 ]\n",
      " [ 0.15795478  0.11723232  0.03405146 ...  0.01810344 -0.09997677\n",
      "  -0.07646797]\n",
      " [-0.02771997  0.18556229 -0.12988063 ... -0.0016957   0.16040818\n",
      "  -0.06728946]\n",
      " ...\n",
      " [ 0.19604049  0.14806505 -0.07867793 ...  0.25480248  0.29266877\n",
      "   0.06650376]\n",
      " [ 0.00916931  0.06298648 -0.29070344 ...  0.06301895  0.07153287\n",
      "   0.03745853]\n",
      " [-0.10338742 -0.11624144  0.00871033 ... -0.11570483  0.20120921\n",
      "   0.12616958]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_nodes1 = 100\n",
    "n_nodes2 = 50\n",
    "h = He()\n",
    "print(h.W(n_nodes1,n_nodes2))\n",
    "h.B(n_nodes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class XaHe:\n",
    "    def __init__(self,weight = \"relu\"):\n",
    "        self.sigma = None\n",
    "        \n",
    "    def W(self,n_nodes1, n_nodes2, weight=\"relu\"):\n",
    "        if str(weight).lower() in (\"relu\",\"he\"):\n",
    "            self.sigma = np.sqrt(2.0 / n_nodes1)\n",
    "        else:\n",
    "            self.sigma = np.sqrt(1.0 / n_nodes1)\n",
    "        return self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "    \n",
    "    def B(self,n_nodes2):\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11812177, -0.06118408,  0.31981648, ..., -0.19414065,\n",
       "         0.00147421,  0.11787149],\n",
       "       [ 0.16648452, -0.0646948 ,  0.13095929, ..., -0.02291938,\n",
       "        -0.06520607, -0.10504736],\n",
       "       [ 0.26578159, -0.11082851,  0.08272409, ..., -0.12499598,\n",
       "        -0.20683998, -0.17650477],\n",
       "       ...,\n",
       "       [ 0.06293136, -0.11198514, -0.18561754, ...,  0.01920316,\n",
       "         0.0162983 ,  0.04304205],\n",
       "       [ 0.13762489,  0.09148703, -0.15722044, ..., -0.14090796,\n",
       "        -0.39920156, -0.27217435],\n",
       "       [-0.32030991, -0.33134014,  0.1845845 , ..., -0.07180986,\n",
       "        -0.0626317 ,  0.07653759]])"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_nodes1 = 100\n",
    "n_nodes2 = 50\n",
    "\n",
    "xh = XaHe()\n",
    "xh.W(n_nodes1,n_nodes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03410818,  0.05407139,  0.10603512, ..., -0.05602699,\n",
       "        -0.11899521,  0.16384215],\n",
       "       [ 0.24422587,  0.05067346, -0.00768257, ...,  0.02416897,\n",
       "        -0.02647679,  0.01644473],\n",
       "       [ 0.0995582 , -0.11971915,  0.09326708, ..., -0.02169568,\n",
       "         0.08804552,  0.38706065],\n",
       "       ...,\n",
       "       [ 0.09426679, -0.05874599,  0.07908557, ...,  0.05724648,\n",
       "        -0.02127589,  0.07762253],\n",
       "       [-0.10160777,  0.0552524 ,  0.01569049, ..., -0.09852118,\n",
       "        -0.05262311, -0.23273073],\n",
       "       [-0.05226161,  0.08035758, -0.07698241, ...,  0.00106318,\n",
       "         0.10423705, -0.13227817]])"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_nodes1 = 100\n",
    "n_nodes2 = 50\n",
    "\n",
    "xh = XaHe(weight = \"sig\")\n",
    "xh.W(n_nodes1,n_nodes2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題7】最適化手法\n",
    "\n",
    "学習率は学習過程で変化させていく方法が一般的です。基本的な手法である AdaGrad のクラスを作成してください。\n",
    "\n",
    "\n",
    "まず、これまで使ってきたSGDを確認します。\n",
    "\n",
    "$$W_i^{\\prime} = W_i - \\alpha E(\\frac{\\partial L}{\\partial W_i}) \\\\\n",
    "B_i^{\\prime} = B_i - \\alpha E(\\frac{\\partial L}{\\partial B_i})$$\n",
    "\n",
    "$\n",
    "α\n",
    " $: 学習率（層ごとに変えることも可能だが、基本的には全て同じとする）\n",
    " \n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_i}$  :$\n",
    "W\n",
    "i\n",
    "$ に関する損失 $\n",
    "L\n",
    "$ の勾配\n",
    "\n",
    "$\\frac{\\partial L}{\\partial B_i}$\n",
    " : $\n",
    "B\n",
    "i\n",
    "$ に関する損失 $\n",
    "L\n",
    " $の勾配\n",
    "\n",
    "\n",
    "\n",
    "$\n",
    "E\n",
    "(\n",
    ")\n",
    "$ : ミニバッチ方向にベクトルの平均を計算\n",
    "\n",
    "\n",
    "続いて、AdaGradです。バイアスの数式は省略しますが、重みと同様のことをします。\n",
    "\n",
    "\n",
    "更新された分だけその重みに対する学習率を徐々に下げていきます。イテレーションごとの勾配の二乗和 \n",
    "H\n",
    " を保存しておき、その分だけ学習率を小さくします。\n",
    "\n",
    "\n",
    "学習率は重み一つひとつに対して異なることになります。\n",
    "\n",
    "\n",
    "$$ H_i^{\\prime}  = H_i+E(\\frac{\\partial L}{\\partial W_i})×E(\\frac{\\partial L}{\\partial W_i})\\\\\n",
    "W_i^{\\prime} = W_i - \\alpha \\frac{1}{\\sqrt{H_i^{\\prime} }} E(\\frac{\\partial L}{\\partial W_i}) \\\\ $$\n",
    "\n",
    "$\n",
    "H\n",
    "i\n",
    " $: i層目に関して、前のイテレーションまでの勾配の二乗和（初期値は0）\n",
    "\n",
    "$\n",
    "H\n",
    "′\n",
    "i\n",
    "$ : 更新した $\n",
    "H\n",
    "i\n",
    "$\n",
    "\n",
    "\n",
    "《論文》\n",
    "[Duchi JDUCHI, J., & Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization * Elad Hazan. Journal of Machine Learning Research (Vol. 12).](https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, layer):\n",
    "        if (self.h_W is None) and (self.h_b is None):\n",
    "            self.h_W = 0\n",
    "            self.h_b = 0\n",
    "        \n",
    "        self.h_W += (layer.dW ** 2).sum()\n",
    "        self.h_b += (layer.db ** 2).sum()\n",
    "        layer.W -= self.lr * layer.dW / (np.sqrt(self.h_W) + 1e-7)\n",
    "        layer.b -= self.lr * layer.db / (np.sqrt(self.h_b) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max()) # 1.0\n",
    "print(X_train.min()) # 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "print(y_train.shape) # (60000,)\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) # float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(12000, 784)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "print(X_train.shape) # (48000, 784)\n",
    "print(X_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_val = y_val.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題8】クラスの完成\n",
    "\n",
    "任意の構成で学習と推定が行えるScratchDeepNeuralNetrowkClassifierクラスを完成させてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # W,bを初期化\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.b = initializer.B(n_nodes2)\n",
    "        self.X = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        out : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        self.X = X\n",
    "        out = np.dot(X, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        batch_size = dA.shape[0]\n",
    "        dX = np.dot(dA, self.W.T)\n",
    "        self.dW = np.dot(self.X.T, dA) #/ batch_size\n",
    "        self.db = np.sum(dA, axis=0) #/ batch_size\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "code_folding": [
     0,
     14,
     18
    ]
   },
   "outputs": [],
   "source": [
    "class Xa:\n",
    "    def __init__(self):\n",
    "        self.sigma = None\n",
    "    \n",
    "    def W(self,n_nodes1, n_nodes2):\n",
    "        self.sigma = np.sqrt(1.0 /n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self,n_nodes2):\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B\n",
    "    \n",
    "    \n",
    "class He:\n",
    "    def __init__(self):\n",
    "        self.sigma = None\n",
    "    \n",
    "    def W(self,n_nodes1, n_nodes2):\n",
    "        self.sigma = np.sqrt(2.0 / n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self,n_nodes2):\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "code_folding": [
     0,
     16,
     32,
     50
    ]
   },
   "outputs": [],
   "source": [
    "class Tangent():\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = np.tanh(X)\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dX = dout*(1 - self.out**2)\n",
    "        \n",
    "        return dX\n",
    "\n",
    "    \n",
    "class Sigmoid():\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = 1 / (1 + np.exp(-X))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dX = dout*(1.0 - self.out) * self.out\n",
    "        \n",
    "        return dX\n",
    "\n",
    "    \n",
    "class Relu():\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.mask = (X <= 0)\n",
    "        out = X.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "    \n",
    "    \n",
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "    \n",
    "    def forward(self, X, t):\n",
    "        self.t = t\n",
    "        self.y = self.softmax(X)\n",
    "        self.loss = self.cross_entropy_error(self.y, self.t)\n",
    "        return self.loss \n",
    "    \n",
    "    def cross_entropy_error(self, y, t):     \n",
    "        batch_size = y.shape[0]\n",
    "\n",
    "        return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "\n",
    "    def softmax(self, X):\n",
    "        X = X - np.max(X, axis=-1, keepdims=True)\n",
    "        y = np.exp(X) / np.sum(np.exp(X), axis=-1, keepdims=True)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dX = (self.y - self.t) / batch_size\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.b -= self.lr * layer.db\n",
    "        \n",
    "        \n",
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h_W = None\n",
    "        self.h_b = None\n",
    "        \n",
    "    def update(self, layer):\n",
    "        if (self.h_W is None) and (self.h_b is None):\n",
    "            self.h_W = 0\n",
    "            self.h_b = 0\n",
    "        \n",
    "        self.h_W += (layer.dW ** 2).sum()\n",
    "        self.h_b += (layer.db ** 2).sum()\n",
    "        layer.W -= self.lr * layer.dW / (np.sqrt(self.h_W) + 1e-7)\n",
    "        layer.b -= self.lr * layer.db / (np.sqrt(self.h_b) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowk(object):\n",
    "    \"\"\"\n",
    "    全結合による多層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr, verbose=True, batch_size=20, max_iter=3):\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        \n",
    "        # レイヤの生成\n",
    "        initializer = He()\n",
    "        optimizer = SGD(lr=lr)\n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        self.layers[\"FC1\"] = FC(784, 100, initializer, optimizer)\n",
    "        \n",
    "        self.layers[\"ReLU\"] = Relu()\n",
    "        \n",
    "        self.layers[\"FC2\"] = FC(100, 50, initializer, optimizer)\n",
    "        \n",
    "        self.layers[\"Relu\"] = Relu()\n",
    "        \n",
    "        self.layers[\"FC3\"] = FC(50, 10, initializer, optimizer)\n",
    "        \n",
    "        self.lastLayer = Softmax()\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        # 1エポック当たりの繰り返し回数\n",
    "        iter_per_epoch = 1000\n",
    "        # エポック複数繰り返す\n",
    "        for _  in range(self.max_iter):\n",
    "            \n",
    "            # 1エポック\n",
    "            for i in range(iter_per_epoch):\n",
    "                #損失計算用のリスト\n",
    "                s_list = []\n",
    "                \n",
    "                batch_mask = np.random.choice(X.shape[0], self.batch_size)\n",
    "                X_batch = X[batch_mask]\n",
    "                y_batch = y[batch_mask]\n",
    "                \n",
    "                #勾配\n",
    "                self._gradient(X_batch, y_batch)\n",
    "                    \n",
    "                loss = self._loss(X_batch, y_batch)\n",
    "                s_list.append(loss)\n",
    "        \n",
    "                if self.verbose:\n",
    "                    #verboseをTrueにした際は学習過程などを出力する\n",
    "                    print(loss)\n",
    "                        \n",
    "            self.train_loss.append(loss)\n",
    "            if (X_val is not None) and (y_val is not None):\n",
    "                loss_test = self._loss(X_val, y_val)\n",
    "                self.test_loss.append(loss_test)\n",
    "                    \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        推論\n",
    "        \n",
    "        return 画像データ（入力データ）\n",
    "        \"\"\"\n",
    "        for layer in self.layers.values():\n",
    "            X = layer.forward(X)\n",
    "            pred = np.argmax(X, axis=1)\n",
    "        return pred\n",
    "    \n",
    "    \n",
    "    def _loss(self, X, t):\n",
    "#         y = self.predict(X)\n",
    "#         return self.lastLayer.forward(y, t)\n",
    "\n",
    "        for layer in self.layers.values():\n",
    "            X = layer.forward(X)\n",
    "            \n",
    "        return self.lastLayer.forward(X, t)\n",
    "\n",
    "        \n",
    "    def _gradient(self, X, t):\n",
    "        \"\"\"\n",
    "        重みパラメータに対する勾配を誤差逆伝播法によって求める\n",
    "        \n",
    "        X : 画像データ（入力データ)\n",
    "        t : 教師データ\n",
    "        \n",
    "        \"\"\"\n",
    "        self._loss(X, t)\n",
    "        \n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題9】学習と推定\n",
    "\n",
    "層の数や活性化関数を変えたいくつかのネットワークを作成してください。そして、MNISTのデータを学習・推定し、Accuracyを計算してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(48000, 10)\n",
      "(12000, 784)\n",
      "(12000, 10)\n"
     ]
    }
   ],
   "source": [
    "sdn = ScratchDeepNeuralNetrowk(lr=0.1, verbose=False, batch_size=20, max_iter=5)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdn.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正答率:0.9631666666666666\n"
     ]
    }
   ],
   "source": [
    "y_val = np.argmax(y_val, axis=1)\n",
    "y_pred = sdn.predict(X_val)\n",
    "accuracy = accuracy_score(y_pred, y_val)\n",
    "print(\"正答率:{}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1, len(sdn.train_loss)+1)\n",
    "plt.plot(x, sdn.train_loss, label=\"loss\")\n",
    "plt.plot(x, sdn.test_loss, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なんだこのグラフ？\n",
    "\n",
    "どうなっているんだ？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "層を増やしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowk(object):\n",
    "    \"\"\"\n",
    "    全結合による多層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr, verbose=True, batch_size=20, max_iter=3):\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        \n",
    "        # レイヤの生成\n",
    "        initializer = He()\n",
    "        # AdaGrad使用\n",
    "        optimizer = AdaGrad(lr=lr)\n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        self.layers[\"FC1\"] = FC(784, 600, initializer, optimizer)\n",
    "        \n",
    "        self.layers[\"ReLU1\"] = Relu()\n",
    "        \n",
    "        self.layers[\"FC2\"] = FC(600, 500, initializer, optimizer)\n",
    "        \n",
    "        self.layers[\"Relu2\"] = Relu()\n",
    "        \n",
    "        self.layers[\"FC3\"] = FC(500, 400, initializer, optimizer)\n",
    "        \n",
    "        self.layers[\"Relu3\"] = Relu()\n",
    "        \n",
    "        self.layers[\"FC4\"] = FC(400, 300, initializer, optimizer)\n",
    "        \n",
    "        self.layers[\"tanh1\"] = Tangent()\n",
    "        \n",
    "        self.layers[\"FC5\"] = FC(300, 200, initializer, optimizer)\n",
    "        \n",
    "        self.layers[\"tanh2\"] = Tangent()\n",
    "        \n",
    "        self.layers[\"FC6\"] = FC(200, 100, initializer, optimizer)\n",
    "        \n",
    "        self.layers[\"tanh3\"] = Tangent()\n",
    "        \n",
    "        self.layers[\"FC7\"] = FC(100, 50, initializer, optimizer)\n",
    "        \n",
    "        self.layers[\"tanh4\"] = Tangent()\n",
    "        \n",
    "        self.layers[\"FC8\"] = FC(50, 10, initializer, optimizer)\n",
    "        \n",
    "        self.lastLayer = Softmax()\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        # 1エポック当たりの繰り返し回数\n",
    "        iter_per_epoch = 1000\n",
    "        # エポック複数繰り返す\n",
    "        for _  in range(self.max_iter):\n",
    "            \n",
    "            # 1エポック\n",
    "            for i in range(iter_per_epoch):\n",
    "                #損失計算用のリスト\n",
    "                s_list = []\n",
    "                \n",
    "                batch_mask = np.random.choice(X.shape[0], self.batch_size)\n",
    "                X_batch = X[batch_mask]\n",
    "                y_batch = y[batch_mask]\n",
    "                \n",
    "                #勾配\n",
    "                self._gradient(X_batch, y_batch)\n",
    "                    \n",
    "                loss = self._loss(X_batch, y_batch)\n",
    "                s_list.append(loss)\n",
    "        \n",
    "                if self.verbose:\n",
    "                    #verboseをTrueにした際は学習過程などを出力する\n",
    "                    print(loss)\n",
    "                        \n",
    "            self.train_loss.append(sum(s_list) / len(s_list))\n",
    "            if (X_val is not None) and (y_val is not None):\n",
    "                loss_test = self._loss(X_val, y_val)\n",
    "                self.test_loss.append(loss_test)\n",
    "                    \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        推論\n",
    "        \n",
    "        return 画像データ（入力データ）\n",
    "        \"\"\"\n",
    "        for layer in self.layers.values():\n",
    "            X = layer.forward(X)\n",
    "            pred = np.argmax(X, axis=1)\n",
    "        return pred\n",
    "    \n",
    "    \n",
    "    def _loss(self, X, t):\n",
    "#         y = self.predict(X)\n",
    "#         return self.lastLayer.forward(y, t)\n",
    "\n",
    "        for layer in self.layers.values():\n",
    "            X = layer.forward(X)\n",
    "        return self.lastLayer.forward(X, t)\n",
    "\n",
    "        \n",
    "    def _gradient(self, X, t):\n",
    "        \"\"\"\n",
    "        重みパラメータに対する勾配を誤差逆伝播法によって求める\n",
    "        \n",
    "        X : 画像データ（入力データ)\n",
    "        t : 教師データ\n",
    "        \n",
    "        \"\"\"\n",
    "        self._loss(X, t)\n",
    "        \n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max()) # 1.0\n",
    "print(X_train.min()) # 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "print(y_train.shape) # (60000,)\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) # float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(12000, 784)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "print(X_train.shape) # (48000, 784)\n",
    "print(X_val.shape) # (12000, 784)\n",
    "y_train = y_train.astype(int)\n",
    "y_val = y_val.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(48000, 10)\n",
      "(12000, 784)\n",
      "(12000, 10)\n"
     ]
    }
   ],
   "source": [
    "sdn2 = ScratchDeepNeuralNetrowk(lr=0.1, verbose=False, batch_size=20, max_iter=5)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdn2.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正答率:0.89825\n"
     ]
    }
   ],
   "source": [
    "y_val = np.argmax(y_val, axis=1)\n",
    "y_pred = sdn2.predict(X_val)\n",
    "accuracy = accuracy_score(y_pred, y_val)\n",
    "print(\"正答率:{}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1, len(sdn2.train_loss)+1)\n",
    "plt.plot(x, sdn2.train_loss, label=\"loss\")\n",
    "plt.plot(x, sdn2.test_loss, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "層の数や大きさを上げたらよいというわけではなさそう\n",
    "\n",
    "このグラフはなんでだ？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "課題\n",
    "\n",
    "ゼロから作るDeep Learning参考に作成\n",
    "コードをしっかりと理解しないと\n",
    "\n",
    "正解率は9割出ているのでよし\n",
    "\n",
    "ロスの曲線がおかしいのでコードを修正しないと"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
