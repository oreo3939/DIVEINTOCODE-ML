{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ディープニューラルネットワークスクラッチ\n",
    "\n",
    "前回は3層のニューラルネットワークを作成しましたが、今回はこれを任意の層数に拡張しやすいものに書き換えていきます。その上で、活性化関数や初期値、最適化手法について発展的なものを扱えるようにしていきます。\n",
    "\n",
    "\n",
    "このようなスクラッチを行うことで、今後各種フレームワークを利用していくにあたり、内部の動きが想像できることを目指します。\n",
    "\n",
    "\n",
    "名前は新しくScratchDeepNeuralNetrowkClassifierクラスとしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##層などのクラス化\n",
    "\n",
    "クラスにまとめて行くことで、構成を変更しやすい実装にしていきます。\n",
    "\n",
    "\n",
    "手を加える箇所\n",
    "\n",
    "\n",
    "・層の数\n",
    "\n",
    "・層の種類（今後畳み込み層など他のタイプの層が登場する）\n",
    "\n",
    "・活性化関数の種類\n",
    "\n",
    "・重みやバイアスの初期化方法\n",
    "\n",
    "・最適化手法\n",
    "\n",
    "そのために、全結合層、各種活性化関数、重みやバイアスの初期化、最適化手法それぞれのクラスを作成します。\n",
    "\n",
    "\n",
    "**実装方法は自由**ですが、簡単な例を紹介します。サンプルコード1のように全結合層と活性化関数のインスタンスを作成し、サンプルコード2,3のようにして使用します。それぞれのクラスについてはこのあと解説します。\n",
    "\n",
    "《サンプルコード1》\n",
    "\n",
    "\n",
    "ScratchDeepNeuralNetrowkClassifierのfitメソッド内\n",
    "\n",
    "    # self.sigma : ガウス分布の標準偏差\n",
    "    # self.lr : 学習率\n",
    "    # self.n_nodes1 : 1層目のノード数\n",
    "    # self.n_nodes2 : 2層目のノード数\n",
    "    # self.n_output : 出力層のノード数\n",
    "    optimizer = SGD(self.lr)\n",
    "    self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "    self.activation1 = Tanh()\n",
    "    self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "    self.activation2 = Tanh()\n",
    "    self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "    self.activation3 = Softmax()\n",
    "    \n",
    " \n",
    "《サンプルコード2》\n",
    "\n",
    "\n",
    "イテレーションごとのフォワード\n",
    "\n",
    "    A1 = self.FC1.forward(X)\n",
    "    Z1 = self.activation1.forward(A1)\n",
    "    A2 = self.FC2.forward(Z1)\n",
    "    Z2 = self.activation2.forward(A2)\n",
    "    A3 = self.FC3.forward(Z2)\n",
    "    Z3 = self.activation3.forward(A3)\n",
    "    \n",
    "    \n",
    " 《サンプルコード3》\n",
    "\n",
    "\n",
    "イテレーションごとのバックワード\n",
    "\n",
    "    dA3 = self.activation3.backward(Z3, Y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "    dZ2 = self.FC3.backward(dA3)\n",
    "    dA2 = self.activation2.backward(dZ2)\n",
    "    dZ1 = self.FC2.backward(dA2)\n",
    "    dA1 = self.activation1.backward(dZ1)\n",
    "    dZ0 = self.FC1.backward(dA1) # dZ0は使用しない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題1】全結合層のクラス化\n",
    "\n",
    "全結合層のクラス化を行なってください。\n",
    "\n",
    "\n",
    "以下に雛形を載せました。コンストラクタで重みやバイアスの初期化をして、あとはフォワードとバックワードのメソッドを用意します。重みW、バイアスB、およびフォワード時の入力Xをインスタンス変数として保持しておくことで、煩雑な入出力は不要になります。\n",
    "\n",
    "\n",
    "なお、インスタンスも引数として渡すことができます。そのため、初期化方法のインスタンスinitializerをコンストラクタで受け取れば、それにより初期化が行われます。渡すインスタンスを変えれば、初期化方法が変えられます。\n",
    "\n",
    "\n",
    "また、引数として自身のインスタンスselfを渡すこともできます。これを利用してself.optimizer.update(self)という風に層の重みの更新が可能です。更新に必要な値は複数ありますが、全て全結合層が持つインスタンス変数にすることができます。\n",
    "\n",
    "\n",
    "初期化方法と最適化手法のクラスについては後述します。\n",
    "\n",
    "\n",
    "《雛形》\n",
    "\n",
    "    class FC:\n",
    "        \"\"\"\n",
    "        ノード数n_nodes1からn_nodes2への全結合層\n",
    "        Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        pass\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        pass\n",
    "        return A\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        pass\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題2】初期化方法のクラス化\n",
    "\n",
    "初期化を行うコードをクラス化してください。\n",
    "\n",
    "\n",
    "前述のように、全結合層のコンストラクタに初期化方法のインスタンスを渡せるようにします。以下の雛形に必要なコードを書き加えていってください。標準偏差の値（sigma）はコンストラクタで受け取るようにすることで、全結合層のクラス内にこの値（sigma）を渡さなくてすむようになります。\n",
    "\n",
    "\n",
    "これまで扱ってきた初期化方法はSimpleInitializerクラスと名付けることにします。\n",
    "\n",
    "\n",
    "《雛形》\n",
    "\n",
    "    class SimpleInitializer:\n",
    "        \"\"\"\n",
    "        ガウス分布によるシンプルな初期化\n",
    "        Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        pass\n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        pass\n",
    "        return B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題3】最適化手法のクラス化\n",
    "\n",
    "最適化手法のクラス化を行なってください。\n",
    "\n",
    "\n",
    "最適化手法に関しても初期化方法同様に全結合層にインスタンスとして渡します。バックワードのときにself.optimizer.update(self)のように更新できるようにします。以下の雛形に必要なコードを書き加えていってください。\n",
    "\n",
    "\n",
    "これまで扱ってきた最適化手法はSGDクラス（Stochastic Gradient Descent、確率的勾配降下法）として作成します。\n",
    "\n",
    "\n",
    "雛形\n",
    "\n",
    "    class SGD:\n",
    "        \"\"\"\n",
    "        確率的勾配降下法\n",
    "        Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "\n",
    "        \"\"\"\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題4】活性化関数のクラス化\n",
    "\n",
    "活性化関数のクラス化を行なってください。\n",
    "\n",
    "\n",
    "ソフトマックス関数のバックプロパゲーションには交差エントロピー誤差の計算も含む実装を行うことで計算が簡略化されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [
     0,
     31,
     47,
     63,
     81
    ]
   },
   "outputs": [],
   "source": [
    "class Affine():\n",
    "    def __init__(self, W, b, optimize):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.X = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        out = np.dot(X, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dX = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.X.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dX = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.X.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    \n",
    "class Tangent():\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = np.tanh(X)\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dX = dout*(1 - self.out**2)\n",
    "        \n",
    "        return dX\n",
    "\n",
    "    \n",
    "class Sigmoid():\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = 1 / (1 + np.exp(-X))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dX = dout*(1.0 - self.out) * self.out\n",
    "        \n",
    "        return dX\n",
    "\n",
    "    \n",
    "class Relu():\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.mask = (X <= 0)\n",
    "        out = X.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "    \n",
    "    \n",
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "    \n",
    "    def forward(self, X, t):\n",
    "        self.t = t\n",
    "        self.y = self.softmax(X)\n",
    "        self.loss = self.cross_entropy_error(self.y, self.t)\n",
    "        return self.loss \n",
    "    \n",
    "    def cross_entropy_error(self, y, t):\n",
    "        if t.size == y.size:\n",
    "            t = t.argmax(axis=1)\n",
    "            \n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "\n",
    "    def softmax(self, X):\n",
    "        X = X - np.max(X, axis=-1, keepdims=True)\n",
    "        y = np.exp(X) / np.sum(np.exp(X), axis=-1, keepdims=True)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dX = (self.y - self.t) / batch_size\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##発展的要素\n",
    "\n",
    "活性化関数や重みの初期値、最適化手法に関してこれまで見てきた以外のものを実装していきます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題5】ReLUクラスの作成\n",
    "\n",
    "現在一般的に使われている活性化関数であるReLU（Rectified Linear Unit）をReLUクラスとして実装してください。\n",
    "\n",
    "\n",
    "ReLUは以下の数式です。\n",
    "\n",
    "$$f(x) = ReLU(x) = \\begin{cases}\n",
    "x  & \\text{if $x>0$,}\\\\\n",
    "0 & \\text{if $x\\leqq0$.}\n",
    "\\end{cases}$$\n",
    "\n",
    "$\n",
    "x\n",
    " $ : ある特徴量。スカラー\n",
    "\n",
    "\n",
    "実装上はnp.maximumを使い配列に対してまとめて計算が可能です。\n",
    "\n",
    "\n",
    "numpy.maximum — NumPy v1.15 Manual\n",
    "\n",
    "\n",
    "一方、バックプロパゲーションのための $\n",
    "x\n",
    "$ に関する $\n",
    "f\n",
    "(\n",
    "x\n",
    ")\n",
    " $の微分は以下のようになります。\n",
    " \n",
    " $$\\frac{\\partial f(x)}{\\partial x} = \\begin{cases}\n",
    "1  & \\text{if $x>0$,}\\\\\n",
    "0 & \\text{if $x\\leqq0$.}\n",
    "\\end{cases}$$\n",
    "\n",
    "数学的には微分可能ではないですが、 $\n",
    "x\n",
    "=$\n",
    "0\n",
    " のとき \n",
    "0\n",
    " とすることで対応しています。\n",
    "\n",
    "\n",
    "フォワード時の $\n",
    "x\n",
    "$ の正負により、勾配を逆伝播するかどうかが決まるということになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __init__(self):\n",
    "        self.re = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.re = (X <= 0) # Xが0以下True, その他False\n",
    "        out = X.copy() # X配列コピー\n",
    "        out[self.re] = 0 # Trueを0に\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.re] = 0 # x<=0がtrueはdoutも0で流す。それ以外はそのまま\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 3.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = Relu()\n",
    "X = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(X)\n",
    "r.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 3.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.backward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題6】重みの初期値\n",
    "\n",
    "ここまでは重みやバイアスの初期値は単純にガウス分布で、標準偏差をハイパーパラメータとして扱ってきました。しかし、どのような値にすると良いかが知られています。**シグモイド関数やハイパボリックタンジェント関数のときは Xavierの初期値 （またはGlorotの初期値）、ReLUのときは Heの初期値**が使われます。\n",
    "\n",
    "\n",
    "XavierInitializerクラスと、HeInitializerクラスを作成してください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Xavierの初期値\n",
    "\n",
    "Xavierの初期値における標準偏差 $\n",
    "σ\n",
    " $は次の式で求められます。\n",
    "\n",
    "$$\\sigma = \\frac{1}{\\sqrt{n}}$$\n",
    "\n",
    "$\n",
    "n\n",
    "$ : 前の層のノード数\n",
    "\n",
    "\n",
    "《論文》\n",
    "\n",
    "\n",
    "[Glorot, X., & Bengio, Y. (n.d.). Understanding the difficulty of training deep feedforward neural networks.](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00993982, -0.05662293,  0.01893552, ...,  0.11018268,\n",
       "         0.06971416, -0.01207858],\n",
       "       [-0.0095043 ,  0.07856503, -0.00718747, ...,  0.04541258,\n",
       "         0.00568269, -0.00032771],\n",
       "       [ 0.03598337,  0.02155519, -0.04547866, ..., -0.00965341,\n",
       "         0.03605077, -0.05264931],\n",
       "       ...,\n",
       "       [-0.03251694, -0.02059958, -0.08385732, ..., -0.0771515 ,\n",
       "        -0.01986004, -0.04434048],\n",
       "       [-0.07668733,  0.07537683,  0.0955091 , ...,  0.0083466 ,\n",
       "        -0.02348123,  0.0056229 ],\n",
       "       [ 0.02919418, -0.00239146,  0.00102621, ...,  0.06558561,\n",
       "         0.03374067,  0.02774844]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = 784\n",
    "n_nodes1 = 100\n",
    "W = np.random.randn(n_features, n_nodes1) * np.sqrt(1.0 / n_features)\n",
    "\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.01272515e-02, -9.89685335e-03, -7.12050071e-02,\n",
       "        -3.54996943e-03,  5.46856226e-02, -2.55265315e-01,\n",
       "        -2.35425779e-01,  3.66509234e-02,  2.30057812e-01,\n",
       "        -3.93838032e-02],\n",
       "       [ 1.49842774e-01, -7.14285307e-02,  1.02882332e-01,\n",
       "        -2.70562003e-01,  1.38468705e-01, -1.11835739e-01,\n",
       "         2.20288796e-01, -1.26899386e-01,  4.73488680e-02,\n",
       "         8.31581000e-02],\n",
       "       [ 1.95241528e-01,  1.55294059e-01, -4.54144018e-03,\n",
       "         1.01180351e-01,  1.15364333e-01,  2.09106569e-03,\n",
       "         1.66307039e-02,  5.94395818e-02,  1.00284310e-01,\n",
       "         6.39650842e-02],\n",
       "       [ 4.63717046e-03,  9.41104064e-02, -1.37077005e-01,\n",
       "        -3.47093566e-02, -1.83603288e-01,  1.49753139e-02,\n",
       "        -1.28998962e-02,  3.52269165e-02, -8.15627504e-02,\n",
       "        -7.59487256e-02],\n",
       "       [ 1.30841470e-01, -4.18304683e-02,  2.74889614e-01,\n",
       "        -5.68593193e-02, -1.10721182e-01, -2.13898920e-01,\n",
       "        -2.03756952e-02, -5.12116193e-02, -8.66701865e-02,\n",
       "         3.61356868e-01],\n",
       "       [ 1.33349167e-01,  3.52865315e-01, -4.09238605e-02,\n",
       "        -7.71254458e-03,  2.04137723e-01,  2.75416405e-02,\n",
       "         1.27201793e-02,  1.66855061e-01,  8.07143661e-02,\n",
       "         1.37688986e-01],\n",
       "       [-2.64057986e-01,  2.30480346e-02,  5.50407001e-02,\n",
       "         7.65365993e-02,  1.64694374e-01,  6.69554611e-02,\n",
       "         2.19564957e-01,  2.36562409e-01,  2.23909526e-01,\n",
       "         4.74395069e-02],\n",
       "       [ 7.81158554e-02,  5.09763930e-03,  2.03686035e-01,\n",
       "         1.77807641e-01,  1.34123066e-01, -1.22768802e-01,\n",
       "        -1.90188750e-02,  4.81595898e-02,  2.67636144e-01,\n",
       "         1.09191342e-01],\n",
       "       [-2.64658944e-01, -2.07687973e-02,  6.15842152e-02,\n",
       "        -1.66505973e-01, -1.04005301e-01,  6.94411163e-03,\n",
       "        -1.47153678e-01, -4.37620914e-03, -4.83110279e-04,\n",
       "        -2.08413840e-01],\n",
       "       [-7.81092604e-02, -9.99421944e-03, -1.90663792e-01,\n",
       "        -1.33229688e-01, -1.20724906e-01,  2.36947930e-01,\n",
       "        -4.95728670e-02,  1.20043152e-01, -5.55011891e-02,\n",
       "        -8.52782054e-02],\n",
       "       [-1.66083369e-01, -3.22020730e-02,  2.50423431e-01,\n",
       "        -2.84403278e-02,  2.31897711e-02,  9.59289132e-02,\n",
       "        -9.97545192e-02,  1.98852807e-02, -9.80951854e-03,\n",
       "        -2.62802669e-02],\n",
       "       [-1.13191133e-02, -1.09685839e-01, -4.42991829e-02,\n",
       "         7.01780007e-02, -2.10796134e-01, -8.81650042e-02,\n",
       "         2.60307627e-02, -1.52974270e-01, -4.17103909e-02,\n",
       "         1.87258780e-02],\n",
       "       [-5.81279509e-02, -2.33470510e-02,  1.43474550e-01,\n",
       "         6.34279595e-02,  1.62818649e-01, -4.02234359e-02,\n",
       "         6.63685573e-03,  6.30388641e-02, -1.26814806e-01,\n",
       "         2.28123348e-01],\n",
       "       [-1.97201668e-01, -1.56079910e-01, -2.65723752e-02,\n",
       "         1.12480939e-01,  2.40395918e-02,  1.76439291e-01,\n",
       "         2.92414781e-01, -7.73204848e-02,  1.07622506e-01,\n",
       "         2.25624288e-01],\n",
       "       [ 2.08126634e-01,  6.64298733e-02, -2.32949168e-01,\n",
       "         2.24548391e-01, -1.79882255e-01, -2.80808140e-01,\n",
       "         4.39647783e-02,  2.12670303e-01, -8.68413375e-02,\n",
       "        -4.03993679e-01],\n",
       "       [ 1.39959916e-01,  1.94286825e-01, -8.71570667e-02,\n",
       "        -1.13516586e-01,  8.16055139e-02,  1.27240611e-01,\n",
       "         1.52739750e-01, -2.12886046e-01, -3.12179936e-02,\n",
       "         1.59832582e-01],\n",
       "       [-1.32605103e-02,  2.25154849e-01,  1.61388080e-01,\n",
       "        -6.26676932e-02,  4.75117512e-03,  2.20383620e-02,\n",
       "        -1.40066195e-01,  6.43098479e-02,  2.08422594e-01,\n",
       "        -2.35253944e-01],\n",
       "       [-1.31640875e-01,  7.44022748e-02,  2.11466217e-01,\n",
       "        -1.13154670e-01, -5.54715038e-02,  2.70987047e-01,\n",
       "         1.62153423e-02, -2.39162635e-02,  1.12878531e-01,\n",
       "        -4.06620630e-02],\n",
       "       [ 1.77335762e-01, -2.52489197e-01,  6.19875026e-02,\n",
       "        -2.01889612e-01,  3.64760756e-01,  1.84349975e-01,\n",
       "         4.47387077e-02,  1.29122359e-01, -1.60234756e-02,\n",
       "         2.11129470e-01],\n",
       "       [-7.55265981e-02, -3.85260211e-02, -2.36571460e-01,\n",
       "         2.00410185e-03, -2.27283882e-01,  1.06469237e-01,\n",
       "        -1.09080541e-01,  1.19406783e-01, -3.71113257e-02,\n",
       "        -1.11469005e-01],\n",
       "       [-3.77094514e-02, -2.18476346e-01,  2.49545881e-01,\n",
       "        -2.15556368e-01,  3.30426830e-02,  1.47309746e-01,\n",
       "        -1.06326770e-01,  1.87681199e-01, -2.44735861e-01,\n",
       "         5.25968760e-02],\n",
       "       [ 6.56513476e-02,  1.02377627e-01,  1.84082173e-01,\n",
       "        -6.92561274e-02, -1.27911469e-01, -3.79521739e-02,\n",
       "        -1.48217487e-01,  5.93415627e-02,  2.24876855e-01,\n",
       "         2.21009145e-01],\n",
       "       [ 9.29043307e-02,  7.60472648e-02,  5.19732459e-02,\n",
       "        -1.49465841e-01, -6.96296909e-02, -2.10948329e-01,\n",
       "        -8.37095777e-02,  4.58519602e-02,  2.96282046e-02,\n",
       "        -5.84924900e-02],\n",
       "       [ 7.19789259e-02,  2.03785598e-01,  1.79306201e-01,\n",
       "         1.39326822e-01, -3.63120895e-01, -1.04923099e-01,\n",
       "         1.29577336e-02, -1.48085026e-01,  4.26415083e-02,\n",
       "        -8.18058755e-02],\n",
       "       [-1.69193727e-03,  4.29563751e-02, -4.75975778e-02,\n",
       "        -1.39857281e-01,  1.77977196e-01, -4.66673384e-02,\n",
       "         1.90517496e-02, -1.57968509e-01,  2.95906430e-01,\n",
       "         1.80768136e-01],\n",
       "       [-4.59259531e-02, -1.28137790e-01, -1.70459303e-01,\n",
       "         1.19191174e-01,  2.16902263e-01,  1.43439958e-01,\n",
       "        -1.09041725e-02,  7.85057907e-02, -4.26183704e-02,\n",
       "        -3.07760247e-01],\n",
       "       [-1.97986244e-01, -2.30577467e-01,  7.94626409e-02,\n",
       "         1.55664825e-01, -1.27099827e-01, -6.87695004e-02,\n",
       "        -3.87830569e-03,  8.95688284e-03, -2.80578788e-02,\n",
       "        -2.02665588e-01],\n",
       "       [ 7.18871500e-02, -2.09225142e-01, -1.80259167e-01,\n",
       "        -1.31311024e-01, -2.08528443e-01,  7.06322249e-03,\n",
       "        -8.78635099e-02,  9.74126789e-02, -2.60932050e-01,\n",
       "         1.92028907e-02],\n",
       "       [-1.66662842e-01,  9.88408790e-02, -8.04446063e-02,\n",
       "        -5.50965006e-02,  1.59625073e-01, -8.26560478e-02,\n",
       "        -1.16716971e-01,  2.63318058e-02, -6.02775672e-02,\n",
       "        -7.00330650e-02],\n",
       "       [-4.59014624e-02,  9.66027254e-02, -1.07607823e-01,\n",
       "         5.63622326e-02, -1.69488499e-03, -7.35358874e-03,\n",
       "         2.10204039e-01,  5.34002763e-02, -4.60610679e-02,\n",
       "         1.12984624e-01],\n",
       "       [-9.60575342e-02,  1.64627296e-01, -7.48208125e-03,\n",
       "        -2.05426182e-01, -5.77538767e-02,  2.73805839e-01,\n",
       "         1.24883603e-02,  6.89409276e-03, -1.04686329e-01,\n",
       "         1.22393924e-01],\n",
       "       [-1.20916701e-01, -1.84433190e-01,  7.64976237e-02,\n",
       "         1.20060124e-01, -1.87564235e-01, -1.32817368e-02,\n",
       "         1.57144964e-01,  7.57868748e-03, -1.67300234e-01,\n",
       "        -7.95340887e-02],\n",
       "       [ 1.68239050e-01, -1.08099615e-01, -1.97965003e-02,\n",
       "         8.95194977e-02, -7.18351988e-02, -8.88067982e-02,\n",
       "        -6.84867973e-03,  5.38727680e-02,  4.61937700e-01,\n",
       "         6.73891828e-02],\n",
       "       [-1.87869143e-01, -1.05475920e-01,  1.02025217e-01,\n",
       "         6.66724872e-02, -1.07955284e-01,  1.06851021e-01,\n",
       "         1.06640992e-01, -2.56408693e-01, -5.69961821e-03,\n",
       "         1.41330854e-01],\n",
       "       [ 8.02357009e-02, -1.34603356e-01, -3.74880787e-02,\n",
       "         1.06980536e-01,  4.07324496e-02, -2.19362460e-01,\n",
       "         8.62428662e-02, -1.28596436e-01,  2.27837166e-01,\n",
       "         1.40942660e-01],\n",
       "       [-6.08287626e-02,  1.03614795e-01, -2.32532426e-01,\n",
       "        -8.73761849e-02,  6.09934328e-02, -1.17809087e-01,\n",
       "         5.64200447e-02,  2.18726059e-01,  2.43795715e-01,\n",
       "         7.54834685e-02],\n",
       "       [ 7.65492052e-02, -4.72562076e-02, -2.30622571e-01,\n",
       "         3.50837259e-02, -2.18291802e-02, -5.46578445e-02,\n",
       "         5.33429832e-02,  7.95440962e-02,  3.14364283e-01,\n",
       "         1.27021483e-01],\n",
       "       [-4.11163425e-02, -7.03356351e-02,  5.31407708e-03,\n",
       "         2.14737184e-02,  9.78936085e-02, -7.18723874e-02,\n",
       "        -3.93500853e-03, -1.02899403e-01, -1.09743619e-01,\n",
       "         2.41424925e-02],\n",
       "       [ 1.41257206e-04, -1.65018153e-01, -1.90990588e-01,\n",
       "        -4.02960042e-02,  3.70164982e-02,  8.12663339e-03,\n",
       "        -9.46425218e-02,  3.13752646e-01,  1.54275456e-01,\n",
       "         1.25918555e-02],\n",
       "       [-1.22591346e-01,  5.16779889e-02,  2.25931411e-01,\n",
       "        -2.95198795e-02, -2.76012641e-02, -1.11907085e-02,\n",
       "         9.03727118e-02,  2.01162572e-01,  1.05804240e-01,\n",
       "        -1.66460044e-02],\n",
       "       [-8.03153320e-02, -1.79165658e-02, -1.86246741e-02,\n",
       "         7.84259875e-02, -7.34476479e-02,  3.38149218e-01,\n",
       "        -5.27392167e-02, -4.52364539e-02, -3.30506244e-02,\n",
       "         8.62441628e-03],\n",
       "       [ 4.33037859e-02,  1.32558550e-01,  7.77457643e-02,\n",
       "         5.80656853e-02, -1.76227027e-01, -5.94618549e-02,\n",
       "         3.50458277e-01, -2.10461517e-01,  1.26415765e-02,\n",
       "         1.09105416e-01],\n",
       "       [-1.17178417e-01, -2.64928971e-02, -4.60904454e-02,\n",
       "        -1.06117650e-01, -1.00126230e-01,  1.89738298e-01,\n",
       "         1.66242989e-01,  6.42939612e-02, -1.51341638e-01,\n",
       "        -2.61057767e-01],\n",
       "       [ 9.90368124e-02,  7.55268732e-02, -1.13800247e-01,\n",
       "        -2.08877634e-01,  5.14085719e-02,  1.38831466e-02,\n",
       "         1.71612876e-01, -1.31670064e-01,  3.87222671e-01,\n",
       "         3.21876132e-01],\n",
       "       [ 5.36160174e-02, -3.87581684e-02,  1.35714068e-01,\n",
       "         1.43287044e-01,  6.12203208e-02,  1.01965995e-01,\n",
       "        -2.31758923e-01,  4.74014325e-02,  5.64950632e-02,\n",
       "        -2.48082324e-01],\n",
       "       [ 7.67721955e-02, -1.21656229e-01,  4.41153491e-03,\n",
       "         1.61320150e-02, -1.81303437e-01, -9.57241006e-02,\n",
       "         9.53978940e-02,  8.84560616e-03,  4.52180631e-02,\n",
       "         1.84284998e-01],\n",
       "       [-1.70887992e-02,  7.91066763e-02, -5.15336266e-02,\n",
       "        -2.24759397e-02, -4.07703347e-02, -7.51495327e-03,\n",
       "        -8.66191984e-02,  1.09592729e-01,  1.54599938e-01,\n",
       "         3.82808722e-02],\n",
       "       [-5.39864886e-02,  1.97196806e-01,  1.45835758e-01,\n",
       "         1.12089882e-01, -1.21852029e-01, -3.17869250e-02,\n",
       "         5.57385663e-02,  1.06291241e-01, -5.61637820e-02,\n",
       "         8.04055449e-02],\n",
       "       [-8.68546700e-02, -5.21334762e-02, -5.78062618e-02,\n",
       "         1.10032344e-01, -6.18628149e-03,  4.82967214e-02,\n",
       "        -2.68278961e-01, -1.01085822e-01, -2.21338495e-01,\n",
       "        -2.07476389e-01],\n",
       "       [ 2.80611339e-01, -2.29962053e-01,  1.15853245e-01,\n",
       "         4.11501076e-02, -1.04780203e-02, -1.28587483e-01,\n",
       "        -2.65981833e-01,  7.19342592e-02, -6.45475868e-02,\n",
       "         9.78431598e-03]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#テスト\n",
    "n_features = 784\n",
    "n_nodes_list = [100,50]\n",
    "out_put = 10\n",
    "\n",
    "all_size = [n_features] + n_nodes_list + [out_put]\n",
    "\n",
    "for idx in range(1, len(all_size)):\n",
    "    sig = np.sqrt(1.0 / all_size[idx - 1]) \n",
    "            \n",
    "    W = sig * np.random.randn(all_size[idx-1], all_size[idx])\n",
    "    \n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Heの初期値\n",
    "\n",
    "Heの初期値における標準偏差 $\n",
    "σ\n",
    "$ は次の式で求められます。\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{2}{n}}$$\n",
    "\n",
    "$\n",
    "n\n",
    "$ : 前の層のノード数\n",
    "\n",
    "\n",
    "《論文》\n",
    "\n",
    "\n",
    "[He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.](https://arxiv.org/pdf/1502.01852.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04407457, -0.02042442,  0.01424309, ...,  0.04249791,\n",
       "        -0.00364516, -0.03781344],\n",
       "       [-0.01780659, -0.04644926,  0.03114321, ..., -0.01208965,\n",
       "         0.02867198, -0.0012079 ],\n",
       "       [-0.01471756, -0.00487631, -0.04059753, ...,  0.00013431,\n",
       "        -0.04318664,  0.007218  ],\n",
       "       ...,\n",
       "       [-0.01349859, -0.08335759,  0.00127902, ...,  0.02594528,\n",
       "         0.03026292, -0.09315953],\n",
       "       [ 0.05026497,  0.00022598, -0.00274955, ...,  0.11352182,\n",
       "         0.00365417, -0.00321237],\n",
       "       [ 0.12445151, -0.05929608, -0.01624362, ...,  0.02292689,\n",
       "         0.04789485, -0.03768623]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_nodes1 = 100\n",
    "n_nodes2 = 50\n",
    "\n",
    "\n",
    "W = np.random.randn(n_features, n_nodes1) * np.sqrt(2.0 / n_features)\n",
    "\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.28864199, -0.06215646, -0.05879537, ..., -0.15842031,\n",
       "         0.13964738,  0.16557787],\n",
       "       [-0.30191686,  0.09435001, -0.02163267, ..., -0.01667084,\n",
       "        -0.06853424,  0.13714099],\n",
       "       [ 0.03613298, -0.17021376,  0.10407657, ..., -0.14525232,\n",
       "        -0.05764651, -0.08733439],\n",
       "       ...,\n",
       "       [ 0.25626263, -0.07066893,  0.03194354, ..., -0.23759958,\n",
       "         0.1457977 , -0.13385122],\n",
       "       [ 0.20351462, -0.02618746, -0.06957511, ..., -0.15199151,\n",
       "         0.04790812, -0.02236514],\n",
       "       [-0.0610086 , -0.02717208,  0.05997129, ...,  0.0218079 ,\n",
       "        -0.13751868,  0.23439549]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#テスト\n",
    "n_features = 784\n",
    "n_nodes_list = [100,50]\n",
    "out_put = 10\n",
    "\n",
    "\n",
    "all_size = [n_features] + n_nodes_list + [out_put]\n",
    "\n",
    "for idx in range(1, len(n_nodes_list)):\n",
    "    sig = np.sqrt(2.0 / n_nodes_list[idx - 1]) \n",
    "        \n",
    "        \n",
    "    W = sig * np.random.randn(n_nodes_list[idx-1], n_nodes_list[idx])\n",
    "    \n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "がっちゃん子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.17064704, -0.31868538, -0.05507874, -0.00457947,  0.10611308,\n",
       "        -0.37072696,  0.33856971,  0.17894038, -0.09120928, -0.4120244 ],\n",
       "       [ 0.10568227,  0.01730688,  0.09725147, -0.13740957, -0.09041942,\n",
       "        -0.15444612, -0.07495771, -0.16370247,  0.35170051,  0.0169678 ],\n",
       "       [ 0.17396531, -0.30506195, -0.50391557,  0.11795949, -0.19529565,\n",
       "        -0.26016156,  0.21184617, -0.36819201,  0.1820924 ,  0.02232979],\n",
       "       [ 0.05910245, -0.08049669, -0.05837503, -0.10307186,  0.00999909,\n",
       "        -0.31030933, -0.16707628, -0.02871301,  0.07498274, -0.02317705],\n",
       "       [ 0.05180334, -0.09465567, -0.28314546,  0.34087394, -0.08398572,\n",
       "         0.11240949,  0.06095579, -0.15470408, -0.39618463, -0.501212  ],\n",
       "       [ 0.21065196,  0.02519392, -0.01394606, -0.05137246,  0.09839488,\n",
       "        -0.07717745, -0.07234915, -0.28906295,  0.04204162,  0.11509024],\n",
       "       [-0.12782074, -0.31580097, -0.09178159, -0.09678778,  0.42565927,\n",
       "        -0.45353772, -0.15700741, -0.1746383 ,  0.06049361, -0.34018561],\n",
       "       [ 0.02277935, -0.32822488,  0.17067797,  0.26214061,  0.23959177,\n",
       "         0.02804842, -0.07348269,  0.13019948, -0.10371562, -0.10325109],\n",
       "       [-0.20515648, -0.11151269,  0.22339092, -0.08715224,  0.14570473,\n",
       "        -0.0848954 ,  0.04045815, -0.09705926, -0.06550579,  0.04490904],\n",
       "       [ 0.37351097, -0.33179387, -0.02385611,  0.01467888, -0.07614558,\n",
       "        -0.00231779, -0.08375361,  0.23197932,  0.0826158 ,  0.18905593],\n",
       "       [-0.11591372, -0.20542337, -0.05839901,  0.02025247,  0.23750309,\n",
       "        -0.03953759,  0.04926162,  0.27577942, -0.12155506,  0.41809455],\n",
       "       [-0.11087121,  0.22949371, -0.10177472,  0.09736776, -0.07813577,\n",
       "        -0.0703769 ,  0.05431572,  0.30966089, -0.13767248,  0.29441124],\n",
       "       [-0.05728623, -0.07168455,  0.16089803,  0.199299  ,  0.02633907,\n",
       "        -0.18326575, -0.03463414, -0.09048866, -0.27690839,  0.0375157 ],\n",
       "       [ 0.51108414,  0.05626169, -0.35754618, -0.02465771, -0.19437435,\n",
       "         0.36882767,  0.02641726,  0.22120331, -0.1196121 , -0.05297883],\n",
       "       [-0.01650491, -0.16439333, -0.17979284, -0.17150669,  0.40262128,\n",
       "        -0.01400129,  0.07693732,  0.18074798,  0.17649232,  0.19931936],\n",
       "       [-0.01486106,  0.22762906,  0.04424439, -0.27379428,  0.01440408,\n",
       "         0.13443401,  0.1734152 , -0.10061694,  0.07677217,  0.11931221],\n",
       "       [ 0.00717261,  0.16172601,  0.21477933,  0.06249165,  0.26577964,\n",
       "         0.04849303, -0.35635061,  0.21110309, -0.21710241,  0.12714501],\n",
       "       [ 0.28026653, -0.44864513,  0.02466423, -0.18595495,  0.13693177,\n",
       "         0.09408564,  0.01146376, -0.32714348, -0.06844779, -0.27468653],\n",
       "       [-0.33642201,  0.32186142,  0.27143745,  0.00230785, -0.27656435,\n",
       "        -0.37206493, -0.22568664, -0.07459103,  0.11977246, -0.27881205],\n",
       "       [-0.22908396, -0.13461926,  0.12357132,  0.11432806,  0.14795724,\n",
       "         0.15639274,  0.01107806,  0.11975495, -0.08100661, -0.14013518],\n",
       "       [-0.03438594,  0.06015142, -0.36629836, -0.12372034, -0.21177259,\n",
       "        -0.22138049,  0.08390952,  0.33004709,  0.11548444,  0.06601747],\n",
       "       [ 0.42005496, -0.11776293, -0.24461117, -0.20952945, -0.20013056,\n",
       "        -0.23414465, -0.15031632,  0.18050948,  0.00275504, -0.02378431],\n",
       "       [-0.18845102,  0.13165001,  0.20553498, -0.04386575, -0.11099456,\n",
       "         0.14661063, -0.07077973, -0.1395065 , -0.09363488, -0.05779835],\n",
       "       [-0.28867126, -0.1447909 ,  0.05794272, -0.02446337,  0.22136455,\n",
       "         0.1068893 , -0.16172495,  0.05642736, -0.12310037, -0.29868347],\n",
       "       [ 0.31063096,  0.25620093, -0.11125116, -0.0147102 , -0.26066909,\n",
       "         0.36261335,  0.12219525, -0.25221147,  0.00957542,  0.11230961],\n",
       "       [-0.04215459, -0.01000607, -0.09146556,  0.08085461, -0.20890137,\n",
       "        -0.33038071, -0.15295475, -0.19439534, -0.35680305, -0.1443397 ],\n",
       "       [ 0.09324606, -0.00356599, -0.19385516, -0.23310904, -0.098297  ,\n",
       "         0.38292919, -0.23615564, -0.04106532,  0.12243354,  0.00345971],\n",
       "       [-0.02611719,  0.26437205, -0.13659844, -0.03758409, -0.08188961,\n",
       "        -0.1525026 ,  0.10875897,  0.15859596, -0.09791667, -0.0304732 ],\n",
       "       [-0.14722834,  0.26178328, -0.1826145 ,  0.27268345, -0.10365977,\n",
       "         0.37075268,  0.42527546,  0.33218588,  0.17489743,  0.08721208],\n",
       "       [-0.21333323, -0.24922483, -0.20902315,  0.4364154 , -0.09153144,\n",
       "         0.18761097,  0.15867173, -0.11652852,  0.26989445, -0.33188186],\n",
       "       [-0.11571417,  0.07078295,  0.00066986,  0.21897737,  0.14970137,\n",
       "        -0.07480909,  0.25798449, -0.12897431,  0.32440648, -0.07965151],\n",
       "       [-0.19034741,  0.15471176, -0.02420723, -0.05689163,  0.00350298,\n",
       "         0.01291018, -0.12812646,  0.12414102,  0.12297583, -0.06436641],\n",
       "       [ 0.02661032,  0.04666662, -0.20906471, -0.312351  , -0.05802842,\n",
       "         0.28206631, -0.00666064,  0.47528449,  0.00516178,  0.11467719],\n",
       "       [ 0.12799569, -0.25964131,  0.25269475, -0.1587075 ,  0.1275573 ,\n",
       "         0.21767416,  0.1868967 ,  0.32908769,  0.22923321,  0.09264201],\n",
       "       [-0.04258648,  0.01921077, -0.2418073 ,  0.11453589,  0.00183592,\n",
       "        -0.117008  , -0.09114876,  0.01892404, -0.00779183,  0.03759139],\n",
       "       [ 0.02457517,  0.18183282,  0.04793908,  0.00190395,  0.07115424,\n",
       "        -0.21651347, -0.04004713,  0.0045605 , -0.05176529,  0.34223023],\n",
       "       [ 0.14740052,  0.40856958,  0.06507129,  0.56762169,  0.03920995,\n",
       "        -0.01028449, -0.01053294, -0.01835413,  0.14580898, -0.19581262],\n",
       "       [-0.16868224, -0.29202057,  0.04282381, -0.00177648,  0.03218121,\n",
       "        -0.29083983,  0.15757224,  0.18559277,  0.01281449, -0.06627993],\n",
       "       [ 0.06007714, -0.19352879, -0.06433234,  0.25175886,  0.21445818,\n",
       "         0.24920576,  0.15319037, -0.05817442,  0.05577835,  0.25436992],\n",
       "       [ 0.00843078,  0.20288695,  0.32065194,  0.13952848,  0.02726429,\n",
       "        -0.07418584,  0.10546651, -0.16019996, -0.21426087, -0.2203202 ],\n",
       "       [-0.43346497, -0.22202317,  0.07397326,  0.25012682,  0.10038574,\n",
       "        -0.30669665, -0.07438942, -0.00208454, -0.505651  , -0.03262524],\n",
       "       [-0.04929745,  0.05546869, -0.24386962, -0.35580715,  0.16837593,\n",
       "         0.10288925, -0.10820875,  0.19343529,  0.09169724, -0.23264111],\n",
       "       [ 0.27990201, -0.16307982, -0.08356988, -0.25197578, -0.1068567 ,\n",
       "         0.3877672 , -0.08471552, -0.03283803,  0.4426588 , -0.15883763],\n",
       "       [ 0.50117266,  0.12832408,  0.20846311, -0.2450304 ,  0.00998393,\n",
       "        -0.17041012,  0.14792854,  0.13821037, -0.34643172, -0.0986571 ],\n",
       "       [-0.12217347,  0.16132548,  0.0904919 ,  0.09679999,  0.06217387,\n",
       "        -0.1421383 ,  0.11752202,  0.405535  , -0.29206744,  0.24521965],\n",
       "       [-0.04656337, -0.1798106 ,  0.24257251, -0.22162242, -0.03756608,\n",
       "        -0.3079571 ,  0.16673226, -0.26279733,  0.14760293, -0.2474695 ],\n",
       "       [ 0.14341721,  0.18567365,  0.03442738,  0.0321642 ,  0.42633337,\n",
       "        -0.18912078,  0.11237524, -0.20098243, -0.11915989,  0.29763425],\n",
       "       [-0.22635291,  0.19563814, -0.0145842 ,  0.13844034,  0.27119372,\n",
       "         0.53474999, -0.13007852, -0.07814256,  0.255006  ,  0.02417313],\n",
       "       [ 0.04645579, -0.04278241,  0.00676626, -0.0610314 ,  0.04136512,\n",
       "        -0.11085522, -0.16644825,  0.17238483,  0.06026767, -0.01522246],\n",
       "       [ 0.16828681,  0.11112197, -0.04341712,  0.07480899, -0.22391716,\n",
       "        -0.001023  ,  0.2717954 , -0.51223605, -0.20624748, -0.04378048]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#テスト\n",
    "n_features = 784\n",
    "n_nodes_list = [100,50]\n",
    "out_put = 10\n",
    "weight_init_std = \"relu\"\n",
    "\n",
    "\n",
    "all_size = [n_features] + n_nodes_list + [out_put]\n",
    "\n",
    "for idx in range(1, len(all_size)):\n",
    "    sig = weight_init_std\n",
    "    if str(weight_init_std).lower() in ('relu', 'he'):\n",
    "        sig = np.sqrt(2.0 / all_size[idx - 1]) \n",
    "    else:\n",
    "        sig = np.sqrt(1.0 / all_size[idx - 1]) \n",
    "        \n",
    "        \n",
    "    W = sig * np.random.randn(all_size[idx-1], all_size[idx])\n",
    "    \n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題7】最適化手法\n",
    "\n",
    "学習率は学習過程で変化させていく方法が一般的です。基本的な手法である AdaGrad のクラスを作成してください。\n",
    "\n",
    "\n",
    "まず、これまで使ってきたSGDを確認します。\n",
    "\n",
    "$$W_i^{\\prime} = W_i - \\alpha E(\\frac{\\partial L}{\\partial W_i}) \\\\\n",
    "B_i^{\\prime} = B_i - \\alpha E(\\frac{\\partial L}{\\partial B_i})$$\n",
    "\n",
    "$\n",
    "α\n",
    " $: 学習率（層ごとに変えることも可能だが、基本的には全て同じとする）\n",
    " \n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_i}$  :$\n",
    "W\n",
    "i\n",
    "$ に関する損失 $\n",
    "L\n",
    "$ の勾配\n",
    "\n",
    "$\\frac{\\partial L}{\\partial B_i}$\n",
    " : $\n",
    "B\n",
    "i\n",
    "$ に関する損失 $\n",
    "L\n",
    " $の勾配\n",
    "\n",
    "\n",
    "\n",
    "$\n",
    "E\n",
    "(\n",
    ")\n",
    "$ : ミニバッチ方向にベクトルの平均を計算\n",
    "\n",
    "\n",
    "続いて、AdaGradです。バイアスの数式は省略しますが、重みと同様のことをします。\n",
    "\n",
    "\n",
    "更新された分だけその重みに対する学習率を徐々に下げていきます。イテレーションごとの勾配の二乗和 \n",
    "H\n",
    " を保存しておき、その分だけ学習率を小さくします。\n",
    "\n",
    "\n",
    "学習率は重み一つひとつに対して異なることになります。\n",
    "\n",
    "\n",
    "$$ H_i^{\\prime}  = H_i+E(\\frac{\\partial L}{\\partial W_i})×E(\\frac{\\partial L}{\\partial W_i})\\\\\n",
    "W_i^{\\prime} = W_i - \\alpha \\frac{1}{\\sqrt{H_i^{\\prime} }} E(\\frac{\\partial L}{\\partial W_i}) \\\\ $$\n",
    "\n",
    "$\n",
    "H\n",
    "i\n",
    " $: i層目に関して、前のイテレーションまでの勾配の二乗和（初期値は0）\n",
    "\n",
    "$\n",
    "H\n",
    "′\n",
    "i\n",
    "$ : 更新した $\n",
    "H\n",
    "i\n",
    "$\n",
    "\n",
    "\n",
    "《論文》\n",
    "[Duchi JDUCHI, J., & Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization * Elad Hazan. Journal of Machine Learning Research (Vol. 12).](https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class AdaGrad():\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題8】クラスの完成\n",
    "\n",
    "任意の構成で学習と推定が行えるScratchDeepNeuralNetrowkClassifierクラスを完成させてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0,
     20
    ]
   },
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "\n",
    "    def update(self, params,grads):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "\n",
    "        \"\"\"\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "            \n",
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": [
     0,
     29,
     45,
     49,
     55,
     61,
     79
    ]
   },
   "outputs": [],
   "source": [
    "class Affine():\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.X = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        out = np.dot(X, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dX = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.X.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dX = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.X.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    \n",
    "class Tangent():\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = np.tanh(X)\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dX = dout*(1 - self.out**2)\n",
    "        \n",
    "        return dX\n",
    "\n",
    "    \n",
    "class Sigmoid():\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = 1 / (1 + np.exp(-X))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dX = dout*(1.0 - self.out) * self.out\n",
    "        \n",
    "        return dX\n",
    "\n",
    "    \n",
    "class Relu():\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.mask = (X <= 0)\n",
    "        out = X.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "    \n",
    "    \n",
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "    \n",
    "    def forward(self, X, t):\n",
    "        self.t = t\n",
    "        self.y = self.softmax(X)\n",
    "        self.loss = self.cross_entropy_error(self.y, self.t)\n",
    "        return self.loss \n",
    "    \n",
    "    def cross_entropy_error(self, y, t):\n",
    "        if t.size == y.size:\n",
    "            t = t.argmax(axis=1)\n",
    "            \n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "\n",
    "    def softmax(self, X):\n",
    "        X = X - np.max(X, axis=-1, keepdims=True)\n",
    "        y = np.exp(X) / np.sum(np.exp(X), axis=-1, keepdims=True)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dX = (self.y - self.t) / batch_size\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowk(object):\n",
    "    \"\"\"\n",
    "    全結合による多層ニューラルネットワーク\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_features : 入力サイズ（MNISTの場合は784）\n",
    "    n_nodes_list : 中間層(隠れ層)のニューロンの数のリスト（[100, 100, 100]）\n",
    "    n_output : 出力サイズ（MNISTの場合は10）\n",
    "    activation : 'ReLU' or 'Sigmoid'\n",
    "    weight_init_std : 重みの標準偏差を指定（0.01）\n",
    "        'ReLU'または'He'を指定した場合は「Heの初期値」を設定\n",
    "        'Sigmoid'または'Xavier'を指定した場合は「Xavierの初期値」を設定\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, n_nodes_list, n_output, activation = 'ReLU', weight_init_std = 'ReLU'):\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes_list = n_nodes_list\n",
    "        self.n_output = n_output\n",
    "        self.n_nodes_layer_num = len(n_nodes_list)\n",
    "        self.params = {}\n",
    "        self.optimizers = {}\n",
    "        self.layers = OrderedDict() #「順序付き」辞書型変数\n",
    "        \n",
    "        # 重みの初期化\n",
    "        self.initialization(weight_init_std)\n",
    "        \n",
    "        # レイヤの生成\n",
    "        activation_layer = {'Sigmoid':Sigmoid, 'Relu':Relu}\n",
    "        \n",
    "        # 中間層\n",
    "        for idx in range(1, self.n_nodes_layer_num+1):\n",
    "            # Affine1,Affine2.....\n",
    "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
    "                                                     self.params['b' + str(idx)])\n",
    "            # Relu1,Relu2....\n",
    "            self.layers['Activation_func' + str(idx)] = activation_layer[activation]()\n",
    "        \n",
    "        # 出力層\n",
    "        idx = self.n_nodes_layer_num + 1\n",
    "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
    "        self.params['b' + str(idx)])\n",
    "        \n",
    "        self.last_layer = Softmax()\n",
    "        \n",
    "        \n",
    "    def initialization(self,weight_init_std):\n",
    "        \"\"\"\n",
    "        重みの初期設定\n",
    "\n",
    "        \"\"\"\n",
    "        # 入力、中間、出力の配列[784,n,n.....,10]\n",
    "        all_size_list = [self.n_features] + self.n_nodes_list + [self.n_output]\n",
    "        \n",
    "        for idx in range(1, len(all_size_list)):\n",
    "            sig = weight_init_std\n",
    "            # ReLU or He\n",
    "            if str(weight_init_std).lower() in ('relu', 'he'):\n",
    "                # 入力、中間の標準偏差\n",
    "                sig = np.sqrt(2.0 / all_size_list[idx - 1])\n",
    "            # その他\n",
    "            else:\n",
    "                # 入力、中間の標準偏差\n",
    "                sig = np.sqrt(1.0 / all_size_list[idx - 1]) \n",
    "            \n",
    "            # W,b の初期化\n",
    "            self.params['W' + str(idx)] = sig * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
    "            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        for layer in self.layers.values():\n",
    "            X = layer.forward(x)\n",
    "\n",
    "        return X\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "class ScratchDeepNeuralNetrowkClassifier(object):\n",
    "    \n",
    "    def __init__(self,):\n",
    "        self.SDNN = {}\n",
    "        self.optimizers = {}\n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "    \n",
    "    def fit(self, X, y, X_val, y_val):\n",
    "        self.optimizers['SGD'] = SGD()\n",
    "        self.optimizers['AdaGrad'] = AdaGrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題9】学習と推定\n",
    "\n",
    "層の数や活性化関数を変えたいくつかのネットワークを作成してください。そして、MNISTのデータを学習・推定し、Accuracyを計算してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
