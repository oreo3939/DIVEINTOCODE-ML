{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1次元の畳み込みニューラルネットワークスクラッチ\n",
    "\n",
    "畳み込みニューラルネットワーク（CNN） のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "\n",
    "このSprintでは1次元の 畳み込み層 を作成し、畳み込みの基礎を理解することを目指します。次のSprintでは2次元畳み込み層とプーリング層を作成することで、一般的に画像に対して利用されるCNNを完成させます。\n",
    "\n",
    "\n",
    "クラスの名前はScratch1dCNNClassifierとしてください。クラスの構造などは前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1次元畳み込み層とは\n",
    "\n",
    "CNNでは画像に対しての2次元畳み込み層が定番ですが、ここでは理解しやすくするためにまずは1次元畳み込み層を実装します。1次元畳み込みは実用上は自然言語や波形データなどの 系列データ で使われることが多いです。\n",
    "\n",
    "\n",
    "畳み込みは任意の次元に対して考えることができ、立体データに対しての3次元畳み込みまではフレームワークで一般的に用意されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの用意\n",
    "\n",
    "検証には引き続きMNISTデータセットを使用します。1次元畳み込みでは全結合のニューラルネットワークと同様に平滑化されたものを入力します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成\n",
    "\n",
    "チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。基本構造は前のSprintで作成した全結合層のFCクラスと同じになります。なお、重みの初期化に関するクラスは必要に応じて作り変えてください。Xavierの初期値などを使う点は全結合層と同様です。\n",
    "\n",
    "\n",
    "ここでは パディング は考えず、ストライド も1に固定します。また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。この部分の拡張はアドバンス課題とします。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。\n",
    "\n",
    "$$a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b$$\n",
    "\n",
    "$\n",
    "a\n",
    "i\n",
    "$ : 出力される配列のi番目の値\n",
    "\n",
    "$\n",
    "F\n",
    "$ : フィルタのサイズ\n",
    "\n",
    "$\n",
    "x\n",
    "(\n",
    "i\n",
    "+\n",
    "s\n",
    ")\n",
    " $: 入力の配列の(i+s)番目の値\n",
    "\n",
    "$\n",
    "w\n",
    "s\n",
    "$ : 重みの配列のs番目の値\n",
    "\n",
    "$\n",
    "b\n",
    "$ : バイアス項\n",
    "\n",
    "\n",
    "全てスカラーです。\n",
    "\n",
    "\n",
    "次に更新式です。ここがAdaGradなどに置き換えられる点は全結合層と同様です。\n",
    "\n",
    "$$w_s^{\\prime} = w_s - \\alpha \\frac{\\partial L}{\\partial w_s} \\\\\n",
    "b^{\\prime} = b - \\alpha \\frac{\\partial L}{\\partial b}$$\n",
    "\n",
    "$\n",
    "α\n",
    " $: 学習率\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_s}$\n",
    " : $\n",
    "w\n",
    "s\n",
    "$ に関する損失 $\n",
    "L\n",
    "$ の勾配\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b}$\n",
    " : $\n",
    "b\n",
    "$ に関する損失 $\n",
    "L\n",
    "$ の勾配\n",
    "\n",
    "\n",
    "勾配 \n",
    "$\\frac{\\partial L}{\\partial w_s}$\n",
    " や \n",
    "$\\frac{\\partial L}{\\partial b}$\n",
    "\n",
    " を求めるためのバックプロパゲーションの数式が以下です\n",
    " \n",
    " \n",
    "$$\\frac{\\partial L}{\\partial w_s} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}x_{(i+s)}\\\\\n",
    "\\frac{\\partial L}{\\partial b} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}$$ \n",
    "\n",
    "$\\frac{\\partial L}{\\partial a_i}$\n",
    ": 勾配の配列のi番目の値\n",
    "\n",
    "$N_{out}$\n",
    ": 出力のサイズ\n",
    "\n",
    "前の層に流す誤差の数式は以下です。\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x_j} = \\sum_{s=0}^{F-1} \\frac{\\partial L}{\\partial a_{(j-s)}}w_s$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_j}$\n",
    ": 前の層に流す誤差の配列のj番目の値\n",
    "\n",
    "ただし、$j-s<0$ または  $j-s>N_{out}-1$  のとき $\\frac{\\partial L}{\\partial a_{(j-s)}} =0$　です。\n",
    "\n",
    "\n",
    "全結合層との大きな違いは、重みが複数の特徴量に対して共有されていることです。この場合は共有されている分の誤差を全て足すことで勾配を求めます。計算グラフ上での分岐はバックプロパゲーションの際に誤差の足し算をすれば良いことになります。\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# テスト\n",
    "x = np.array([1, 2, 3, 4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#配列作成\n",
    "idx1 = np.arange(w.shape[0])\n",
    "idx2 = np.arange(w.shape[0] - 1).reshape(-1, 1)\n",
    "index = idx1 + idx2\n",
    "a = np.dot(x[index], w.T) + b\n",
    "print(a)\n",
    "\n",
    "# a = np.array([35, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = np.array([10, 20])\n",
    "d = np.sum(da)\n",
    "d\n",
    "\n",
    "#delta_b = np.array([30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.dot(da,x[index])\n",
    "print(da)\n",
    "print(x[index])\n",
    "s\n",
    "\n",
    "#d elta_w = np.array([50, 80, 110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#0配列を用意\n",
    "dx = np.zeros(x.shape[0])\n",
    "\n",
    "\n",
    "for j in range(x.shape[0]):    \n",
    "    for s in range(w.shape[0]):   \n",
    "        \n",
    "        # j 0,0,0,1,1,1....\n",
    "        # s 0,1,2,0,1,2....\n",
    "        # 0未満　or １超える\n",
    "        if j - s < 0 or j - s > 1:\n",
    "            \n",
    "            # w[s] = 3,5,7,3,5,7....\n",
    "            # w[s] = 条件該当\n",
    "            dx[j] += 0 * w[s]\n",
    "\n",
    "        # その他(0,1)\n",
    "        else:\n",
    "            \n",
    "            # da[j-s]は該当値*3,5,7\n",
    "            dx[j] += da[j - s] * w[s]\n",
    "\n",
    "dx\n",
    "\n",
    "# delta_x = np.array([30, 110, 170, 140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class SimpleConv1d:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W, self.b = W, b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.X =None\n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        FN, C, FL = self.W.shape #n_filters, n_channels, filter_length\n",
    "        N, C, L = X.shape #batch_size, n_channels, n_features\n",
    "        out_size = get_output_size(L, FL, stride=self.stride, pad=self.pad)\n",
    "        col = im2col_1d(input_data=X, filter_size=FL, stride=self.stride, pad=self.pad)\n",
    "\n",
    "        out = col @ self.W.reshape(FN, -1).T + self.b\n",
    "        out = out.reshape(N, out_size, -1).transpose(0, 2, 1)\n",
    "        self.X = X\n",
    "        self.col = col\n",
    "        return out\n",
    "\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FL = self.W.shape #n_filters, n_channels, filter_length\n",
    "        N, C, L = self.X.shape\n",
    "        dout = dout.transpose(0, 2, 1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = self.col.T @ dout\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FL)\n",
    "        dcol = dout @ self.W.reshape(FN, -1)\n",
    "\n",
    "        dx = col2im_1d(col=dcol, input_shape=self.X.shape, filter_size=FL, stride=self.stride, pad=self.pad)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】1次元畳み込み後の出力サイズの計算\n",
    "\n",
    "畳み込みを行うと特徴量の数が変化します。どのように変化するかは以下の数式から求められます。パディングやストライドも含めています。この計算を行う関数を作成してください。\n",
    "\n",
    "$$ N_{out} =  \\frac{N_{in}+2P-F}{S} + 1\\\\ $$\n",
    "\n",
    "$N_{out}$\n",
    " : 出力のサイズ（特徴量の数）\n",
    "\n",
    "\n",
    "$N_{in}$\n",
    " : 入力のサイズ（特徴量の数）\n",
    "\n",
    "$\n",
    "P\n",
    "$ : ある方向へのパディングの数\n",
    "\n",
    "$\n",
    "F\n",
    "$ : フィルタのサイズ\n",
    "\n",
    "\n",
    "$S$\n",
    " : ストライドのサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_output_size(n_features, filter_length, stride=1, pad=0):\n",
    "    return int(1 + (n_features + 2 * pad - filter_length) / stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     28
    ]
   },
   "outputs": [],
   "source": [
    "def im2col_1d(input_data, filter_size, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Paramaters\n",
    "    ----------------\n",
    "    input_data: ndarray of shape(n_data, channel, n_features)\n",
    "    filter_size: int\n",
    "    stride: int\n",
    "    pad: int\n",
    "    Returns\n",
    "    -----------------\n",
    "    col: 1D array\n",
    "    \"\"\"\n",
    "\n",
    "    N, C, L = input_data.shape#\n",
    "    out_size = get_output_size(n_features=L, filter_length=filter_size, stride=stride, pad=pad)\n",
    "\n",
    "    #padding\n",
    "    img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_size, out_size))\n",
    "\n",
    "    for f in range(filter_size):\n",
    "        f_max = f + stride * out_size\n",
    "        col[:, :, f, :] = img[:, :, f:f_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 3, 1, 2)\n",
    "    col = col.reshape(N * out_size, -1)\n",
    "    return col\n",
    "\n",
    "def col2im_1d(col, input_shape, filter_size, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    -----------------\n",
    "    col: data to convert\n",
    "    input_shape:\n",
    "        i.e. shape(10, 1, 784)\n",
    "    filter_size: int\n",
    "    stride: int\n",
    "    pad: int\n",
    "    Returns\n",
    "    -----------------\n",
    "    ndarray of shape input_shape\n",
    "    \"\"\"\n",
    "    N, C, L = input_shape#shape(n_data, channel, n_features)\n",
    "    out_size = get_output_size(n_features=L, filter_length=filter_size, stride=stride, pad=pad)\n",
    "\n",
    "    col = col.reshape(N, out_size, C, filter_size).transpose(0, 2, 3, 1)\n",
    "    img = np.zeros((N, C, L + 2 * pad +  stride - 1))\n",
    "    for f in range(filter_size):\n",
    "        f_max = f + stride * out_size\n",
    "        img[:, :, f:f_max:stride] += col[:, :, f, :]\n",
    "\n",
    "    return img[:, :, pad:L + pad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】小さな配列での1次元畳み込み層の実験\n",
    "\n",
    "次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。\n",
    "\n",
    "\n",
    "入力x、重みw、バイアスbを次のようにします。\n",
    "\n",
    "    x = np.array([1,2,3,4])\n",
    "    w = np.array([3, 5, 7])\n",
    "    b = np.array([1])\n",
    "    \n",
    "フォワードプロパゲーションをすると出力は次のようになります\n",
    "\n",
    "    a = np.array([35, 50])\n",
    "    \n",
    "    \n",
    "次にバックプロパゲーションを考えます。誤差は次のようであったとします。\n",
    "\n",
    "    delta_a = np.array([10, 20])\n",
    "    \n",
    "バックプロパゲーションをすると次のような値になります。\n",
    "\n",
    "    delta_b = np.array([30])\n",
    "    delta_w = np.array([50, 80, 110])\n",
    "    delta_x = np.array([30, 110, 170, 140])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[[1, 2, 3, 4]]])\n",
    "W = np.array([[[3, 5, 7]]])\n",
    "b = np.array([1])\n",
    "delta_a = np.array([[[10, 20]]])\n",
    "\n",
    "s = SimpleConv1d(W=W,b=b)\n",
    "sc = s.forward(X)\n",
    "print(sc)\n",
    "\n",
    "dx = s.backward(delta_a)\n",
    "print(s.db)\n",
    "print(s.dW)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装上の工夫\n",
    "\n",
    "畳み込みを実装する場合は、まずはfor文を重ねていく形で構いません。しかし、できるだけ計算は効率化させたいため、以下の式を一度に計算する方法を考えることにします。\n",
    "\n",
    "$$ a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b $$\n",
    "\n",
    "バイアス項は単純な足し算のため、重みの部分を見ます。\n",
    "\n",
    "$$ \\sum_{s=0}^{F-1}x_{(i+s)}w_s $$\n",
    "\n",
    "これは、xの一部を取り出した配列とwの配列の内積です。具体的な状況を考えると、以下のようなコードで計算できます。この例では流れを分かりやすくするために、各要素同士でアダマール積を計算してから合計を計算しています。これは結果的に内積と同様です。\n",
    "\n",
    "    x = np.array([1, 2, 3, 4])\n",
    "    w = np.array([3, 5, 7])\n",
    "    a = np.empty((2, 3))\n",
    "    indexes0 = np.array([0, 1, 2]).astype(np.int)\n",
    "    indexes1 = np.array([1, 2, 3]).astype(np.int)\n",
    "    a[0] = x[indexes0]*w # x[indexes0]は([1, 2, 3])である\n",
    "    a[1] = x[indexes1]*w # x[indexes1]は([2, 3, 4])である\n",
    "    a = a.sum(axis=1)\n",
    "    \n",
    "ndarrayは配列を使ったインデックス指定ができることを利用した方法です。\n",
    "\n",
    "\n",
    "また、二次元配列を使えば一次元配列から二次元配列が取り出せます。\n",
    "\n",
    "    x = np.array([1, 2, 3, 4])\n",
    "    indexes = np.array([[0, 1, 2], [1, 2, 3]]).astype(np.int)\n",
    "    print(x[indexes]) # ([[1, 2, 3], [2, 3, 4]])\n",
    "\n",
    "\n",
    "このこととブロードキャストなどをうまく組み合わせることで、一度にまとめて計算することも可能です。\n",
    "\n",
    "\n",
    "畳み込みの計算方法に正解はないので、自分なりに効率化していってください。\n",
    "\n",
    "\n",
    "《参考》\n",
    "\n",
    "\n",
    "以下のページのInteger array indexingの部分がこの方法についての記述です。\n",
    "\n",
    "\n",
    "[Indexing — NumPy v1.17 Manual](https://numpy.org/doc/stable/reference/arrays.indexing.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
    "\n",
    "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。\n",
    "\n",
    "\n",
    "例えば以下のようなx, w, bがあった場合は、\n",
    "\n",
    "    x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "    w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "    b = np.array([1, 2, 3]) # （出力チャンネル数）\n",
    "\n",
    "出力は次のようになります。\n",
    "\n",
    "    a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、特徴量数）である。\n",
    "\n",
    "入力が2チャンネル、出力が3チャンネルの例です。計算グラフを書いた上で、バックプロパゲーションも手計算で考えてみましょう。計算グラフの中には和と積しか登場しないので、微分を新たに考える必要はありません。\n",
    "\n",
    "\n",
    "《補足》\n",
    "\n",
    "\n",
    "チャンネル数を加える場合、配列をどういう順番にするかという問題があります。(バッチサイズ、チャンネル数、特徴量数)または(バッチサイズ、特徴量数、チャンネル数)が一般的で、ライブラリによって順番は異なっています。（切り替えて使用できるものもあります）\n",
    "\n",
    "\n",
    "今回のスクラッチでは自身の実装上どちらが効率的かを考えて選んでください。上記の例ではバッチサイズは考えておらず、(チャンネル数、特徴量数)です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[[1, 2, 3, 4], [2, 3, 4, 5]]])\n",
    "W = np.ones((3, 2, 3))\n",
    "b = np.array([1, 2, 3])\n",
    "a = np.array([[16, 22], [17, 23], [18, 24]]) \n",
    "\n",
    "\n",
    "fc = SimpleConv1d(W=W,b=b)\n",
    "te = fc.forward(X)\n",
    "print(te)\n",
    "\n",
    "bk = fc.backward(te)\n",
    "print(bk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】（アドバンス課題）パディングの実装\n",
    "\n",
    "畳み込み層にパディングの機能を加えてください。1次元配列の場合、前後にn個特徴量を増やせるようにしてください。\n",
    "\n",
    "\n",
    "最も単純なパディングは全て0で埋める ゼロパディング であり、CNNでは一般的です。他に端の値を繰り返す方法などもあります。\n",
    "\n",
    "\n",
    "フレームワークによっては、元の入力のサイズを保つようにという指定をすることができます。この機能も持たせておくと便利です。なお、NumPyにはパディングの関数が存在します。\n",
    "\n",
    "\n",
    "[numpy.pad — NumPy v1.17 Manual](https://numpy.org/doc/stable/reference/generated/numpy.pad.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = SimpleConv1d(W=W,b=b,stride=1,pad=1)\n",
    "te1 = pd.forward(X)\n",
    "print(te1)\n",
    "te2 = pd.backward(te1)\n",
    "print(te2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】（アドバンス課題）ミニバッチへの対応\n",
    "\n",
    "ここまでの課題はバッチサイズ1で良いとしてきました。しかし、実際は全結合層同様にミニバッチ学習が行われます。Conv1dクラスを複数のデータが同時に計算できるように変更してください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題１で実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題7】（アドバンス課題）任意のストライド数\n",
    "\n",
    "ストライドは1限定の実装をしてきましたが、任意のストライド数に対応できるようにしてください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = SimpleConv1d(W=W,b=b,stride=3,pad=0)\n",
    "te3 = pd.forward(X)\n",
    "print(te3)\n",
    "te4 = pd.backward(te3)\n",
    "print(te4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題8】学習と推定\n",
    "\n",
    "これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えてMNISTを学習・推定し、Accuracyを計算してください。\n",
    "\n",
    "\n",
    "出力層だけは全結合層をそのまま使ってください。ただし、チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、 平滑化 を行なってください。\n",
    "\n",
    "\n",
    "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "        \"\"\"\n",
    "        ミニバッチを取得するイテレータ\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          学習データ\n",
    "        y : 次の形のndarray, shape (n_samples, 1)\n",
    "          正解値\n",
    "        batch_size : int\n",
    "          バッチサイズ\n",
    "        seed : int\n",
    "          NumPyの乱数のシード\n",
    "        \"\"\"\n",
    "        def __init__(self, X, y, batch_size=10, seed=0):\n",
    "                self.batch_size = batch_size\n",
    "                np.random.seed(seed)\n",
    "                shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "                self.X = X[shuffle_index]\n",
    "                self.y = y[shuffle_index]\n",
    "                self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "        def __len__(self):\n",
    "                return self._stop\n",
    "\n",
    "        def __getitem__(self,item):\n",
    "                p0 = item*self.batch_size\n",
    "                p1 = item*self.batch_size + self.batch_size\n",
    "                return self.X[p0:p1], self.y[p0:p1]        \n",
    "\n",
    "        def __iter__(self):\n",
    "                self._counter = 0\n",
    "                return self\n",
    "\n",
    "        def __next__(self):\n",
    "                if self._counter >= self._stop:\n",
    "                        raise StopIteration()\n",
    "                p0 = self._counter*self.batch_size\n",
    "                p1 = self._counter*self.batch_size + self.batch_size\n",
    "                self._counter += 1\n",
    "                return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = np.copy(x)\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx\n",
    "\n",
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x - np.max(x, axis=1, keepdims=True)\n",
    "        #softmax\n",
    "        out = np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "\n",
    "        batch_size = dout.shape[0]\n",
    "        dx = (dout) / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     8,
     23
    ]
   },
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    #stochastic gradient descent\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n",
    "            \n",
    "class AdaGrad():\n",
    "    #AdaGrad\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "    def update(self, params, grads):\n",
    "        if (self.h is None):\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "            \n",
    "\n",
    "class Adam():\n",
    "    #Adam\n",
    "    def __init__(self, lr=0.01, beta1=0.9, beta2=0.999):\n",
    "        self.lr =lr\n",
    "        self.beta1=beta1\n",
    "        self.beta2=beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "    def update(self, params, grads):\n",
    "        if (self.m is None):\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2 ** self.iter) / (1.0 - self.beta1 **self.iter)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key] ** 2 - self.v[key])\n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        self.x = x\n",
    "        out = self.x @ self.W + self.b\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout @ self.W.T\n",
    "        self.dW = self.x.T @ dout\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx\n",
    "\n",
    "class Flatten():\n",
    "    def __init__(self):\n",
    "        self.prev_layer_shape = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.prev_layer_shape = x.shape\n",
    "        #(N, C, H, W)を(N, C*H*W)に\n",
    "        return x.reshape((x.shape[0], -1))\n",
    "\n",
    "    def backward(self, dout):\n",
    "        #(N, C*H*W)を(N, C, H, W)に\n",
    "        return (dout.reshape(self.prev_layer_shape))\n",
    "    \n",
    "class MaxPooling1D():\n",
    "    def __init__(self, pool_size, stride=1, pad=0):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.x = None\n",
    "        self.arg_max=None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, L = x.shape #batch_size, n_channels, n_features\n",
    "        out_size = get_output_size(L, self.pool_size, stride=self.stride, pad=self.pad)\n",
    "\n",
    "        col = im2col_1d(x, self.pool_size, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_size)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_size, C).transpose(0, 2, 1)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 1)\n",
    "        dmax = np.zeros((dout.size, self.pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (self.pool_size, ))\n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[2], -1)\n",
    "        dx = col2im_1d(col=dcol, input_shape=self.x.shape, filter_size=self.pool_size, stride=self.stride, pad=self.pad)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Scratch1dCNNClassifier():\n",
    "    def __init__(\n",
    "        self, conv_param={'n_filters': 30, 'filter_size': 3, 'stride': 1, 'pad': 0},\n",
    "        pool_param={'pool_size': 2},\n",
    "        n_epochs=5, batch_size=100, optimizer='Adam',\n",
    "        optimizer_param={'lr': 0.001},\n",
    "        layer_nodes = {'hidden': 100, 'output': 10},\n",
    "        weight_init_std=0.01,\n",
    "        verbose=True\n",
    "    ):\n",
    "        self.conv_param = conv_param\n",
    "        self.pool_param = pool_param\n",
    "        self.layer_nodes = layer_nodes\n",
    "        self.weight_init_std = weight_init_std\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "        optimizer_class_dict = {'sgd': SGD, 'adagrad': AdaGrad, 'adam': Adam}\n",
    "        #**kwargsでdictで引数をまとめて受け取っている\n",
    "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
    "\n",
    "        self.train_loss_list =[]\n",
    "        self.train_acc_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.val_acc_list = []\n",
    "\n",
    "    def fit(self, x_train, y_train, x_val=None, y_val=None):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "        #layerを生成\n",
    "        self._gen_layers()\n",
    "        #epoch数だけ学習\n",
    "        for epoch in range(self.n_epochs):\n",
    "            self._train()\n",
    "            print(\"epoch: \" + str(epoch))\n",
    "            #verbose=Trueなら学習中のlossなど計算して表示\n",
    "            if (self.verbose):\n",
    "                self._calc_loss_acc()\n",
    "                print(\"train_acc: \" + str(self.train_acc_list[epoch]) + \", val_acc\" + str(self.val_acc_list[epoch]))\n",
    "                print(\"train loss: \" + str(self.train_loss_list[epoch]) + \", val_loss\" + str(self.val_loss_list[epoch]) )\n",
    "        return self.train_loss_list, self.train_loss_list\n",
    "\n",
    "    def predict(self, x):\n",
    "        proba = self._propagate_forward(x)\n",
    "        return np.argmax(proba, axis=1)\n",
    "\n",
    "    def _gen_layers(self):\n",
    "        \"\"\"\n",
    "        x_train: ndarray of shape(n_samples, n_channels, n_features)\n",
    "        \"\"\"\n",
    "        self.n_train_samples, n_channels, n_features = self.x_train.shape\n",
    "        n_filters = self.conv_param['n_filters']\n",
    "        filter_size = self.conv_param['filter_size']\n",
    "        filter_stride = self.conv_param['stride']\n",
    "        filter_pad = self.conv_param['pad']\n",
    "        pool_size = self.pool_param['pool_size']\n",
    "\n",
    "        conv_output_size = get_output_size(n_features, filter_size, filter_stride, filter_pad)\n",
    "        pool_output_size = int(n_filters * conv_output_size/ pool_size)\n",
    "\n",
    "        #initialize hyper parameters\n",
    "        self.params ={}\n",
    "        self.params['W1'] = self.weight_init_std * np.random.randn(n_filters, n_channels, filter_size)\n",
    "        self.params['b1'] = np.zeros(n_filters)\n",
    "        self.params['W2'] = self.weight_init_std * np.random.randn(pool_output_size, self.layer_nodes['hidden'])\n",
    "        self.params['b2'] = np.zeros(self.layer_nodes['hidden'])\n",
    "        self.params['W3'] = self.weight_init_std * np.random.randn(self.layer_nodes['hidden'], self.layer_nodes['output'])\n",
    "        self.params['b3'] =  np.zeros(self.layer_nodes['output'])\n",
    "\n",
    "        #generate layers\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = SimpleConv1d(self.params['W1'], self.params['b1'], filter_stride, filter_pad)\n",
    "        self.layers['Relu1'] = ReLU()\n",
    "        self.layers['Pool1'] = MaxPooling1D(pool_size=pool_size, stride=pool_size)\n",
    "        self.layers['Flatten1'] = Flatten()\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = ReLU()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.layers['Last'] = Softmax()\n",
    "\n",
    "        #gradients\n",
    "        self.grads = {}\n",
    "\n",
    "    def _train(self):\n",
    "        mini_batch = GetMiniBatch(X=self.x_train, y=self.y_train, batch_size=self.batch_size, seed=0)\n",
    "\n",
    "        for mini_x, mini_y in mini_batch:\n",
    "            #forward\n",
    "            z = self._propagate_forward(mini_x)\n",
    "            #backward\n",
    "            self._propagate_backward(z - mini_y)\n",
    "            #gradient更新\n",
    "            self.optimizer.update(self.params, self.grads)\n",
    "        return\n",
    "\n",
    "    def _loss(self, y_actual, pred_proba):\n",
    "        return -(y_actual * np.log(pred_proba + 1e-7)).sum() / y_actual.shape[0]\n",
    "\n",
    "    def _accuracy(self, y_actual, pred_proba):\n",
    "        y_actual = np.argmax(y_actual, axis=1)\n",
    "        pred = np.argmax(pred_proba, axis=1)\n",
    "        acc = np.sum( y_actual == pred) / y_actual.shape[0]\n",
    "        return acc\n",
    "\n",
    "    def _propagate_forward(self, x):\n",
    "        #forward\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def _propagate_backward(self, dout):\n",
    "        #backward\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        self.grads['W1'], self.grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        self.grads['W2'], self.grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        self.grads['W3'], self.grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        #返却値なし\n",
    "        return\n",
    "    def _calc_loss_acc(self):\n",
    "        proba = self._propagate_forward(self.x_train)\n",
    "        #loss計算\n",
    "        loss = self._loss(self.y_train, proba)\n",
    "        self.train_loss_list.append(loss)\n",
    "        #accuracy計算\n",
    "        train_acc = self._accuracy(self.y_train, proba)\n",
    "        self.train_acc_list.append(train_acc)\n",
    "\n",
    "        if((self.x_val is not None) & (self.y_val is not None)):\n",
    "            proba = self._propagate_forward(self.x_val)\n",
    "            #loss計算\n",
    "            val_loss = self._loss(self.y_val, proba)\n",
    "            self.val_loss_list.append(loss)\n",
    "            #accuracy計算\n",
    "            val_acc = self._accuracy(self.y_val, proba)\n",
    "            self.val_acc_list.append(val_acc)\n",
    "        #返却値なし\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 1, 784)\n",
      "(6000, 1, 784)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#data数を絞る\n",
    "X_train = X_train[:30000]\n",
    "y_train = y_train[:30000]\n",
    "X_test = X_test[:500]\n",
    "y_test = y_test[:500]\n",
    "\n",
    "#reshape(N, 28, 28) -> (N, 1, 784)\n",
    "X_train = X_train.reshape(-1, 1, X_train.shape[1]*X_train.shape[2])\n",
    "X_test = X_test.reshape(-1, 1, X_test.shape[1]*X_test.shape[2])\n",
    "\n",
    "#normalize\n",
    "X_train = X_train.astype(np.float)/255\n",
    "X_test = X_test.astype(np.float)/255\n",
    "X_train.shape\n",
    "#one-hot\n",
    "eye = np.eye(len(np.unique(y_train)))\n",
    "y_train = eye[y_train]\n",
    "y_test = eye[y_test]\n",
    "\n",
    "#split into train, val\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape) \n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "train_acc: 0.7156666666666667, val_acc0.7238333333333333\n",
      "train loss: 1.3920215719357045, val_loss1.3920215719357045\n",
      "epoch: 1\n",
      "train_acc: 0.8407083333333333, val_acc0.8406666666666667\n",
      "train loss: 0.5195759020137002, val_loss0.5195759020137002\n",
      "epoch: 2\n",
      "train_acc: 0.893875, val_acc0.8886666666666667\n",
      "train loss: 0.3769295664763949, val_loss0.3769295664763949\n"
     ]
    }
   ],
   "source": [
    "cnn = Scratch1dCNNClassifier(\n",
    "    conv_param={'n_filters': 30, 'filter_size': 5, 'stride': 1, 'pad': 0},\n",
    "    pool_param={'pool_size': 2},\n",
    "    n_epochs=3,\n",
    "    batch_size=1000,\n",
    "    ba='Adam',\n",
    "    optimizer_param={'lr': 0.001},\n",
    "    layer_nodes = {'hidden': 100, 'output': 10},\n",
    "    weight_init_std=0.01,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "cnn.fit(x_train=X_train, y_train=y_train, x_val=X_val, y_val=y_val)\n",
    "pred = cnn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acuracy:  0.878\n"
     ]
    }
   ],
   "source": [
    "y_actual = np.argmax(y_test, axis=1)\n",
    "print(\"acuracy: \", (pred == y_actual).sum() / len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチサイズ1000、optimizerをAdamに変更することで改善"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
