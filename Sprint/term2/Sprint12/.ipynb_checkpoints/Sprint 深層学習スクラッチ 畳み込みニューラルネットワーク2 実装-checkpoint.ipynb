{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの用意\n",
    "\n",
    "引き続きMNISTデータセットを使用します。2次元畳み込み層へは、28×28の状態で入力します。\n",
    "\n",
    "\n",
    "今回は白黒画像ですからチャンネルは1つしかありませんが、チャンネル方向の軸は用意しておく必要があります。\n",
    "\n",
    "\n",
    "(n_samples, n_channels, height, width)のNCHWまたは(n_samples, height, width, n_channels)のNHWCどちらかの形にしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】2次元畳み込み層の作成\n",
    "\n",
    "1次元畳み込み層のクラスConv1dを発展させ、2次元畳み込み層のクラスConv2dを作成してください。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。\n",
    "\n",
    "$$ a_{i,j,m} = \\sum_{k=0}^{K-1}\\sum_{s=0}^{F_{h}-1}\\sum_{t=0}^{F_{w}-1}x_{(i+s),(j+t),k}w_{s,t,k,m}+b_{m} $$\n",
    "\n",
    "$a_{i,j,m}$ : 出力される配列のi行j列、mチャンネルの値\n",
    "\n",
    "$\n",
    "i\n",
    "$ : 配列の行方向のインデックス\n",
    "\n",
    "$\n",
    "j\n",
    "$ : 配列の列方向のインデックス\n",
    "\n",
    "$\n",
    "m\n",
    "$ : 出力チャンネルのインデックス\n",
    "\n",
    "$\n",
    "K\n",
    "$ : 入力チャンネル数\n",
    "\n",
    "$F_{h}, F_{w}$ : 高さ方向（h）と幅方向（w）のフィルタのサイズ\n",
    "\n",
    "$\n",
    "x\n",
    "(\n",
    "i\n",
    "+\n",
    "s\n",
    ")\n",
    ",\n",
    "(\n",
    "j\n",
    "+\n",
    "t\n",
    ")\n",
    ",\n",
    "k\n",
    "$ : 入力の配列の(i+s)行(j+t)列、kチャンネルの値\n",
    "\n",
    "$\n",
    "w\n",
    "s\n",
    ",\n",
    "t\n",
    ",\n",
    "k\n",
    ",\n",
    "m\n",
    "$ : 重みの配列のs行t列目。kチャンネルの入力に対して、mチャンネルへ出力する重み\n",
    "\n",
    "$\n",
    "b\n",
    "m\n",
    "$ : mチャンネルへの出力のバイアス項\n",
    "\n",
    "\n",
    "全てスカラーです。\n",
    "\n",
    "\n",
    "次に更新式です。1次元畳み込み層や全結合層と同じ形です。\n",
    "\n",
    "\n",
    "\n",
    "$$ w_{s,t,k,m}^{\\prime} = w_{s,t,k,m} - \\alpha \\frac{\\partial L}{\\partial w_{s,t,k,m}} \\\\\n",
    "b_{m}^{\\prime} = b_{m} - \\alpha \\frac{\\partial L}{\\partial b_{m}} $$\n",
    "\n",
    "$\n",
    "α\n",
    "$ : 学習率\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_{s,t,k,m}}$ \n",
    "$\n",
    "w\n",
    "s\n",
    ",\n",
    "t\n",
    ",\n",
    "k\n",
    ",\n",
    "m\n",
    "$ に関する損失 $\n",
    "L\n",
    "$ の勾配\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b_{m}}$: \n",
    "$\n",
    "b\n",
    "m\n",
    "$ に関する損失 $\n",
    "L\n",
    "$ の勾配\n",
    "\n",
    "\n",
    "勾配 $\\frac{\\partial L}{\\partial w_{s,t,k,m}}$ や　$\\frac{\\partial L}{\\partial b_{m}}$ を求めるためのバックプロパゲーションの数式が以下である。\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w_{s,t,k,m}} = \\sum_{i=0}^{N_{out,h}-1}\\sum_{j=0}^{N_{out,w}-1} \\frac{\\partial L}{\\partial a_{i,j,m}}x_{(i+s)(j+t),k}\\\\\n",
    "\\frac{\\partial L}{\\partial b_{m}} = \\sum_{i=0}^{N_{out,h}-1}\\sum_{j=0}^{N_{out,w}-1}\\frac{\\partial L}{\\partial a_{i,j,m}} $$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a_i}$: 勾配の配列のi行j列、mチャンネルの値\n",
    "$\n",
    "N\n",
    "o\n",
    "u\n",
    "t\n",
    ",\n",
    "h$\n",
    ",$\n",
    "N\n",
    "o\n",
    "u\n",
    "t\n",
    "$,$\n",
    "w\n",
    "$ : 高さ方向（h）と幅方向（w）の出力のサイズ\n",
    "\n",
    "\n",
    "前の層に流す誤差の数式は以下です。\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial x_{i,j,k}} = \\sum_{m=0}^{M-1}\\sum_{s=0}^{F_{h}-1}\\sum_{t=0}^{F_{w}-1} \\frac{\\partial L}{\\partial a_{(i-s),(j-t),m}}w_{s,t,k,m}$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_{i,j,k}}$: 前の層に流す誤差の配列のi列j行、kチャンネルの値\n",
    "\n",
    "$\n",
    "M\n",
    "$ : 出力チャンネル数\n",
    "\n",
    "\n",
    "ただし、 $\n",
    "i\n",
    "−\n",
    "s\n",
    "<\n",
    "0\n",
    "$ または $\n",
    "i\n",
    "−\n",
    "s\n",
    ">\n",
    "N\n",
    "o\n",
    "u\n",
    "t\n",
    ",\n",
    "h\n",
    "−\n",
    "1\n",
    "$ または $\n",
    "j\n",
    "−\n",
    "t\n",
    "<\n",
    "0\n",
    " $または $\n",
    "j\n",
    "−\n",
    "t\n",
    ">\n",
    "N\n",
    "o\n",
    "u\n",
    "t\n",
    ",\n",
    "w\n",
    "−\n",
    "1\n",
    "$ のとき $\\frac{\\partial L}{\\partial a_{(i-s),(j-t),m}} =0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[参考](https://qiita.com/daizutabi/items/856042fb1ea9da150741)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(3, 1, 8, 8)\n",
    "W = np.random.rand(5, 1, 7, 7)\n",
    "b = np.array([3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "ss_str = \"../../term2/common\"\n",
    "if (ss_str not in sys.path):\n",
    "    sys.path.append(ss_str)\n",
    "    \n",
    "from layer import Conv2d, MaxPooling, Flatten, Affine\n",
    "from utils import output_size, imcol2, col2im\n",
    "from optimizer import AdaGrad, SGD\n",
    "from mini_batch import GetMiniBatch\n",
    "from activ import ReLU, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5, 2, 2)\n",
      "(3, 1, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "cov2 = Conv2d(W=W, b=b,stride=1,pad=0)\n",
    "s = cov2.forward(X)\n",
    "print(s.shape)\n",
    "d = cov2.backward(s)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】2次元畳み込み後の出力サイズ\n",
    "\n",
    "畳み込みを行うと特徴マップのサイズが変化します。どのように変化するかは以下の数式から求められます。この計算を行う関数を作成してください\n",
    "\n",
    "$$ N_{h,out} =  \\frac{N_{h,in}+2P_{h}-F_{h}}{S_{h}} + 1\\\\\n",
    "N_{w,out} =  \\frac{N_{w,in}+2P_{w}-F_{w}}{S_{w}} + 1 $$\n",
    "\n",
    "$\n",
    "N\n",
    "o\n",
    "u\n",
    "t\n",
    "$ : 出力のサイズ（特徴量の数）\n",
    "\n",
    "$\n",
    "N\n",
    "i\n",
    "n\n",
    "$ : 入力のサイズ（特徴量の数）\n",
    "\n",
    "$\n",
    "P\n",
    " $: ある方向へのパディングの数\n",
    "\n",
    "\n",
    "$F$\n",
    " : フィルタのサイズ\n",
    "\n",
    "\n",
    "$S$\n",
    " : ストライドのサイズ\n",
    "\n",
    "\n",
    "$h$\n",
    " が高さ方向、 \n",
    "$w$\n",
    " が幅方向である"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = 9\n",
    "FH = 5\n",
    "\n",
    "osH = output_size(H,FH)\n",
    "\n",
    "osH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = 8\n",
    "FW = 2\n",
    "\n",
    "osW = output_size(W,FW)\n",
    "\n",
    "osW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】最大プーリング層の作成\n",
    "\n",
    "最大プーリング層のクラスMaxPool2Dを作成してください。プーリング層は数式で表さない方が分かりやすい部分もありますが、数式で表すとフォワードプロパゲーションは以下のようになります。\n",
    "\n",
    "$$ a_{i,j,k} = \\max_{(p,q)\\in P_{i,j}}x_{p,q,k} $$\n",
    "\n",
    "$\n",
    "P\n",
    "i\n",
    ",\n",
    "j\n",
    "$ : i行j列への出力する場合の入力配列のインデックスの集合。$ \n",
    "S\n",
    "h\n",
    "×\n",
    "S\n",
    "w\n",
    "$ の範囲内の行$（p）$と列$（q）$\n",
    "\n",
    "$\n",
    "S\n",
    "h\n",
    ",\n",
    "S\n",
    "w\n",
    "$ : 高さ方向$（h）$と幅方向$（w）$のストライドのサイズ\n",
    "\n",
    "$\n",
    "(\n",
    "p\n",
    ",\n",
    "q\n",
    ")\n",
    "∈\n",
    "P\n",
    "i\n",
    ",\n",
    "j\n",
    " : \n",
    "P\n",
    "i\n",
    ",\n",
    "j\n",
    "$ に含まれる行$（p）$と列$（q）$のインデックス\n",
    "\n",
    "$\n",
    "a\n",
    "i\n",
    ",\n",
    "j\n",
    ",\n",
    "m\n",
    "$ : 出力される配列のi行j列、kチャンネルの値\n",
    "\n",
    "$\n",
    "x\n",
    "p\n",
    ",\n",
    "q\n",
    ",\n",
    "k\n",
    "$ : 入力の配列のp行q列、kチャンネルの値\n",
    "\n",
    "\n",
    "ある範囲の中でチャンネル方向の軸は残したまま最大値を計算することになります。\n",
    "\n",
    "\n",
    "バックプロパゲーションのためには、フォワードプロパゲーションのときの最大値のインデックス $\n",
    "(\n",
    "p\n",
    ",\n",
    "q\n",
    ")\n",
    "$ を保持しておく必要があります。フォワード時に最大値を持っていた箇所にそのままの誤差を流し、そこ以外には0を入れるためです。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 6, 6)\n",
      "(3, 1, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "max2 = MaxPooling(3,3)\n",
    "s = max2.forward(X)\n",
    "print(s.shape)\n",
    "d = max2.backward(s)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】（アドバンス課題）平均プーリングの作成\n",
    "\n",
    "平均プーリング層のクラスAveragePool2Dを作成してください。\n",
    "\n",
    "\n",
    "範囲内の最大値ではなく、平均値を出力とするプーリング層です。\n",
    "\n",
    "\n",
    "画像認識関係では最大プーリング層が一般的で、平均プーリングはあまり使われません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class AvePooling:\n",
    "        \"\"\"\n",
    "        pool_h (int): プーリング領域の高さ\n",
    "        pool_w (int): プーリング領域の幅\n",
    "        stride (int, optional): ストライド、デフォルトは1。\n",
    "        pad (int, optional): パディング、デフォルトは0。\n",
    "        \"\"\" \n",
    "        def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "            self.pool_h = pool_h\n",
    "            self.pool_w = pool_w\n",
    "            self.stride = stride\n",
    "            self.pad = pad\n",
    "            self.X = None\n",
    "            self.arg_max=None\n",
    "\n",
    "        def forward(self, X):\n",
    "            \"\"\"\n",
    "            x (numpy.ndarray): 入力、形状は(N, C, H, W)。\n",
    "\n",
    "            Returns:\n",
    "                numpy.ndarray: 出力、形状は(N, C, OH, OW)。\n",
    "            \"\"\"\n",
    "\n",
    "            N, C, H, W = X.shape\n",
    "\n",
    "            OH = output_size(H, self.pool_h, self.stride, self.pad)\n",
    "            OW = output_size(W, self.pool_w, self.stride, self.pad)\n",
    "\n",
    "            # (N, C, H, W) → (N * OH * OW, C * PH * PW)\n",
    "            col = imcol2(X, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "\n",
    "            # (N * OH * OW, C * PH * PW) → (N * OH * OW * C, PH * PW)\n",
    "            col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "\n",
    "             # 最大値の位置（インデックス）\n",
    "            arg_max = np.argmax(col, axis=1)\n",
    "\n",
    "            # (N * OH * OW * C, PH * PW) → (N * OH * OW * C)\n",
    "            out = np.min(col, axis=1)\n",
    "\n",
    "            # (N * OH * OW * C) → (N, OH, OW, C) → (N, C, OH, OW)\n",
    "            out = out.reshape(N, OH, OW, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "            self.X = X\n",
    "            self.arg_max = arg_max\n",
    "            return out\n",
    "\n",
    "\n",
    "        def backward(self, dout):\n",
    "            \"\"\" \n",
    "            dout (numpy.ndarray): 右の層から伝わってくる微分値、形状は(N, C, OH, OW)。\n",
    "\n",
    "            Returns:\n",
    "                numpy.ndarray: 微分値（勾配）、形状は(N, C, H, W)。\n",
    "            \"\"\"\n",
    "             # (N, C, OH, OW) → (N, OH, OW, C)\n",
    "            dout = dout.transpose(0, 2, 3, 1)\n",
    "\n",
    "            # (N * OH * OW * C, PH * PW)\n",
    "            pool_size = self.pool_h * self.pool_w\n",
    "            dmax =dout / pool_size\n",
    "\n",
    "            return dmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 6, 6)\n",
      "(3, 6, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "a = AvePooling(3,3)\n",
    "af = a.forward(X)\n",
    "print(af.shape)\n",
    "ab = a.backward(af)\n",
    "print(ab.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】平滑化\n",
    "\n",
    "平滑化するためのFlattenクラスを作成してください。\n",
    "\n",
    "\n",
    "フォワードのときはチャンネル、高さ、幅の3次元を1次元にreshapeします。その値は記録しておき、バックワードのときに再びreshapeによって形を戻します。\n",
    "\n",
    "\n",
    "この平滑化のクラスを挟むことで出力前の全結合層に適した配列を作ることができます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 64)\n",
      "(3, 1, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "f2 = Flatten()\n",
    "\n",
    "fw = f2.forward(X)\n",
    "print(fw.shape)\n",
    "\n",
    "fb = f2.backward(fw)\n",
    "print(fb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1×8×8 = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】学習と推定\n",
    "\n",
    "作成したConv2dを使用してMNISTを学習・推定し、Accuracyを計算してください。\n",
    "\n",
    "\n",
    "精度は低くともまずは動くことを目指してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     26,
     49,
     84,
     91,
     96,
     100,
     107,
     119
    ]
   },
   "outputs": [],
   "source": [
    "class Scratch2d():\n",
    "    def __init__(\n",
    "        self, conv_param={'n_filters': 30, 'filter_size': 3, 'stride': 1, 'pad': 0},\n",
    "        pool_param={'pool_size': 2},\n",
    "        n_epochs=5, batch_size=100, optimizer='AdaGrad',\n",
    "        optimizer_param={'lr': 0.001},\n",
    "        layer_nodes = {'hidden': 100, 'output': 10},\n",
    "        weight_init_std=0.01,\n",
    "        verbose=True\n",
    "    ):\n",
    "        self.conv_param = conv_param\n",
    "        self.pool_param = pool_param\n",
    "        self.layer_nodes = layer_nodes\n",
    "        self.weight_init_std = weight_init_std\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "        optimizer_class_dict = {'sgd': SGD,'adagrad': AdaGrad}\n",
    "\n",
    "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
    "\n",
    "        self.train_loss_list =[]\n",
    "        self.train_acc_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.val_acc_list = []\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self._gen_layers()\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            self._train()\n",
    "            print(\"epoch: \" + str(epoch))\n",
    "            #verbose=Trueなら学習中のlossなど計算して表示\n",
    "            if (self.verbose):\n",
    "                self._calc_loss_acc()\n",
    "                print(\"train_acc: \" + str(self.train_acc_list[epoch]) + \", val_acc\" + str(self.val_acc_list[epoch]))\n",
    "                print(\"train loss: \" + str(self.train_loss_list[epoch]) + \", val_loss\" + str(self.val_loss_list[epoch]) )\n",
    "        return self.train_loss_list, self.train_loss_list\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self._propagate_forward(X)\n",
    "        return np.argmax(proba, axis=1)\n",
    "\n",
    "    def _gen_layers(self):\n",
    "        \"\"\"\n",
    "        x_train: ndarray of shape(n_samples, n_channels, height, width)\n",
    "        \"\"\"\n",
    "        self.n_train_samples, n_channels, input_size, _ = self.X_train.shape\n",
    "        n_filters = self.conv_param['n_filters']\n",
    "        filter_size = self.conv_param['filter_size']\n",
    "        filter_stride = self.conv_param['stride']\n",
    "        filter_pad = self.conv_param['pad']\n",
    "        pool_size = self.pool_param['pool_size']\n",
    "\n",
    "        conv_output_size = output_size(input_size, filter_size, filter_stride, filter_pad)\n",
    "        pool_output_size = int(n_filters * np.power(conv_output_size/ pool_size, 2))\n",
    "\n",
    "        self.params ={}\n",
    "        self.params['W1'] = self.weight_init_std * np.random.randn(n_filters, n_channels, filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(n_filters)\n",
    "        self.params['W2'] = self.weight_init_std * np.random.randn(pool_output_size, self.layer_nodes['hidden'])\n",
    "        self.params['b2'] = np.zeros(self.layer_nodes['hidden'])\n",
    "        self.params['W3'] = self.weight_init_std * np.random.randn(self.layer_nodes['hidden'], self.layer_nodes['output'])\n",
    "        self.params['b3'] =  np.zeros(self.layer_nodes['output'])\n",
    "\n",
    "        self.layers = {}\n",
    "        self.layers['Conv1'] = Conv2d(self.params['W1'], self.params['b1'], filter_stride, filter_pad)\n",
    "        self.layers['Relu1'] = ReLU()\n",
    "        self.layers['Pool1'] = MaxPooling(pool_h=pool_size, pool_w=pool_size, stride=pool_size)\n",
    "        self.layers['Flatten1'] = Flatten()\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = ReLU()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.layers['Last'] = Softmax()\n",
    "\n",
    "        #gradients\n",
    "        self.grads = {}\n",
    "\n",
    "    def _train(self):\n",
    "        mini_batch = GetMiniBatch(X=self.X_train, y=self.y_train, batch_size=self.batch_size, seed=0)\n",
    "\n",
    "        for mini_x, mini_y in mini_batch:\n",
    "            #forward\n",
    "            z = self._propagate_forward(mini_x)\n",
    "            #backward\n",
    "            self._propagate_backward(z - mini_y)\n",
    "            #gradient更新\n",
    "            self.optimizer.update(self.params, self.grads)\n",
    "        return\n",
    "\n",
    "    def _loss(self, y_actual, pred_proba):\n",
    "        return -(y_actual * np.log(pred_proba + 1e-7)).sum() / y_actual.shape[0]\n",
    "\n",
    "    def _accuracy(self, y_actual, pred_proba):\n",
    "        y_actual = np.argmax(y_actual, axis=1)\n",
    "        pred = np.argmax(pred_proba, axis=1)\n",
    "        acc = np.sum( y_actual == pred) / y_actual.shape[0]\n",
    "        return acc\n",
    "\n",
    "    \n",
    "    def _propagate_forward(self, X):\n",
    "        #forward\n",
    "        for layer in self.layers.values():\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def _propagate_backward(self, dout):\n",
    "        #backward\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        self.grads['W1'], self.grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        self.grads['W2'], self.grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        self.grads['W3'], self.grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def _calc_loss_acc(self):\n",
    "        proba = self._propagate_forward(self.X_train)\n",
    "        #loss計算\n",
    "        loss = self._loss(self.y_train, proba)\n",
    "        self.train_loss_list.append(loss)\n",
    "        #accuracy計算\n",
    "        train_acc = self._accuracy(self.y_train, proba)\n",
    "        self.train_acc_list.append(train_acc)\n",
    "\n",
    "        if((self.X_val is not None) & (self.y_val is not None)):\n",
    "            proba = self._propagate_forward(self.X_val)\n",
    "            #loss計算\n",
    "            val_loss = self._loss(self.y_val, proba)\n",
    "            self.val_loss_list.append(loss)\n",
    "            #accuracy計算\n",
    "            val_acc = self._accuracy(self.y_val, proba)\n",
    "            self.val_acc_list.append(val_acc)\n",
    "        #返却値なし\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 1, 28, 28)\n",
      "(12000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#データセットの用意\n",
    "from keras.datasets import mnist\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#reshape\n",
    "X_train = X_train.reshape(len(X_train), 1, 28, 28)\n",
    "X_test = X_test.reshape(len(X_test), 1, 28, 28)\n",
    "#one-hot\n",
    "eye = np.eye(len(np.unique(y_train)))\n",
    "y_train = eye[y_train]\n",
    "y_test = eye[y_test]\n",
    "\n",
    "#normalize\n",
    "X_train = X_train.astype(np.float)/255\n",
    "X_test = X_test.astype(np.float)/255\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "y_train = y_train.astype(int)\n",
    "y_val = y_val.astype(int)\n",
    "print(X_train.shape) \n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 6.18 GiB for an array with shape (48000, 30, 2, 2, 12, 12) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-1dcfe61ca1b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0ms2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-2db4094a83b5>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;31m#verbose=Trueなら学習中のlossなど計算して表示\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_calc_loss_acc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train_acc: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_acc_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\", val_acc\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_acc_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train loss: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\", val_loss\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_loss_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-2db4094a83b5>\u001b[0m in \u001b[0;36m_calc_loss_acc\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_calc_loss_acc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_propagate_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m         \u001b[1;31m#loss計算\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproba\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-2db4094a83b5>\u001b[0m in \u001b[0;36m_propagate_forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m#forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\git\\diveintocode-ml\\Sprint\\term2\\common\\layer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[1;31m# (N, C, H, W) → (N * OH * OW, C * PH * PW)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimcol2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;31m# (N * OH * OW, C * PH * PW) → (N * OH * OW * C, PH * PW)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\git\\diveintocode-ml\\Sprint\\term2\\common\\utils.py\u001b[0m in \u001b[0;36mimcol2\u001b[1;34m(input_data, filter_h, filter_w, stride, pad)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'constant'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 6.18 GiB for an array with shape (48000, 30, 2, 2, 12, 12) and data type float64"
     ]
    }
   ],
   "source": [
    "s2 = Scratch2d(\n",
    "    conv_param={'n_filters': 30, 'filter_size': 5, 'stride': 1, 'pad': 0},\n",
    "    pool_param={'pool_size': 2},\n",
    "    n_epochs=3,\n",
    "    batch_size=10,\n",
    "    optimizer='AdaGrad',\n",
    "    optimizer_param={'lr': 0.001},\n",
    "    layer_nodes = {'hidden': 100, 'output': 10},\n",
    "    weight_init_std=0.01,\n",
    "    verbose=True\n",
    ")\n",
    "s2.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val)\n",
    "pred = s2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題7】（アドバンス課題）LeNet\n",
    "\n",
    "CNNで画像認識を行う際は、フィルタサイズや層の数などを１から考えるのではなく、有名な構造を利用することが一般的です。現在では実用的に使われることはありませんが、歴史的に重要なのは1998年の LeNet です。この構造を再現してMNISTに対して動かし、Accuracyを計算してください。\n",
    "\n",
    "\n",
    "[Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)\n",
    "\n",
    "![](https://t.gyazo.com/teams/diveintocode/83358987a273743a589b9388dfdf59ac.png)\n",
    "\n",
    "※上記論文から引用\n",
    "\n",
    "\n",
    "サブサンプリングとは現在のプーリングに相当するものです。現代風に以下のように作ってみることにします。活性化関数も当時はシグモイド関数ですが、ReLUとします。\n",
    "\n",
    "\n",
    "1.畳み込み層　出力チャンネル数6、フィルタサイズ5×5、ストライド1\n",
    "\n",
    "2.ReLU\n",
    "\n",
    "3.最大プーリング\n",
    "\n",
    "4.畳み込み層　出力チャンネル数16、フィルタサイズ5×5、ストライド1\n",
    "\n",
    "5.ReLU\n",
    "\n",
    "6.最大プーリング\n",
    "\n",
    "\n",
    "7.平滑化\n",
    "\n",
    "8.全結合層　出力ノード数120\n",
    "\n",
    "9.ReLU\n",
    "\n",
    "10.全結合層　出力ノード数84\n",
    "\n",
    "11.ReLU\n",
    "\n",
    "12.全結合層　出力ノード数10\n",
    "\n",
    "13.ソフトマックス関数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題8】（アドバンス課題）有名な画像認識モデルの調査\n",
    "\n",
    "CNNの代表的な構造としてははAlexNet(2012)、VGG16(2014)などがあります。こういったものはフレームワークで既に用意されていることも多いです。\n",
    "\n",
    "\n",
    "どういったものがあるか簡単に調べてまとめてください。名前だけでも見ておくと良いでしょう。\n",
    "\n",
    "\n",
    "《参考》\n",
    "\n",
    "\n",
    "[Applications - Keras Documentation](https://keras.io/ja/applications/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題9】出力サイズとパラメータ数の計算\n",
    "\n",
    "CNNモデルを構築する際には、全結合層に入力する段階で特徴量がいくつになっているかを事前に計算する必要があります。\n",
    "\n",
    "\n",
    "また、巨大なモデルを扱うようになると、メモリや計算速度の関係でパラメータ数の計算は必須になってきます。フレームワークでは各層のパラメータ数を表示させることが可能ですが、意味を理解していなくては適切な調整が行えません。\n",
    "\n",
    "\n",
    "以下の3つの畳み込み層の出力サイズとパラメータ数を計算してください。パラメータ数についてはバイアス項も考えてください。\n",
    "\n",
    "\n",
    "1.\n",
    "\n",
    "\n",
    "・入力サイズ : 144×144, 3チャンネル\n",
    "・フィルタサイズ : 3×3, 6チャンネル\n",
    "・ストライド : 1\n",
    "・パディング : なし\n",
    "\n",
    "2.\n",
    "\n",
    "\n",
    "・入力サイズ : 60×60, 24チャンネル\n",
    "・フィルタサイズ : 3×3, 48チャンネル\n",
    "・ストライド　: 1\n",
    "・パディング : なし\n",
    "\n",
    "3.\n",
    "\n",
    "\n",
    "・入力サイズ : 20×20, 10チャンネル\n",
    "・フィルタサイズ: 3×3, 20チャンネル\n",
    "・ストライド : 2\n",
    "・パディング : なし\n",
    "\n",
    "＊最後の例は丁度良く畳み込みをすることができない場合です。フレームワークでは余ったピクセルを見ないという処理が行われることがあるので、その場合を考えて計算してください。端が欠けてしまうので、こういった設定は好ましくないという例です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題10】（アドバンス課題）フィルタサイズに関する調査\n",
    "\n",
    "畳み込み層にはフィルタサイズというハイパーパラメータがありますが、2次元畳み込み層において現在では3×3と1×1の使用が大半です。以下のそれぞれを調べたり、自分なりに考えて説明してください。\n",
    "\n",
    "\n",
    "・7×7などの大きめのものではなく、3×3のフィルタが一般的に使われる理由\n",
    "・高さや幅方向を持たない1×1のフィルタの効果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
