{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ディープニューラルネットワークスクラッチ\n",
    "\n",
    "前回は3層のニューラルネットワークを作成しましたが、今回はこれを任意の層数に拡張しやすいものに書き換えていきます。その上で、活性化関数や初期値、最適化手法について発展的なものを扱えるようにしていきます。\n",
    "\n",
    "\n",
    "このようなスクラッチを行うことで、今後各種フレームワークを利用していくにあたり、内部の動きが想像できることを目指します。\n",
    "\n",
    "\n",
    "名前は新しくScratchDeepNeuralNetrowkClassifierクラスとしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##層などのクラス化\n",
    "\n",
    "クラスにまとめて行くことで、構成を変更しやすい実装にしていきます。\n",
    "\n",
    "\n",
    "手を加える箇所\n",
    "\n",
    "\n",
    "・層の数\n",
    "\n",
    "・層の種類（今後畳み込み層など他のタイプの層が登場する）\n",
    "\n",
    "・活性化関数の種類\n",
    "\n",
    "・重みやバイアスの初期化方法\n",
    "\n",
    "・最適化手法\n",
    "\n",
    "そのために、全結合層、各種活性化関数、重みやバイアスの初期化、最適化手法それぞれのクラスを作成します。\n",
    "\n",
    "\n",
    "**実装方法は自由**ですが、簡単な例を紹介します。サンプルコード1のように全結合層と活性化関数のインスタンスを作成し、サンプルコード2,3のようにして使用します。それぞれのクラスについてはこのあと解説します。\n",
    "\n",
    "《サンプルコード1》\n",
    "\n",
    "\n",
    "ScratchDeepNeuralNetrowkClassifierのfitメソッド内\n",
    "\n",
    "    # self.sigma : ガウス分布の標準偏差\n",
    "    # self.lr : 学習率\n",
    "    # self.n_nodes1 : 1層目のノード数\n",
    "    # self.n_nodes2 : 2層目のノード数\n",
    "    # self.n_output : 出力層のノード数\n",
    "    optimizer = SGD(self.lr)\n",
    "    self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "    self.activation1 = Tanh()\n",
    "    self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "    self.activation2 = Tanh()\n",
    "    self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "    self.activation3 = Softmax()\n",
    "    \n",
    " \n",
    "《サンプルコード2》\n",
    "\n",
    "\n",
    "イテレーションごとのフォワード\n",
    "\n",
    "    A1 = self.FC1.forward(X)\n",
    "    Z1 = self.activation1.forward(A1)\n",
    "    A2 = self.FC2.forward(Z1)\n",
    "    Z2 = self.activation2.forward(A2)\n",
    "    A3 = self.FC3.forward(Z2)\n",
    "    Z3 = self.activation3.forward(A3)\n",
    "    \n",
    "    \n",
    " 《サンプルコード3》\n",
    "\n",
    "\n",
    "イテレーションごとのバックワード\n",
    "\n",
    "    dA3 = self.activation3.backward(Z3, Y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "    dZ2 = self.FC3.backward(dA3)\n",
    "    dA2 = self.activation2.backward(dZ2)\n",
    "    dZ1 = self.FC2.backward(dA2)\n",
    "    dA1 = self.activation1.backward(dZ1)\n",
    "    dZ0 = self.FC1.backward(dA1) # dZ0は使用しない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題1】全結合層のクラス化\n",
    "\n",
    "全結合層のクラス化を行なってください。\n",
    "\n",
    "\n",
    "以下に雛形を載せました。コンストラクタで重みやバイアスの初期化をして、あとはフォワードとバックワードのメソッドを用意します。重みW、バイアスB、およびフォワード時の入力Xをインスタンス変数として保持しておくことで、煩雑な入出力は不要になります。\n",
    "\n",
    "\n",
    "なお、インスタンスも引数として渡すことができます。そのため、初期化方法のインスタンスinitializerをコンストラクタで受け取れば、それにより初期化が行われます。渡すインスタンスを変えれば、初期化方法が変えられます。\n",
    "\n",
    "\n",
    "また、引数として自身のインスタンスselfを渡すこともできます。これを利用してself.optimizer.update(self)という風に層の重みの更新が可能です。更新に必要な値は複数ありますが、全て全結合層が持つインスタンス変数にすることができます。\n",
    "\n",
    "\n",
    "初期化方法と最適化手法のクラスについては後述します。\n",
    "\n",
    "\n",
    "《雛形》\n",
    "\n",
    "    class FC:\n",
    "        \"\"\"\n",
    "        ノード数n_nodes1からn_nodes2への全結合層\n",
    "        Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        pass\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        pass\n",
    "        return A\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        pass\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.b = initializer.B(n_nodes2)\n",
    "        self.X = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        out : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        self.X = X # 保持\n",
    "        out = np.dot(X, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"        \n",
    "        # 入力値の微分 = (逆伝播の)入力値 * 重みの転置\n",
    "        dX = np.dot(dA, self.W.T)\n",
    "        # 重みパラメータの微分 = 順伝播時の入力値の転置 * 逆伝播の入力値\n",
    "        self.dW = np.dot(self.X.T, dA)\n",
    "        # バイアスパラメータの微分 = 逆伝播の入力値の総和\n",
    "        self.db = np.sum(dA, axis=0)\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題2】初期化方法のクラス化\n",
    "\n",
    "初期化を行うコードをクラス化してください。\n",
    "\n",
    "\n",
    "前述のように、全結合層のコンストラクタに初期化方法のインスタンスを渡せるようにします。以下の雛形に必要なコードを書き加えていってください。標準偏差の値（sigma）はコンストラクタで受け取るようにすることで、全結合層のクラス内にこの値（sigma）を渡さなくてすむようになります。\n",
    "\n",
    "\n",
    "これまで扱ってきた初期化方法はSimpleInitializerクラスと名付けることにします。\n",
    "\n",
    "\n",
    "《雛形》\n",
    "\n",
    "    class SimpleInitializer:\n",
    "        \"\"\"\n",
    "        ガウス分布によるシンプルな初期化\n",
    "        Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        pass\n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        pass\n",
    "        return B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題3】最適化手法のクラス化\n",
    "\n",
    "最適化手法のクラス化を行なってください。\n",
    "\n",
    "\n",
    "最適化手法に関しても初期化方法同様に全結合層にインスタンスとして渡します。バックワードのときにself.optimizer.update(self)のように更新できるようにします。以下の雛形に必要なコードを書き加えていってください。\n",
    "\n",
    "\n",
    "これまで扱ってきた最適化手法はSGDクラス（Stochastic Gradient Descent、確率的勾配降下法）として作成します。\n",
    "\n",
    "\n",
    "雛形\n",
    "\n",
    "    class SGD:\n",
    "        \"\"\"\n",
    "        確率的勾配降下法\n",
    "        Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # dW = 重みパラメータの微分\n",
    "        # db = バイアスパラメータの微分\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.b -= self.lr * layer.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題4】活性化関数のクラス化\n",
    "\n",
    "活性化関数のクラス化を行なってください。\n",
    "\n",
    "\n",
    "ソフトマックス関数のバックプロパゲーションには交差エントロピー誤差の計算も含む実装を行うことで計算が簡略化されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##発展的要素\n",
    "\n",
    "活性化関数や重みの初期値、最適化手法に関してこれまで見てきた以外のものを実装していきます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     16,
     32,
     50
    ]
   },
   "outputs": [],
   "source": [
    "class Tangent():\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = np.tanh(X)\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dX = dout*(1 - self.out**2)\n",
    "        \n",
    "        return dX\n",
    "\n",
    "    \n",
    "class Sigmoid():\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = 1 / (1 + np.exp(-X))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dX = dout*(1.0 - self.out) * self.out\n",
    "        \n",
    "        return dX\n",
    "\n",
    "    \n",
    "class Relu():\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.mask = (X <= 0)\n",
    "        out = X.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "    \n",
    "    \n",
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "    \n",
    "    def forward(self, X, t):\n",
    "        self.t = t\n",
    "        self.y = self.softmax(X)\n",
    "        self.loss = self.cross_entropy_error(self.y, self.t)\n",
    "        return self.loss \n",
    "    \n",
    "    def cross_entropy_error(self, y, t):\n",
    "        if t.size == y.size:\n",
    "            t = t.argmax(axis=1)\n",
    "            \n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "\n",
    "    def softmax(self, X):\n",
    "        X = X - np.max(X, axis=-1, keepdims=True)\n",
    "        y = np.exp(X) / np.sum(np.exp(X), axis=-1, keepdims=True)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dX = (self.y - self.t) / batch_size\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題5】ReLUクラスの作成\n",
    "\n",
    "現在一般的に使われている活性化関数であるReLU（Rectified Linear Unit）をReLUクラスとして実装してください。\n",
    "\n",
    "\n",
    "ReLUは以下の数式です。\n",
    "\n",
    "$$f(x) = ReLU(x) = \\begin{cases}\n",
    "x  & \\text{if $x>0$,}\\\\\n",
    "0 & \\text{if $x\\leqq0$.}\n",
    "\\end{cases}$$\n",
    "\n",
    "$\n",
    "x\n",
    " $ : ある特徴量。スカラー\n",
    "\n",
    "\n",
    "実装上はnp.maximumを使い配列に対してまとめて計算が可能です。\n",
    "\n",
    "\n",
    "numpy.maximum — NumPy v1.15 Manual\n",
    "\n",
    "\n",
    "一方、バックプロパゲーションのための $\n",
    "x\n",
    "$ に関する $\n",
    "f\n",
    "(\n",
    "x\n",
    ")\n",
    " $の微分は以下のようになります。\n",
    " \n",
    " $$\\frac{\\partial f(x)}{\\partial x} = \\begin{cases}\n",
    "1  & \\text{if $x>0$,}\\\\\n",
    "0 & \\text{if $x\\leqq0$.}\n",
    "\\end{cases}$$\n",
    "\n",
    "数学的には微分可能ではないですが、 $\n",
    "x\n",
    "=$\n",
    "0\n",
    " のとき \n",
    "0\n",
    " とすることで対応しています。\n",
    "\n",
    "\n",
    "フォワード時の $\n",
    "x\n",
    "$ の正負により、勾配を逆伝播するかどうかが決まるということになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __init__(self):\n",
    "        self.re = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.re = (X <= 0) # Xが0以下True, その他False\n",
    "        out = X.copy() # X配列コピー\n",
    "        out[self.re] = 0 # Trueを0に\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.re] = 0 # X <= 0がtrueはdoutも0で流す。それ以外はそのまま\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 3.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = Relu()\n",
    "X = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(X)\n",
    "r.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 3.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.backward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題6】重みの初期値\n",
    "\n",
    "ここまでは重みやバイアスの初期値は単純にガウス分布で、標準偏差をハイパーパラメータとして扱ってきました。しかし、どのような値にすると良いかが知られています。**シグモイド関数やハイパボリックタンジェント関数のときは Xavierの初期値 （またはGlorotの初期値）、ReLUのときは Heの初期値**が使われます。\n",
    "\n",
    "\n",
    "XavierInitializerクラスと、HeInitializerクラスを作成してください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Xavierの初期値\n",
    "\n",
    "Xavierの初期値における標準偏差 $\n",
    "σ\n",
    " $は次の式で求められます。\n",
    "\n",
    "$$\\sigma = \\frac{1}{\\sqrt{n}}$$\n",
    "\n",
    "$\n",
    "n\n",
    "$ : 前の層のノード数\n",
    "\n",
    "\n",
    "《論文》\n",
    "\n",
    "\n",
    "[Glorot, X., & Bengio, Y. (n.d.). Understanding the difficulty of training deep feedforward neural networks.](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.16596774  0.04134366 -0.07634078 ...  0.15320779 -0.00765714\n",
      "  -0.15913247]\n",
      " [ 0.10910505  0.06939297  0.03977043 ...  0.0855751   0.10807304\n",
      "   0.0811386 ]\n",
      " [-0.0005458  -0.05655311  0.16624024 ... -0.20310365 -0.04564968\n",
      "   0.24123173]\n",
      " ...\n",
      " [ 0.06537345 -0.08212668 -0.092878   ...  0.10692695  0.08270611\n",
      "   0.19634295]\n",
      " [ 0.01319648  0.10331601 -0.08131033 ... -0.02767562  0.0657592\n",
      "   0.07821265]\n",
      " [ 0.00942635 -0.21196408  0.13388378 ... -0.14580869  0.10347939\n",
      "  -0.00220008]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "n_nodes1 = 100\n",
    "n_nodes2 = 50\n",
    "\n",
    "W = np.random.randn(n_nodes1, n_nodes1) * np.sqrt(1.0 / n_nodes1)\n",
    "B = np.zeros(n_nodes2)\n",
    "\n",
    "print(W)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Xa:\n",
    "    def __init__(self):\n",
    "        self.sigma = None\n",
    "    # 重みXavierの初期値\n",
    "    def W(self,n_nodes1, n_nodes2):\n",
    "        self.sigma = np.sqrt(1.0 /n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    # バイアスXavierの初期値\n",
    "    def B(self,n_nodes2):\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.15907461  0.09478357 -0.01437048 ...  0.07486118  0.0834418\n",
      "   0.06167519]\n",
      " [-0.13999685 -0.03658261  0.05485498 ...  0.06532079 -0.05598093\n",
      "  -0.02532106]\n",
      " [-0.07203654  0.0162059   0.04485964 ... -0.02930112  0.04159067\n",
      "   0.17563715]\n",
      " ...\n",
      " [-0.05337108 -0.04976893  0.00497999 ... -0.11144727  0.05111585\n",
      "   0.05351869]\n",
      " [ 0.01094921 -0.01256015 -0.1103164  ...  0.01292956  0.00935995\n",
      "  -0.00555607]\n",
      " [ 0.00118825 -0.02249143  0.04604969 ...  0.12898617  0.0705996\n",
      "   0.16956393]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_nodes1 = 100\n",
    "n_nodes2 = 50\n",
    "x = Xa()\n",
    "print(x.W(n_nodes1,n_nodes2))\n",
    "x.B(n_nodes2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Heの初期値\n",
    "\n",
    "Heの初期値における標準偏差 $\n",
    "σ\n",
    "$ は次の式で求められます。\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{2}{n}}$$\n",
    "\n",
    "$\n",
    "n\n",
    "$ : 前の層のノード数\n",
    "\n",
    "\n",
    "《論文》\n",
    "\n",
    "\n",
    "[He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.](https://arxiv.org/pdf/1502.01852.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03426505, -0.11828898, -0.16715475, ..., -0.13484292,\n",
       "         0.20864444, -0.1860629 ],\n",
       "       [ 0.17901617,  0.07118505, -0.14665805, ...,  0.10942153,\n",
       "        -0.15166724,  0.033001  ],\n",
       "       [ 0.05017649, -0.33483854, -0.02549506, ..., -0.10957901,\n",
       "        -0.31453284, -0.03234512],\n",
       "       ...,\n",
       "       [-0.09429315,  0.14836237,  0.03503994, ..., -0.03910265,\n",
       "        -0.06668286, -0.05847402],\n",
       "       [ 0.0332618 , -0.05315197,  0.00334274, ...,  0.10373874,\n",
       "        -0.05996185,  0.06635967],\n",
       "       [-0.16926033, -0.01536074, -0.00354062, ...,  0.33406471,\n",
       "        -0.02833084, -0.12737457]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_nodes1 = 100\n",
    "n_nodes2 = 50\n",
    "\n",
    "\n",
    "W = np.random.randn(n_nodes1, n_nodes1) * np.sqrt(2.0 / n_nodes1)\n",
    "\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class He:\n",
    "    def __init__(self):\n",
    "        self.sigma = None\n",
    "    \n",
    "    def W(self,n_nodes1, n_nodes2):\n",
    "        self.sigma = np.sqrt(2.0 / n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self,n_nodes2):\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01469279  0.04531565 -0.12699921 ...  0.09317769  0.09540882\n",
      "  -0.08918127]\n",
      " [ 0.01394457  0.16429734  0.01718169 ...  0.13928628  0.01157844\n",
      "  -0.1081613 ]\n",
      " [ 0.01821658  0.02787066  0.17234996 ...  0.08896094 -0.19679316\n",
      "   0.0624089 ]\n",
      " ...\n",
      " [ 0.13893473 -0.27640487  0.36381989 ... -0.0440318  -0.00152922\n",
      "  -0.01345992]\n",
      " [ 0.04266979 -0.22245155  0.12896235 ... -0.02361345 -0.13229444\n",
      "   0.03762407]\n",
      " [-0.14141621 -0.08600253 -0.01883921 ...  0.17619011 -0.26556776\n",
      "  -0.11076688]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_nodes1 = 100\n",
    "n_nodes2 = 50\n",
    "h = He()\n",
    "print(h.W(n_nodes1,n_nodes2))\n",
    "h.B(n_nodes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class XaHe:\n",
    "    def __init__(self,weight = \"relu\"):\n",
    "        self.sigma = None\n",
    "        \n",
    "    def W(self,n_nodes1, n_nodes2, weight=\"relu\"):\n",
    "        if str(weight).lower() in (\"relu\",\"he\"):\n",
    "            self.sigma = np.sqrt(2.0 / n_nodes1)\n",
    "        else:\n",
    "            self.sigma = np.sqrt(1.0 / n_nodes1)\n",
    "        return self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "    \n",
    "    def B(self,n_nodes2):\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23682445, -0.20517894, -0.16754571, ...,  0.21390669,\n",
       "        -0.12745285, -0.24834556],\n",
       "       [ 0.25163077,  0.05230796, -0.2935439 , ..., -0.27101365,\n",
       "        -0.06976079, -0.10990028],\n",
       "       [ 0.14164676, -0.15082956, -0.04494215, ..., -0.01808834,\n",
       "         0.0040297 ,  0.14668327],\n",
       "       ...,\n",
       "       [-0.13172687,  0.05600871, -0.00078339, ...,  0.2421714 ,\n",
       "        -0.03989738, -0.03550757],\n",
       "       [-0.03401694, -0.13744403, -0.04127357, ...,  0.06084148,\n",
       "        -0.07984086, -0.04412385],\n",
       "       [-0.21396175,  0.30239262,  0.03005444, ...,  0.13453686,\n",
       "         0.04167189, -0.27654   ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_nodes1 = 100\n",
    "n_nodes2 = 50\n",
    "\n",
    "xh = XaHe()\n",
    "xh.W(n_nodes1,n_nodes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01268553, -0.0231986 ,  0.02039743, ...,  0.02259602,\n",
       "        -0.41062539, -0.07098313],\n",
       "       [ 0.0325844 ,  0.04843175, -0.16989287, ..., -0.00562851,\n",
       "        -0.05871227, -0.08053977],\n",
       "       [ 0.16191953, -0.12326271,  0.05172583, ..., -0.13235442,\n",
       "         0.17194335,  0.25023974],\n",
       "       ...,\n",
       "       [ 0.29112182, -0.03631873,  0.09167873, ...,  0.04279288,\n",
       "         0.06346188, -0.07003524],\n",
       "       [-0.23158979, -0.14310827, -0.26797147, ...,  0.12018152,\n",
       "        -0.08465732,  0.04216491],\n",
       "       [ 0.29950694, -0.08991401,  0.09407903, ...,  0.00122431,\n",
       "        -0.19805799,  0.07435713]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_nodes1 = 100\n",
    "n_nodes2 = 50\n",
    "\n",
    "xh = XaHe(weight = \"sig\")\n",
    "xh.W(n_nodes1,n_nodes2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題7】最適化手法\n",
    "\n",
    "学習率は学習過程で変化させていく方法が一般的です。基本的な手法である AdaGrad のクラスを作成してください。\n",
    "\n",
    "\n",
    "まず、これまで使ってきたSGDを確認します。\n",
    "\n",
    "$$W_i^{\\prime} = W_i - \\alpha E(\\frac{\\partial L}{\\partial W_i}) \\\\\n",
    "B_i^{\\prime} = B_i - \\alpha E(\\frac{\\partial L}{\\partial B_i})$$\n",
    "\n",
    "$\n",
    "α\n",
    " $: 学習率（層ごとに変えることも可能だが、基本的には全て同じとする）\n",
    " \n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_i}$  :$\n",
    "W\n",
    "i\n",
    "$ に関する損失 $\n",
    "L\n",
    "$ の勾配\n",
    "\n",
    "$\\frac{\\partial L}{\\partial B_i}$\n",
    " : $\n",
    "B\n",
    "i\n",
    "$ に関する損失 $\n",
    "L\n",
    " $の勾配\n",
    "\n",
    "\n",
    "\n",
    "$\n",
    "E\n",
    "(\n",
    ")\n",
    "$ : ミニバッチ方向にベクトルの平均を計算\n",
    "\n",
    "\n",
    "続いて、AdaGradです。バイアスの数式は省略しますが、重みと同様のことをします。\n",
    "\n",
    "\n",
    "更新された分だけその重みに対する学習率を徐々に下げていきます。イテレーションごとの勾配の二乗和 \n",
    "H\n",
    " を保存しておき、その分だけ学習率を小さくします。\n",
    "\n",
    "\n",
    "学習率は重み一つひとつに対して異なることになります。\n",
    "\n",
    "\n",
    "$$ H_i^{\\prime}  = H_i+E(\\frac{\\partial L}{\\partial W_i})×E(\\frac{\\partial L}{\\partial W_i})\\\\\n",
    "W_i^{\\prime} = W_i - \\alpha \\frac{1}{\\sqrt{H_i^{\\prime} }} E(\\frac{\\partial L}{\\partial W_i}) \\\\ $$\n",
    "\n",
    "$\n",
    "H\n",
    "i\n",
    " $: i層目に関して、前のイテレーションまでの勾配の二乗和（初期値は0）\n",
    "\n",
    "$\n",
    "H\n",
    "′\n",
    "i\n",
    "$ : 更新した $\n",
    "H\n",
    "i\n",
    "$\n",
    "\n",
    "\n",
    "《論文》\n",
    "[Duchi JDUCHI, J., & Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization * Elad Hazan. Journal of Machine Learning Research (Vol. 12).](https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, layer):\n",
    "        if (self.h_W is None) and (self.h_b is None):\n",
    "            self.h_W = 0\n",
    "            self.h_b = 0\n",
    "        \n",
    "        self.h_W += (layer.dW ** 2).sum()\n",
    "        self.h_b += (layer.db ** 2).sum()\n",
    "        layer.W -= self.lr * layer.dW / (np.sqrt(self.h_W) + 1e-7)\n",
    "        layer.b -= self.lr * layer.db / (np.sqrt(self.h_b) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max()) # 1.0\n",
    "print(X_train.min()) # 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "print(y_train.shape) # (60000,)\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) # float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(12000, 784)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "print(X_train.shape) # (48000, 784)\n",
    "print(X_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_val = y_val.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題8】クラスの完成\n",
    "\n",
    "任意の構成で学習と推定が行えるScratchDeepNeuralNetrowkClassifierクラスを完成させてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # W,bを初期化\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.b = initializer.B(n_nodes2)\n",
    "        self.X = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        out : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        self.X = X\n",
    "        out = np.dot(X, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        batch_size = dA.shape[0]\n",
    "        dX = np.dot(dA, self.W.T)\n",
    "        self.dW = np.dot(self.X.T, dA) #/ batch_size\n",
    "        self.db = np.sum(dA, axis=0) #/ batch_size\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     0,
     14,
     18
    ]
   },
   "outputs": [],
   "source": [
    "class Xa:\n",
    "    def __init__(self):\n",
    "        self.sigma = None\n",
    "    \n",
    "    def W(self,n_nodes1, n_nodes2):\n",
    "        self.sigma = np.sqrt(1.0 /n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self,n_nodes2):\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B\n",
    "    \n",
    "    \n",
    "class He:\n",
    "    def __init__(self):\n",
    "        self.sigma = None\n",
    "    \n",
    "    def W(self,n_nodes1, n_nodes2):\n",
    "        self.sigma = np.sqrt(2.0 / n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self,n_nodes2):\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [
     0,
     16,
     32,
     50
    ]
   },
   "outputs": [],
   "source": [
    "class Tangent():\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = np.tanh(X)\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dX = dout*(1 - self.out**2)\n",
    "        \n",
    "        return dX\n",
    "\n",
    "    \n",
    "class Sigmoid():\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = 1 / (1 + np.exp(-X))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dX = dout*(1.0 - self.out) * self.out\n",
    "        \n",
    "        return dX\n",
    "\n",
    "    \n",
    "class Relu():\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.mask = (X <= 0)\n",
    "        out = X.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "    \n",
    "    \n",
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "    \n",
    "    def forward(self, X, t):\n",
    "        self.t = t\n",
    "        self.y = self.softmax(X)\n",
    "        self.loss = self.cross_entropy_error(self.y, self.t)\n",
    "        return self.loss \n",
    "    \n",
    "    def cross_entropy_error(self, y, t):     \n",
    "        batch_size = y.shape[0]\n",
    "\n",
    "        return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "\n",
    "    def softmax(self, X):\n",
    "        X = X - np.max(X, axis=-1, keepdims=True)\n",
    "        y = np.exp(X) / np.sum(np.exp(X), axis=-1, keepdims=True)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dX = (self.y - self.t) / batch_size\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.b -= self.lr * layer.db\n",
    "        \n",
    "        \n",
    "class AdaGrad:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.h_W = None\n",
    "        self.h_b = None\n",
    "        \n",
    "    def update(self, layer):\n",
    "        if (self.h_W is None) and (self.h_b is None):\n",
    "            self.h_W = 0\n",
    "            self.h_b = 0\n",
    "        \n",
    "        self.h_W += (layer.dW ** 2).sum()\n",
    "        self.h_b += (layer.db ** 2).sum()\n",
    "        layer.W -= self.lr * layer.dW / (np.sqrt(self.h_W) + 1e-7)\n",
    "        layer.b -= self.lr * layer.db / (np.sqrt(self.h_b) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowk(object):\n",
    "    \"\"\"\n",
    "    全結合による多層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr, verbose=True, batch_size=20, max_iter=3):\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        \n",
    "        # レイヤの生成\n",
    "        initializer = He()\n",
    "        optimizer = SGD(lr=lr)\n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        self.layers[\"FC1\"] = FC(784, 100, initializer, optimizer)\n",
    "        \n",
    "        self.layers[\"ReLU\"] = Relu()\n",
    "        \n",
    "        self.layers[\"FC2\"] = FC(100, 50, initializer, optimizer)\n",
    "        \n",
    "        self.layers[\"ReLU2\"] = Relu()\n",
    "        \n",
    "        self.layers[\"FC3\"] = FC(50, 10, initializer, optimizer)\n",
    "        \n",
    "        self.lastLayer = Softmax()\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        # 1エポック当たりの繰り返し回数\n",
    "        iter_per_epoch = 1000\n",
    "        # エポック複数繰り返す\n",
    "        for _  in range(self.max_iter):\n",
    "            \n",
    "            # 1エポック\n",
    "            for i in range(iter_per_epoch):\n",
    "                #損失計算用のリスト\n",
    "                s_list = []\n",
    "                \n",
    "                batch_mask = np.random.choice(X.shape[0], self.batch_size)\n",
    "                X_batch = X[batch_mask]\n",
    "                y_batch = y[batch_mask]\n",
    "                \n",
    "                #勾配\n",
    "                self._gradient(X_batch, y_batch)\n",
    "                    \n",
    "                loss = self._loss(X_batch, y_batch)\n",
    "                s_list.append(loss)\n",
    "        \n",
    "                if self.verbose:\n",
    "                    #verboseをTrueにした際は学習過程などを出力する\n",
    "                    print(loss)\n",
    "                        \n",
    "            self.train_loss.append(loss)\n",
    "            if (X_val is not None) and (y_val is not None):\n",
    "                loss_test = self._loss(X_val, y_val)\n",
    "                self.test_loss.append(loss_test)\n",
    "                    \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        推論\n",
    "        \n",
    "        return 画像データ（入力データ）\n",
    "        \"\"\"\n",
    "        for layer in self.layers.values():\n",
    "            X = layer.forward(X)\n",
    "            pred = np.argmax(X, axis=1)\n",
    "        return pred\n",
    "    \n",
    "    \n",
    "    def _loss(self, X, t):\n",
    "#         y = self.predict(X)\n",
    "#         return self.lastLayer.forward(y, t)\n",
    "\n",
    "        for layer in self.layers.values():\n",
    "            X = layer.forward(X)\n",
    "            \n",
    "        return self.lastLayer.forward(X, t)\n",
    "\n",
    "        \n",
    "    def _gradient(self, X, t):\n",
    "        \"\"\"\n",
    "        重みパラメータに対する勾配を誤差逆伝播法によって求める\n",
    "        \n",
    "        X : 画像データ（入力データ)\n",
    "        t : 教師データ\n",
    "        \n",
    "        \"\"\"\n",
    "        self._loss(X, t)\n",
    "        \n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##【問題9】学習と推定\n",
    "\n",
    "層の数や活性化関数を変えたいくつかのネットワークを作成してください。そして、MNISTのデータを学習・推定し、Accuracyを計算してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(48000, 10)\n",
      "(12000, 784)\n",
      "(12000, 10)\n"
     ]
    }
   ],
   "source": [
    "sdn = ScratchDeepNeuralNetrowk(lr=0.001, verbose=False, batch_size=20, max_iter=5)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdn.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正答率:0.8624166666666667\n"
     ]
    }
   ],
   "source": [
    "y_val = np.argmax(y_val, axis=1)\n",
    "y_pred = sdn.predict(X_val)\n",
    "accuracy = accuracy_score(y_pred, y_val)\n",
    "print(\"正答率:{}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1, len(sdn.train_loss)+1)\n",
    "plt.plot(x, sdn.train_loss, label=\"loss\")\n",
    "plt.plot(x, sdn.test_loss, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "グラフの原因は学習率だった\n",
    "\n",
    "0.1 を0.001に変更"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "層を増やしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowk(object):\n",
    "    \"\"\"\n",
    "    全結合による多層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr, verbose=True, batch_size=20, max_iter=3):\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        \n",
    "        # レイヤの生成\n",
    "        initializer = He()\n",
    "        optimizer = SGD(lr=lr)\n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        self.layers[\"FC1\"] = FC(784, 400, initializer, optimizer)\n",
    "        \n",
    "        self.layers[\"sig1\"] = Sigmoid()\n",
    "        \n",
    "        self.layers[\"FC2\"] = FC(400, 300, initializer, optimizer)\n",
    "        \n",
    "        self.layers[\"sig2\"] = Sigmoid()\n",
    "        \n",
    "        self.layers[\"FC3\"] = FC(300, 200, initializer, optimizer)\n",
    "        \n",
    "        self.layers[\"ReLU1\"] = Relu()\n",
    "        \n",
    "        self.layers[\"FC4\"] = FC(200, 100, initializer, optimizer)\n",
    "        \n",
    "        self.layers[\"ReLU2\"] = Relu()\n",
    "        \n",
    "        self.layers[\"FC5\"] = FC(100, 50, initializer, optimizer)\n",
    "        \n",
    "        self.layers[\"ReLU3\"] = Relu()\n",
    "        \n",
    "        self.layers[\"FC6\"] = FC(50, 10, initializer, optimizer)\n",
    "        \n",
    "        self.lastLayer = Softmax()\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        # 1エポック当たりの繰り返し回数\n",
    "        iter_per_epoch = 500\n",
    "        # エポック複数繰り返す\n",
    "        for _  in range(self.max_iter):\n",
    "            \n",
    "            # 1エポック\n",
    "            for i in range(iter_per_epoch):\n",
    "\n",
    "                s_list = []\n",
    "                \n",
    "                batch_mask = np.random.choice(X.shape[0], self.batch_size)\n",
    "                X_batch = X[batch_mask]\n",
    "                y_batch = y[batch_mask]\n",
    "                \n",
    "                #勾配\n",
    "                self._gradient(X_batch, y_batch)\n",
    "                    \n",
    "                loss = self._loss(X_batch, y_batch)\n",
    "                s_list.append(loss)\n",
    "                \n",
    "                if self.verbose:\n",
    "                    #verboseをTrueにした際は学習過程などを出力する\n",
    "                    print(train_loss)\n",
    "                        \n",
    "            \n",
    "            self.train_loss.append(loss)\n",
    "            if (X_val is not None) and (y_val is not None):\n",
    "                loss_test = self._loss(X_val, y_val)\n",
    "                self.test_loss.append(loss_test)\n",
    "                    \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        推論\n",
    "        \n",
    "        return 画像データ（入力データ）\n",
    "        \"\"\"\n",
    "        for layer in self.layers.values():\n",
    "            X = layer.forward(X)\n",
    "            pred = np.argmax(X, axis=1)\n",
    "        return pred\n",
    "    \n",
    "    \n",
    "    def _loss(self, X, t):\n",
    "#         y = self.predict(X)\n",
    "#         return self.lastLayer.forward(y, t)\n",
    "\n",
    "        for layer in self.layers.values():\n",
    "            X = layer.forward(X)\n",
    "            \n",
    "        return self.lastLayer.forward(X, t)\n",
    "\n",
    "        \n",
    "    def _gradient(self, X, t):\n",
    "        \"\"\"\n",
    "        重みパラメータに対する勾配を誤差逆伝播法によって求める\n",
    "        \n",
    "        X : 画像データ（入力データ)\n",
    "        t : 教師データ\n",
    "        \n",
    "        \"\"\"\n",
    "        self._loss(X, t)\n",
    "        \n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "y_train = y_train.astype(int)\n",
    "y_val = y_val.astype(int)\n",
    "\n",
    "sdn2 = ScratchDeepNeuralNetrowk(lr=0.1, verbose=False, batch_size=20, max_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdn2.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正答率:0.866\n"
     ]
    }
   ],
   "source": [
    "y_val = np.argmax(y_val, axis=1)\n",
    "y_pred = sdn2.predict(X_val)\n",
    "accuracy = accuracy_score(y_pred, y_val)\n",
    "print(\"正答率:{}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1, len(sdn2.train_loss)+1)\n",
    "plt.plot(x, sdn2.train_loss, label=\"loss\")\n",
    "plt.plot(x, sdn2.test_loss, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "層の数や大きさを上げたらよいというわけではなさそう\n",
    "\n",
    "学習率を0.001から0.1 に変更\n",
    "\n",
    "グラフが向上\n",
    "\n",
    "層の数を増やしたとき学習率は上げた方が良いのか？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "課題\n",
    "\n",
    "ゼロから作るDeep Learning参考に作成\n",
    "コードをしっかりと理解しないと\n",
    "\n",
    "正解率は9割出ているのでよし\n",
    "\n",
    "ロスの曲線がおかしいのでコードを修正しないと"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
